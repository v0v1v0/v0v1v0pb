<div class="container">

<table style="width: 100%;"><tr>
<td>learn_DAG</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>MCMC scheme for Gaussian DAG posterior inference</h2>

<h3>Description</h3>

<p>This function implements a Markov Chain Monte Carlo (MCMC) algorithm for structure learning of Gaussian
DAGs and posterior inference of DAG model parameters
</p>


<h3>Usage</h3>

<pre><code class="language-R">learn_DAG(
  S,
  burn,
  data,
  a,
  U,
  w,
  fast = FALSE,
  save.memory = FALSE,
  collapse = FALSE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>S</code></td>
<td>
<p>integer final number of MCMC draws from the posterior of DAGs and parameters</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>burn</code></td>
<td>
<p>integer initial number of burn-in iterations, needed by the MCMC chain to reach its stationary distribution and not included in the final output</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p><code class="reqn">(n,q)</code> data matrix</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>a</code></td>
<td>
<p>common shape hyperparameter of the compatible DAG-Wishart prior, <code class="reqn">a &gt; q - 1</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>U</code></td>
<td>
<p>position hyperparameter of the compatible DAG-Wishart prior, a <code class="reqn">(q, q)</code> s.p.d. matrix</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>w</code></td>
<td>
<p>edge inclusion probability hyperparameter of the DAG prior in <code class="reqn">[0,1]</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fast</code></td>
<td>
<p>boolean, if <code>TRUE</code> an approximate proposal for the MCMC moves is implemented</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>save.memory</code></td>
<td>
<p>boolean, if <code>TRUE</code> MCMC draws are stored as strings, instead of arrays</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>collapse</code></td>
<td>
<p>boolean, if <code>TRUE</code> only structure learning of DAGs is performed</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>If <code>TRUE</code>, progress bars are displayed</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Consider a collection of random variables <code class="reqn">X_1, \dots, X_q</code> whose distribution is zero-mean multivariate Gaussian with covariance matrix Markov w.r.t. a Directed Acyclic Graph (DAG).
Assuming the underlying DAG is unknown (model uncertainty), a Bayesian method for posterior inference on the joint space of DAG structures and parameters can be implemented.
The proposed method assigns a prior on each DAG structure through independent Bernoulli distributions, <code class="reqn">Ber(w)</code>, on the 0-1 elements of the DAG adjacency matrix.
Conditionally on a given DAG, a prior on DAG parameters <code class="reqn">(D,L)</code> (representing a Cholesky-type reparameterization of the covariance matrix) is assigned through a compatible DAG-Wishart prior;
see also function <code>rDAGWishart</code> for more details.
</p>
<p>Posterior inference on the joint space of DAGs and DAG parameters is carried out through a Partial Analytic Structure (PAS) algorithm.
Two steps are iteratively performed for <code class="reqn">s = 1, 2, ...</code> : (1) update of the DAG through a Metropolis Hastings (MH) scheme;
(2) sampling from the posterior distribution of the (updated DAG) parameters.
In step (1) the update of the (current) DAG is performed by drawing a new (direct successor) DAG from a suitable proposal distribution. The proposed DAG is obtained by applying a local move (insertion, deletion or edge reversal)
to the current DAG and is accepted with probability given by the MH acceptance rate.
The latter requires to evaluate the proposal distribution at both the current and proposed DAGs, which in turn involves the enumeration of
all DAGs that can be obtained from local moves from respectively the current and proposed DAG.
Because the ratio of the two proposals is approximately equal to one, and the approximation becomes as precise as <code class="reqn">q</code> grows, a faster strategy implementing such an approximation is provided with
<code>fast = TRUE</code>. The latter choice is especially recommended for moderate-to-large number of nodes <code class="reqn">q</code>.
</p>
<p>Output of the algorithm is a collection of <code class="reqn">S</code> DAG structures (represented as <code class="reqn">(q,q)</code> adjacency matrices) and DAG parameters <code class="reqn">(D,L)</code> approximately drawn from the joint posterior.
The various outputs are organized in <code class="reqn">(q,q,S)</code> arrays; see also the example below.
If the target is DAG learning only, a collapsed sampler implementing the only step (1) of the MCMC scheme can be obtained
by setting <code>collapse = TRUE</code>. In this case, the algorithm outputs a collection of <code class="reqn">S</code> DAG structures only.
See also functions <code>get_edgeprobs</code>, <code>get_MAPdag</code>, <code>get_MPMdag</code> for posterior summaries of the MCMC output.
</p>
<p>Print, summary and plot methods are available for this function. <code>print</code> provides information about the MCMC output and the values of the input prior hyperparameters. <code>summary</code> returns, besides the previous information, a <code class="reqn">(q,q)</code> matrix collecting the marginal posterior probabilities of edge inclusion. <code>plot</code> returns the estimated Median Probability DAG Model (MPM), a <code class="reqn">(q,q)</code> heat map with estimated marginal posterior probabilities of edge inclusion, and a barplot summarizing the distribution of the size of DAGs visited by the MCMC.
</p>


<h3>Value</h3>

<p>An S3 object of class <code>bcdag</code> containing <code class="reqn">S</code> draws from the posterior of DAGs and (if <code>collapse = FALSE</code>) of DAG parameters <code class="reqn">D</code> and <code class="reqn">L</code>. If <code>save.memory = FALSE</code>, these are stored in three arrays of dimension <code class="reqn">(q,q,S)</code>. Otherwise, they are stored as strings.
</p>


<h3>Author(s)</h3>

<p>Federico Castelletti and Alessandro Mascaro
</p>


<h3>References</h3>

<p>F. Castelletti and A. Mascaro (2021). Structural learning and estimation of joint causal effects among network-dependent variables. <em>Statistical Methods and Applications</em>, Advance publication.
</p>
<p>F. Castelletti and A. Mascaro (2022). BCDAG: An R package for Bayesian structural and Causal learning of Gaussian DAGs. <em>arXiv pre-print</em>, url: https://arxiv.org/abs/2201.12003
</p>
<p>F. Castelletti (2020). Bayesian model selection of Gaussian Directed Acyclic Graph structures. <em>International Statistical Review</em> 88 752-775.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Randomly generate a DAG and the DAG-parameters
q = 8
w = 0.2
set.seed(123)
DAG = rDAG(q = q, w = w)
outDL = rDAGWishart(n = 1, DAG = DAG, a = q, U = diag(1, q))
L = outDL$L; D = outDL$D
Sigma = solve(t(L))%*%D%*%solve(L)
# Generate observations from a Gaussian DAG-model
n = 200
X = mvtnorm::rmvnorm(n = n, sigma = Sigma)

## Set S = 5000 and burn = 1000 for better results

# [1] Run the MCMC for posterior inference of DAGs and parameters (collapse = FALSE)
out_mcmc = learn_DAG(S = 50, burn = 10, a = q, U = diag(1,q)/n, data = X, w = 0.1,
                     fast = FALSE, save.memory = FALSE, collapse = FALSE)
# [2] Run the MCMC for posterior inference of DAGs only (collapse = TRUE)
out_mcmc_collapse = learn_DAG(S = 50, burn = 10, a = q, U = diag(1,q)/n, data = X, w = 0.1,
                              fast = FALSE, save.memory = FALSE, collapse = TRUE)
# [3] Run the MCMC for posterior inference of DAGs only with approximate proposal
# distribution (fast = TRUE)
# out_mcmc_collapse_fast = learn_DAG(S = 50, burn = 10, a = q, U = diag(1,q)/n, data = X, w = 0.1,
#                                    fast = FALSE, save.memory = FALSE, collapse = TRUE)
# Compute posterior probabilities of edge inclusion and Median Probability DAG Model
# from the MCMC outputs [2] and [3]
get_edgeprobs(out_mcmc_collapse)
# get_edgeprobs(out_mcmc_collapse_fast)
get_MPMdag(out_mcmc_collapse)
# get_MPMdag(out_mcmc_collapse_fast)

# Methods
print(out_mcmc)
summary(out_mcmc)
plot(out_mcmc)

</code></pre>


</div>