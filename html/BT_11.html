<div class="container">

<table style="width: 100%;"><tr>
<td>BT_perf</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Performance assessment.</h2>

<h3>Description</h3>

<p>Function to compute the performances of a fitted boosting tree.
</p>


<h3>Usage</h3>

<pre><code class="language-R">BT_perf(
  BTFit_object,
  plot.it = TRUE,
  oobag.curve = FALSE,
  overlay = TRUE,
  method,
  main = ""
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>BTFit_object</code></td>
<td>
<p>a <code>BTFit</code> object resulting from an initial call to <code>BT</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>plot.it</code></td>
<td>
<p>a boolean indicating whether to plot the performance measure. Setting <code>plot.it = TRUE</code> creates two plots.
The first one plots the <code>object$BTErrors$training.error</code> (in black) as well as the <code>object$BTErrors$validation.error</code> (in red) and/or the <code>object$BTErrors$cv.error</code> (in green) depending on the <code>method</code> and
parametrization. These values are plotted as a function of the iteration number. The scale of the error measurement, shown on the left vertical axis, depends on the arguments used in the
initial call to <code>BT</code> and the chosen <code>method</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>oobag.curve</code></td>
<td>
<p>indicates whether to plot the out-of-bag performance measures in a second plot. Note that this option makes sense if the <code>bag.fraction</code> was properly defined in the
initial call to <code>BT</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>overlay</code></td>
<td>
<p>if set to <code>TRUE</code> and <code>oobag.curve=TRUE</code> then a right y-axis is added and the estimated cumulative improvement in the loss function is
plotted versus the iteration number.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>indicates the method used to estimate the optimal number of boosting iterations. Setting <code>method = "OOB"</code> computes the out-of-bag estimate and <code>method = "validation"</code>
uses the validation dataset to compute an out-of-sample estimate. Finally, setting <code>method = "cv"</code> extracts the optimal number of iterations using cross-validation, if
<code>BT</code> was called with <code>cv.folds &gt; 1</code>. If missing, a guessing method is applied.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>main</code></td>
<td>
<p>optional parameter that allows the user to define specific plot title.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>Returns the estimated optimal number of iterations. The method of computation depends on the <code>method</code> argument.
</p>


<h3>Author(s)</h3>

<p>Gireg Willame <a href="mailto:g.willame@detralytics.eu">g.willame@detralytics.eu</a>
</p>
<p><em>This package is inspired by the <code>gbm3</code> package. For more details, see <a href="https://github.com/gbm-developers/gbm3/">https://github.com/gbm-developers/gbm3/</a></em>.
</p>


<h3>References</h3>

<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries |: GLMs and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries ||: Tree-Based Methods and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries |||: Neural Networks and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2022). <strong>Response versus gradient boosting trees, GLMs and neural networks under Tweedie loss and log-link</strong>.
Accepted for publication in <em>Scandinavian Actuarial Journal</em>.
</p>
<p>M. Denuit, J. Huyghe and J. Trufin (2022). <strong>Boosting cost-complexity pruned trees on Tweedie responses: The ABT machine for insurance ratemaking</strong>.
Paper submitted for publication.
</p>
<p>M. Denuit, J. Trufin and T. Verdebout (2022). <strong>Boosting on the responses with Tweedie loss functions</strong>. Paper submitted for publication.
</p>


<h3>See Also</h3>

<p><code>BT</code>, <code>BT_call</code>.
</p>


</div>