<div class="container">

<table style="width: 100%;"><tr>
<td>mbst</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2> Boosting for Multi-Classification</h2>

<h3>Description</h3>

<p>Gradient boosting for optimizing multi-class loss functions with componentwise
linear, smoothing splines, tree models as base learners.
</p>


<h3>Usage</h3>

<pre><code class="language-R">mbst(x, y, cost = NULL, family = c("hinge", "hinge2", "thingeDC", "closs", "clossMM"), 
ctrl = bst_control(), control.tree=list(fixed.depth=TRUE, 
n.term.node=6, maxdepth = 1), learner = c("ls", "sm", "tree"))
## S3 method for class 'mbst'
print(x, ...)
## S3 method for class 'mbst'
predict(object, newdata=NULL, newy=NULL, mstop=NULL, 
type=c("response", "class", "loss", "error"), ...)
## S3 method for class 'mbst'
fpartial(object, mstop=NULL, newdata=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p> a data frame containing the variables in the model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p> vector of responses. <code>y</code> must be 1, 2, ..., k for a k classification problem</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cost</code></td>
<td>
<p> price to pay for false positive, 0 &lt; <code>cost</code> &lt; 1; price of false negative is 1-<code>cost</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>family</code></td>
<td>
 <p><code>family</code> = "hinge" for hinge loss, <code>family</code>="hinge2" for hinge loss but the response is not recoded (see details). <code>family="thingeDC"</code> for DCB loss function, see <code>rmbst</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ctrl</code></td>
<td>
<p> an object of class <code>bst_control</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>control.tree</code></td>
<td>
<p> control parameters of rpart. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>learner</code></td>
<td>
<p> a character specifying the component-wise base learner to be used:
<code>ls</code> linear models, 
<code>sm</code> smoothing splines,
<code>tree</code> regression trees.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p> in <code>predict</code> a character indicating whether the response, all responses across the boosting iterations, classes, loss or classification errors should be predicted in case of <code>hinge</code> 
problems. in <code>plot</code>, plot of boosting iteration or $L_1$ norm. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p> class of <code>mbst</code>. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>newdata</code></td>
<td>
<p> new data for prediction with the same number of columns as <code>x</code>. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>newy</code></td>
<td>
<p> new response. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mstop</code></td>
<td>
<p> boosting iteration for prediction. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p> additional arguments. </p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>A linear or nonlinear classifier is fitted using a boosting algorithm for multi-class responses. This function is different from <code>mhingebst</code> on how to deal with zero-to-sum constraint and loss functions. If <code>family="hinge"</code>, the loss function is the same as in <code>mhingebst</code> but the boosting algorithm is different. If <code>family="hinge2"</code>, the loss function is different from <code>family="hinge"</code>: the response is not recoded as in Wang (2012). In this case, the loss function is  
</p>
<p style="text-align: center;"><code class="reqn">\sum{I(y_i \neq j)(f_j+1)_+}.</code>
</p>
 <p><code>family="thingeDC"</code> for robust loss function used in the DCB algorithm.
</p>


<h3>Value</h3>

<p>An object of class <code>mbst</code> with <code>print</code>, <code>coef</code>,
<code>plot</code> and <code>predict</code> methods are available for linear models.
For nonlinear models, methods <code>print</code> and <code>predict</code> are available.
</p>
<table>
<tr style="vertical-align: top;">
<td><code>x, y, cost, family, learner, control.tree, maxdepth</code></td>
<td>
<p>These are input variables and parameters</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ctrl</code></td>
<td>
<p>the input <code>ctrl</code> with possible updated <code>fk</code> if <code>family="thingeDC"</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>yhat</code></td>
<td>
<p>predicted function estimates</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ens</code></td>
<td>
<p>a list of length <code>mstop</code>. Each element is a fitted model to the pseudo residuals, defined as negative gradient of loss function at the current estimated function</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ml.fit</code></td>
<td>
<p>the last element of <code>ens</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ensemble</code></td>
<td>
<p>a vector of length <code>mstop</code>. Each element is the variable selected in each boosting step when applicable</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xselect</code></td>
<td>
<p>selected variables in <code>mstop</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>coef</code></td>
<td>
<p>estimated coefficients in each iteration. Used internally only</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p> Zhu Wang </p>


<h3>References</h3>

<p>Zhu Wang (2011),
HingeBoost: ROC-Based Boost for Classification and Variable Selection.
<em>The International Journal of Biostatistics</em>, <b>7</b>(1), Article 13.
</p>
<p>Zhu Wang (2012), Multi-class HingeBoost: Method and Application to the Classification of Cancer Types Using Gene Expression Data. <em>Methods of Information in Medicine</em>, <b>51</b>(2), 162â€“7.
</p>


<h3>See Also</h3>

<p><code>cv.mbst</code> for cross-validated stopping iteration. Furthermore see
<code>bst_control</code></p>


<h3>Examples</h3>

<pre><code class="language-R">x &lt;- matrix(rnorm(100*5),ncol=5)
c &lt;- quantile(x[,1], prob=c(0.33, 0.67))
y &lt;- rep(1, 100)
y[x[,1] &gt; c[1] &amp; x[,1] &lt; c[2] ] &lt;- 2
y[x[,1] &gt; c[2]] &lt;- 3
x &lt;- as.data.frame(x)
dat.m &lt;- mbst(x, y, ctrl = bst_control(mstop=50), family = "hinge", learner = "ls")
predict(dat.m)
dat.m1 &lt;- mbst(x, y, ctrl = bst_control(twinboost=TRUE, 
f.init=predict(dat.m), xselect.init = dat.m$xselect, mstop=50))
dat.m2 &lt;- rmbst(x, y, ctrl = bst_control(mstop=50, s=1, trace=TRUE), 
rfamily = "thinge", learner = "ls")
predict(dat.m2)
</code></pre>


</div>