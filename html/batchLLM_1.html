<div class="container">

<table style="width: 100%;"><tr>
<td>batchLLM</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Batch Process LLM Text Completions Using a Data Frame</h2>

<h3>Description</h3>

<p>Batch process large language model (LLM) text completions by looping across the rows of a data frame column.
The package currently supports OpenAI's GPT, Anthropic's Claude, and Google's Gemini models, with built-in delays for API rate limiting.
The package provides advanced text processing features, including automatic logging of batches and metadata to local files, side-by-side comparison of outputs from different LLMs, and integration of a user-friendly Shiny App Addin.
Use cases include natural language processing tasks such as sentiment analysis, thematic analysis, classification, labeling or tagging, and language translation.
</p>


<h3>Usage</h3>

<pre><code class="language-R">batchLLM(
  df,
  df_name = NULL,
  col,
  prompt,
  LLM = "openai",
  model = "gpt-4o-mini",
  temperature = 0.5,
  max_tokens = 500,
  batch_delay = "random",
  batch_size = 10,
  case_convert = NULL,
  sanitize = FALSE,
  attempts = 1,
  log_name = "batchLLM-log",
  hash_algo = "crc32c",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>df</code></td>
<td>
<p>A data frame that contains the input data.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>df_name</code></td>
<td>
<p>An optional string specifying the name of the data frame to log. This is particularly useful in Shiny applications or when the data frame is passed programmatically rather than explicitly. Default is NULL.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>col</code></td>
<td>
<p>The name of the column in the data frame to process.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prompt</code></td>
<td>
<p>A system prompt for the LLM model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>LLM</code></td>
<td>
<p>A string for the name of the LLM with the options: "openai", "anthropic", and "google". Default is "openai".</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>A string for the name of the model from the LLM. Default is "gpt-4o-mini".</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>temperature</code></td>
<td>
<p>A temperature for the LLM model. Default is .5.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_tokens</code></td>
<td>
<p>A maximum number of tokens to generate before stopping. Default is 500.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>batch_delay</code></td>
<td>
<p>A string for the batch delay with the options: "random", "min", and "sec". Numeric examples include "1min" and "30sec". Default is "random" which is an average of 10.86 seconds (n = 1,000 simulations).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>batch_size</code></td>
<td>
<p>The number of rows to process in each batch. Default is 10.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>case_convert</code></td>
<td>
<p>A string for the case conversion of the output with the options: "upper", "lower", or NULL (no change). Default is NULL.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sanitize</code></td>
<td>
<p>Extract the LLM text completion from the model's response by returning only content in <code>&lt;result&gt;</code> XML tags. Additionally, remove all punctuation. This feature prevents unwanted text (e.g., preamble) or punctuation from being included in the model's output. Default is FALSE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>attempts</code></td>
<td>
<p>The maximum number of loop retry attempts. Default is 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>log_name</code></td>
<td>
<p>A string for the name of the log without the <code>.rds</code> file extension. Default is "batchLLM-log".</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>hash_algo</code></td>
<td>
<p>A string for a hashing algorithm from the 'digest' package. Default is <code>crc32c</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Additional arguments to pass on to the LLM API function.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>Returns the input data frame with an additional column containing the text completion output.
The function also writes the output and metadata to the log file after each batch in a nested list format.
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
library(batchLLM)

# Set API keys
Sys.setenv(OPENAI_API_KEY = "your_openai_api_key")
Sys.setenv(ANTHROPIC_API_KEY = "your_anthropic_api_key")
Sys.setenv(GEMINI_API_KEY = "your_gemini_api_key")

# Define LLM configurations
llm_configs &lt;- list(
  list(LLM = "openai", model = "gpt-4o-mini"),
  list(LLM = "anthropic", model = "claude-3-haiku-20240307"),
  list(LLM = "google", model = "1.5-flash")
)

# Apply batchLLM function to each configuration
beliefs &lt;- lapply(llm_configs, function(config) {
  batchLLM(
    df = beliefs,
    col = statement,
    prompt = "classify as a fact or misinformation in one word",
    LLM = config$LLM,
    model = config$model,
    batch_size = 10,
    batch_delay = "1min",
    case_convert = "lower"
  )
})[[length(llm_configs)]]

# Print the updated data frame
print(beliefs)

## End(Not run)
</code></pre>


</div>