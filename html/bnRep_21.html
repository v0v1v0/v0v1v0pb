<div class="container">

<table style="width: 100%;"><tr>
<td>algorithms6</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>algorithms Bayesian Networks</h2>

<h3>Description</h3>

<p>Entropy and the Kullback-Leibler divergence for Bayesian networks: Computational complexity and efficient implementation.
</p>


<h3>Format</h3>

<p>A conditional linear Gaussian Bayesian network to illustrate the algorithms developed in the associated paper (Figure 3, bottom). The probabilities were available from a repository. The vertices are:
</p>

<dl>
<dt>X1</dt>
<dd>
<p>(a, b);</p>
</dd>
<dt>X2</dt>
<dd>
<p>(c, d);</p>
</dd>
<dt>X3</dt>
<dd>
<p>(e, f);</p>
</dd>
<dt>X4</dt>
<dd></dd>
<dt>X5</dt>
<dd></dd>
<dt>X6</dt>
<dd></dd>
</dl>
<h3>Value</h3>

<p>An object of class <code>bn.fit</code>. Refer to the documentation of <code>bnlearn</code> for details.
</p>


<h3>References</h3>

<p>Scutari, M. (2024). Entropy and the Kullback-Leibler Divergence for Bayesian Networks: Computational Complexity and Efficient Implementation. Algorithms, 17(1), 24.
</p>


</div>