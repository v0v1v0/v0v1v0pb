<div class="container">

<table style="width: 100%;"><tr>
<td>bayes_fit</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Bayesian estimation of mixture distributions</h2>

<h3>Description</h3>

<p>Estimation of a univariate mixture with unknown number of components using a sparse finite mixture Markov chain Monte Carlo (SFM MCMC) algorithm.
</p>


<h3>Usage</h3>

<pre><code class="language-R">bayes_fit(
  data,
  K,
  dist,
  priors = list(),
  nb_iter = 2000,
  burnin = nb_iter/2,
  print = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>Vector of observations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>K</code></td>
<td>
<p>Maximum number of mixture components.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dist</code></td>
<td>
<p>String indicating the distribution of the mixture components;
currently supports <code>"normal"</code>, <code>"skew_normal"</code>, <code>"poisson"</code> and <code>"shifted_poisson"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>priors</code></td>
<td>
<p>List of priors; default is an empty list which implies the following priors:<br><code>a0 = 1</code>,<br><code>A0 = 200</code>,<br><code>b0 = median(y)</code>,<br><code>B0 = (max(y) - min(y))^2</code> (normal),<br><code>D_xi = 1</code>,<br><code>D_psi =1</code>, (skew normal: <code>B0 = diag(D_xi,D_psi)</code>), <br><code>c0 = 2.5</code>,<br><code>l0 = 1.1</code> (poisson),<br><code>l0 = 5</code> (shifted poisson),<br><code>L0 = 1.1/median(y)</code>,<br><code>L0 = l0 - 1</code> (shifted poisson),<br><code>g0 = 0.5</code>,<br><code>G0 = 100*g0/c0/B0</code> (normal),<br><code>G0 = g0/(0.5*var(y))</code> (skew normal).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nb_iter</code></td>
<td>
<p>Number of MCMC iterations; default is <code>2000</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>burnin</code></td>
<td>
<p>Number of MCMC iterations used as burnin; default is <code>nb_iter/2</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>print</code></td>
<td>
<p>Showing MCMC progression ? Default is <code>TRUE</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Let <code class="reqn">y_i</code>, <code class="reqn">i=1,\dots,n</code> denote observations.
A general mixture of <code class="reqn">K</code> distributions from the same
parametric family is given by:
</p>
<p style="text-align: center;"><code class="reqn">y_i \sim \sum_{k=1}^{K}\pi_k p(\cdot|\theta_k)</code>
</p>

<p>with <code class="reqn">\sum_{k=1}^{K}\pi_k=1</code> and <code class="reqn">\pi_k\geq 0</code>, <code class="reqn">k=1, ...,K</code>.
<br><br>
The exact number of components does not have to be known <em>a priori</em>
when using an SFM MCMC approach. Rather, an upper bound is specified for the
number of components and the weights of superfluous components are shrunk
towards zero during estimation. Following Malsiner-Walli et al. (2016)
a symmetric Dirichlet prior is used for the mixture weights:
</p>
<p style="text-align: center;"><code class="reqn">\pi_k \sim \text{Dirichlet}(e_0,\dots,e_0),</code>
</p>

<p>where a Gamma hyperprior is used on the concentration parameter <code class="reqn">e_0</code>:<br><br></p>
<p style="text-align: center;"><code class="reqn">e_0 \sim \text{Gamma}\left(a_0, A_0\right).</code>
</p>

<p><strong>Mixture of Normal distributions</strong>
</p>
<p>Normal components take the form:
</p>
<p style="text-align: center;"><code class="reqn">p(y_i|\mu_k,\sigma_k) = \frac{1}{\sqrt{2 \pi} \
  \sigma_k} \exp\left( - \, \frac{1}{2}            \left(  \frac{y_i -
      \mu_k}{\sigma_k} \right)^2     \right).</code>
</p>

<p>Independent conjugate priors are used for <code class="reqn">\mu_k</code> and <code class="reqn">\sigma^2_k</code>
(see for instance Malsiner-Walli et al. 2016):
</p>
<p style="text-align: center;"><code class="reqn">\mu_k \sim \text{Normal}( \text{b}_0, \text{B}_0),</code>
</p>

<p style="text-align: center;"><code class="reqn">\sigma^{-2}_k \sim \text{Gamma}( \text{c}_0, \text{C}_0),</code>
</p>

<p style="text-align: center;"><code class="reqn">C_0 \sim \text{Gamma}( \text{g}_0, \text{G}_0).</code>
</p>

<p><strong>Mixture of skew-Normal distributions</strong>
</p>
<p>We use the skew-Normal of Azzalini (1985) which takes the form:
</p>
<p style="text-align: center;"><code class="reqn">p(y_i| \xi_k,\omega_k,\alpha_k) = \frac{1}{\omega_k\sqrt{2\pi}} \ \exp\left( - \,
\frac{1}{2}            \left(  \frac{y_i - \xi_k}{\omega_k} \right)^2\right) \
\left(1 + \text{erf}\left( \alpha_k\left(\frac{y_i - \xi_k}{\omega_k\sqrt{2}}\right)\right)\right),</code>
</p>

<p>where <code class="reqn">\xi_k</code> is a location parameter, <code class="reqn">\omega_k</code> a scale parameter and <code class="reqn">\alpha_k</code>
the shape parameter introducing skewness. For Bayesian estimation, we adopt the approach of
Frühwirth-Schnatter and Pyne (2010) and use the following reparameterised random-effect model:
</p>
<p style="text-align: center;"><code class="reqn">z_i \sim TN_{[0,\infty)}(0, 1),</code>
</p>

<p style="text-align: center;"><code class="reqn">y_i|(S_i = k) = \xi_k + \psi_k z_i + \epsilon_i, \quad \epsilon_i \sim N(0, \sigma^2_k),</code>
</p>

<p>where the parameters of the skew-Normal are recovered with
</p>
<p style="text-align: center;"><code class="reqn">\omega_k = \frac{\psi_k}{\sigma_k}, \qquad \omega^2_k = \sigma^2_k + \psi^2_k.</code>
</p>

<p>By defining a regressor <code class="reqn">x_i = (1, z_i)'</code>, the skew-Normal mixture can be seen as
random effect model and sampled using standard techniques. Thus we use priors similar to
the Normal mixture model:
</p>
<p style="text-align: center;"><code class="reqn">(\xi_k, \psi_k)' \sim \text{Normal}(\text{b}_0, \text{B}_0),</code>
</p>

<p style="text-align: center;"><code class="reqn">\sigma^{-2}_k \sim \text{Gamma}(\text{c}_0, \text{C}_0),</code>
</p>

<p style="text-align: center;"><code class="reqn">\text{C}_0 \sim \text{Gamma}( \text{g}_0, \text{G}_0).</code>
</p>

<p>We set </p>
<p style="text-align: center;"><code class="reqn">\text{b}_0 = (\text{median}(y), 0)'</code>
</p>
<p> and </p>
<p style="text-align: center;"><code class="reqn">\text{B}_0 = \text{diag}(\text{D}\_\text{xi}, \text{D}\_\text{psi})</code>
</p>
<p> with D_xi = D_psi = 1.
</p>
<p><strong>Mixture of Poisson distributions</strong>
</p>
<p>Poisson components take the form:
</p>
<p style="text-align: center;"><code class="reqn">p(y_i|\lambda_k) = \frac{1}{y_i!} \, \lambda^{y_i}_k \,\exp(-\lambda_k).</code>
</p>

<p>The prior for <code class="reqn">\lambda_k</code> follows from Viallefont et al. (2002):
</p>
<p style="text-align: center;"><code class="reqn">\lambda_k \sim \text{Gamma}(\text{l}_0,\text{L}_0).</code>
</p>

<p><strong>Mixture of shifted-Poisson distributions</strong>
</p>
<p>Shifted-Poisson components take the form
</p>
<p style="text-align: center;"><code class="reqn">p(y_i |\lambda_k, \kappa_k) = \frac{1}{(y_i - \kappa_k)!} \,
\lambda^{(y_i - \kappa_k)!}_k \,\exp(-\lambda_k)</code>
</p>

<p>where <code class="reqn">\kappa_k</code> is a location or shift parameter with uniform prior, see Cross et al. (2024).
</p>


<h3>Value</h3>

<p>A list of class <code>bayes_mixture</code> containing:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>Same as argument.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mcmc</code></td>
<td>
<p>Matrix of MCMC draws where the rows corresponding to burnin have been discarded;</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mcmc_all</code></td>
<td>
<p>Matrix of MCMC draws.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loglik</code></td>
<td>
<p>Log likelihood at each MCMC draw.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>K</code></td>
<td>
<p>Number of components.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dist</code></td>
<td>
<p>Same as argument.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pdf_func</code></td>
<td>
<p>The pdf/pmf of the mixture components.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dist_type</code></td>
<td>
<p>Type of the distribution, i.e. continuous or discrete.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pars_names</code></td>
<td>
<p>Names of the mixture components' parameters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loc</code></td>
<td>
<p>Name of the location parameter of the mixture components.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nb_var</code></td>
<td>
<p>Number of variables/parameters in the mixture distribution.</p>
</td>
</tr>
</table>
<h3>References</h3>

<p>Azzalini A (1985).
“A Class of Distributions Which Includes the Normal Ones.”
<em>Scandinavian Journal of Statistics</em>, <b>12</b>(2), 171–178.
ISSN 0303-6898, Publisher: [Board of the Foundation of the Scandinavian Journal of Statistics, Wiley].<br><br> Cross JL, Hoogerheide L, Labonne P, van Dijk HK (2024).
“Bayesian mode inference for discrete distributions in economics and finance.”
<em>Economics Letters</em>, <b>235</b>, 111579.
ISSN 0165-1765, <a href="https://doi.org/10.1016/j.econlet.2024.111579">doi:10.1016/j.econlet.2024.111579</a>.<br><br> Frühwirth-Schnatter S, Pyne S (2010).
“Bayesian inference for finite mixtures of univariate and multivariate skew-normal and skew-t distributions.”
<em>Biostatistics</em>, <b>11</b>(2), 317–336.
ISSN 1465-4644, <a href="https://doi.org/10.1093/biostatistics/kxp062">doi:10.1093/biostatistics/kxp062</a>.<br><br> Malsiner-Walli G, Fruhwirth-Schnatter S, Grun B (2016).
“Model-based clustering based on sparse finite Gaussian mixtures.”
<em>Statistics and Computing</em>, <b>26</b>(1), 303–324.
ISSN 1573-1375, <a href="https://doi.org/10.1007/s11222-014-9500-2">doi:10.1007/s11222-014-9500-2</a>.<br><br> Viallefont V, Richardson S, Peter
J (2002).
“Bayesian analysis of Poisson mixtures.”
<em>Journal of Nonparametric Statistics</em>, <b>14</b>(1-2), 181–202.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Example with galaxy data ================================================
set.seed(123) 

# retrieve galaxy data
y = galaxy

# estimation
bayesmix = bayes_fit(data = y,
                           K = 5, #not many to run the example rapidly
                           dist = "normal",
                           nb_iter = 500, #not many to run the example rapidly
                           burnin = 100)
                           
# plot estimated mixture
# plot(bayesmix, max_size = 200)

# Changing priors ================================================
set.seed(123) 

# retrieve galaxy data
y = galaxy

# estimation
K = 5
bayesmix = bayes_fit(data = y,
                           K = K, #not many to run the example rapidly
                           dist = "normal",
                           priors = list(a0 = 10,
                                         A0 = 10*K),
                           nb_iter = 500, #not many to run the example rapidly
                           burnin = 100)
                           
# plot estimated mixture
# plot(bayesmix, max_size = 200)

# Example with DNA data =====================================================

set.seed(123) 

# retrieve DNA data
y = d4z4

# estimation
bayesmix = bayes_fit(data = y,
                           K = 5, #not many to run the example rapidly
                           dist = "shifted_poisson",
                           nb_iter = 500, #not many to run the example rapidly
                           burnin = 100)
                           
# plot estimated mixture
# plot(bayesmix, max_size = 200)


</code></pre>


</div>