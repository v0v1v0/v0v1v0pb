<div class="container">

<table style="width: 100%;"><tr>
<td>kldiv</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Kullback-Leibler divergence of two multivariate normal distributions.</h2>

<h3>Description</h3>

<p>Compute the Kullback-Leiber divergence or
<em>symmetrized</em> KL-divergence based on means
and covariances of two normal distributions.
</p>


<h3>Usage</h3>

<pre><code class="language-R">  kldiv(mu1, mu2, sigma1, sigma2, symmetrized=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>mu1, mu2</code></td>
<td>
<p>the two mean vectors.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sigma1, sigma2</code></td>
<td>
<p>the two covariance matrices.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>symmetrized</code></td>
<td>
<p>logical; if <code>TRUE</code>, the <em>symmetrized</em> divergence will be returned.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The Kullback-Leibler divergence (or <em>relative entropy</em>) of two
probability distributions <code class="reqn">p</code> and <code class="reqn">q</code> is defined as the
integral
</p>
<p style="text-align: center;"><code class="reqn">D_{\mathrm{KL}}(p\,||\,q) = \int_\Theta
    \log\Bigl(\frac{p(\theta)}{q(\theta)}\Bigr)\, p(\theta)\,
    \mathrm{d}\theta.</code>
</p>

<p>In the case of two normal distributions with mean and variance
parameters given by (<code class="reqn">\mu_1</code>, <code class="reqn">\Sigma_1</code>) and
(<code class="reqn">\mu_2</code>, <code class="reqn">\Sigma_2</code>), respectively, this
results as 
</p>
<p style="text-align: center;"><code class="reqn">D_{\mathrm{KL}}\bigl(p(\theta|\mu_1,\Sigma_1)\,||\,p(\theta|\mu_2,\Sigma_2)\bigr) = \frac{1}{2}\biggl(\mathrm{tr}(\Sigma_2^{-1} \Sigma_1) + (\mu_1-\mu_2)^\prime \Sigma_2^{-1} (\mu_1-\mu_2) - d + \log\Bigl(\frac{\det(\Sigma_2)}{\det(\Sigma_1)}\Bigr)\biggr)</code>
</p>

<p>where <code class="reqn">d</code> is the dimension.
</p>
<p>The <em>symmetrized</em> divergence simply results as
</p>
<p style="text-align: center;"><code class="reqn">D_{\mathrm{s}}(p\,||\,q)=D_{\mathrm{KL}}(p\,||\,q)+D_{\mathrm{KL}}(q\,||\,p).</code>
</p>



<h3>Value</h3>

<p>The divergence (<code class="reqn">D_{\mathrm{KL}} \geq 0 </code> or <code class="reqn">D_{\mathrm{s}} \geq 0 </code>).
</p>


<h3>Author(s)</h3>

<p>Christian Roever <a href="mailto:christian.roever@med.uni-goettingen.de">christian.roever@med.uni-goettingen.de</a>
</p>


<h3>References</h3>

<p>S. Kullback. <em>Information theory and statistics</em>.
John Wiley and Sons, New York, 1959.
</p>
<p>C. Roever, T. Friede.
Discrete approximation of a mixture distribution via restricted divergence.
<em>Journal of Computational and Graphical Statistics</em>,
<b>26</b>(1):217-222, 2017.
<a href="https://doi.org/10.1080/10618600.2016.1276840">doi:10.1080/10618600.2016.1276840</a>.
</p>


<h3>See Also</h3>

<p><code>bmr</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">kldiv(mu1=c(0,0), mu2=c(1,1), sigma1=diag(c(2,2)), sigma2=diag(c(3,3)))
</code></pre>


</div>