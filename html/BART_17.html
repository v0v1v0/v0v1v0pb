<div class="container">

<table style="width: 100%;"><tr>
<td>gbart</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Generalized BART for continuous and binary outcomes</h2>

<h3>Description</h3>

<p>BART is a Bayesian “sum-of-trees” model.<br>
For a numeric response <code class="reqn">y</code>, we have
<code class="reqn">y = f(x) + \epsilon</code>,
where <code class="reqn">\epsilon \sim N(0,\sigma^2)</code>.<br></p>
<p><code class="reqn">f</code> is the sum of many tree models.
The goal is to have very flexible inference for the uknown
function <code class="reqn">f</code>.
</p>
<p>In the spirit of “ensemble models”,
each tree is constrained by a prior to be a weak learner
so that it contributes a small amount to the overall fit.
</p>


<h3>Usage</h3>

<pre><code class="language-R">gbart(
      x.train, y.train,
      x.test=matrix(0,0,0), type='wbart',
      ntype=as.integer(
          factor(type, levels=c('wbart', 'pbart', 'lbart'))),
      sparse=FALSE, theta=0, omega=1,
      a=0.5, b=1, augment=FALSE, rho=NULL,
      xinfo=matrix(0,0,0), usequants=FALSE,
      rm.const=TRUE,
      sigest=NA, sigdf=3, sigquant=0.90,
      k=2, power=2, base=0.95,
      
      lambda=NA, tau.num=c(NA, 3, 6)[ntype], 
      offset=NULL, w=rep(1, length(y.train)),
      ntree=c(200L, 50L, 50L)[ntype], numcut=100L,
      
      ndpost=1000L, nskip=100L, 
      keepevery=c(1L, 10L, 10L)[ntype],
      printevery=100L, transposed=FALSE,
      hostname=FALSE,
      mc.cores = 1L, ## mc.gbart only
      nice = 19L,    ## mc.gbart only
      seed = 99L     ## mc.gbart only
)

mc.gbart(
         x.train, y.train,
         x.test=matrix(0,0,0), type='wbart',
         ntype=as.integer(
             factor(type, levels=c('wbart', 'pbart', 'lbart'))),
         sparse=FALSE, theta=0, omega=1,
         a=0.5, b=1, augment=FALSE, rho=NULL,
         xinfo=matrix(0,0,0), usequants=FALSE,
         rm.const=TRUE,
         sigest=NA, sigdf=3, sigquant=0.90,
         k=2, power=2, base=0.95,
         
         lambda=NA, tau.num=c(NA, 3, 6)[ntype], 
         offset=NULL, w=rep(1, length(y.train)),
         
         ntree=c(200L, 50L, 50L)[ntype], numcut=100L,
         ndpost=1000L, nskip=100L, 
         keepevery=c(1L, 10L, 10L)[ntype],
         printevery=100L, transposed=FALSE,
         hostname=FALSE,
         mc.cores = 2L, nice = 19L, seed = 99L
)

</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x.train</code></td>
<td>
<p> Explanatory variables for training (in sample)
data.<br> May be a matrix or a data frame, with (as usual) rows
corresponding to observations and columns to variables.<br> If a
variable is a factor in a data frame, it is replaced with dummies.
Note that <code class="reqn">q</code> dummies are created if <code class="reqn">q&gt;2</code> and one dummy
created if <code class="reqn">q=2</code> where <code class="reqn">q</code> is the number of levels of the
factor.  <code>gbart</code> will generate draws of <code class="reqn">f(x)</code> for each
<code class="reqn">x</code> which is a row of <code>x.train</code>.  </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y.train</code></td>
<td>

<p>Continuous or binary dependent variable for training (in sample) data.<br>
If <code class="reqn">y</code> is numeric, then a continuous BART model is fit (Normal errors).<br>
If <code class="reqn">y</code> is binary (has only 0's and 1's), then a binary BART model
with a probit link is fit by default: you can over-ride the default via the
argument <code>type</code> to specify a logit BART model.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x.test</code></td>
<td>
<p> Explanatory variables for test (out of sample)
data. Should have same structure as <code>x.train</code>.
<code>gbart</code> will generate draws of <code class="reqn">f(x)</code> for each <code class="reqn">x</code> which
is a row of <code>x.test</code>.  </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p> You can use this argument to specify the type of fit.
<code>'wbart'</code> for continuous BART, <code>'pbart'</code> for probit BART or
<code>'lbart'</code> for logit BART. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ntype</code></td>
<td>
<p> The integer equivalent of <code>type</code> where
<code>'wbart'</code> is 1, <code>'pbart'</code> is 2 and
<code>'lbart'</code> is 3.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sparse</code></td>
<td>
<p>Whether to perform variable selection based on a
sparse Dirichlet prior rather than simply uniform; see Linero 2016.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>theta</code></td>
<td>
<p>Set <code class="reqn">theta</code> parameter; zero means random.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>omega</code></td>
<td>
<p>Set <code class="reqn">omega</code> parameter; zero means random.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>a</code></td>
<td>
<p>Sparse parameter for <code class="reqn">Beta(a, b)</code> prior:
<code class="reqn">0.5&lt;=a&lt;=1</code> where lower values inducing more sparsity.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>b</code></td>
<td>
<p>Sparse parameter for <code class="reqn">Beta(a, b)</code> prior; typically,
<code class="reqn">b=1</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rho</code></td>
<td>
<p>Sparse parameter: typically <code class="reqn">rho=p</code> where <code class="reqn">p</code> is the
number of covariates under consideration.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>augment</code></td>
<td>
<p>Whether data augmentation is to be performed in sparse
variable selection.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xinfo</code></td>
<td>
<p> You can provide the cutpoints to BART or let BART
choose them for you.  To provide them, use the <code>xinfo</code>
argument to specify a list (matrix) where the items (rows) are the
covariates and the contents of the items (columns) are the
cutpoints.  </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>usequants</code></td>
<td>
<p> If <code>usequants=FALSE</code>, then the
cutpoints in <code>xinfo</code> are generated uniformly; otherwise,
if <code>TRUE</code>, uniform quantiles are used for the cutpoints. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rm.const</code></td>
<td>
<p> Whether or not to remove constant variables.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sigest</code></td>
<td>
<p> The prior for the error variance
(<code class="reqn">sigma^2</code>) is inverted chi-squared (the standard
conditionally conjugate prior).  The prior is specified by choosing
the degrees of freedom, a rough estimate of the corresponding
standard deviation and a quantile to put this rough estimate at.  If
<code>sigest=NA</code> then the rough estimate will be the usual least squares
estimator.  Otherwise the supplied value will be used.
Not used if <code class="reqn">y</code> is binary.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sigdf</code></td>
<td>

<p>Degrees of freedom for error variance prior.
Not used if <code class="reqn">y</code> is binary.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sigquant</code></td>
<td>
<p> The quantile of the prior that the rough estimate
(see <code>sigest</code>) is placed at.  The closer the quantile is to 1, the more
aggresive the fit will be as you are putting more prior weight on
error standard deviations (<code class="reqn">sigma</code>) less than the rough
estimate.  Not used if <code class="reqn">y</code> is binary.  </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p> For numeric <code class="reqn">y</code>, <code>k</code> is the number of prior
standard deviations <code class="reqn">E(Y|x) = f(x)</code> is away from +/-0.5.  For
binary <code class="reqn">y</code>, <code>k</code> is the number of prior standard deviations
<code class="reqn">f(x)</code> is away from +/-3.  The bigger <code>k</code> is, the more
conservative the fitting will be.  </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>power</code></td>
<td>

<p>Power parameter for tree prior.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>base</code></td>
<td>

<p>Base parameter for tree prior.
</p>
</td>
</tr>
</table>
<table>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>

<p>The scale of the prior for the variance.  If <code>lambda</code> is zero,
then the variance is to be considered fixed and known at the given
value of <code>sigest</code>.  Not used if <code class="reqn">y</code> is binary.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tau.num</code></td>
<td>
<p> The numerator in the <code>tau</code> definition, i.e.,
<code>tau=tau.num/(k*sqrt(ntree))</code>. </p>
</td>
</tr>
</table>
<table>
<tr style="vertical-align: top;">
<td><code>offset</code></td>
<td>
<p> Continous BART operates on <code>y.train</code> centered by
<code>offset</code> which defaults to <code>mean(y.train)</code>.  With binary
BART, the centering is <code class="reqn">P(Y=1 | x) = F(f(x) + offset)</code> where
<code>offset</code> defaults to <code>F^{-1}(mean(y.train))</code>.  You can use
the <code>offset</code> parameter to over-ride these defaults.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>w</code></td>
<td>
<p> Vector of weights which multiply the standard deviation.
Not used if <code class="reqn">y</code> is binary.  </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ntree</code></td>
<td>

<p>The number of trees in the sum.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>numcut</code></td>
<td>
<p> The number of possible values of <code class="reqn">c</code> (see
<code>usequants</code>).  If a single number if given, this is used for all
variables.  Otherwise a vector with length equal to
<code>ncol(x.train)</code> is required, where the <code class="reqn">i^{th}</code>
element gives the number of <code class="reqn">c</code> used for the <code class="reqn">i^{th}</code>
variable in <code>x.train</code>.  If usequants is false, numcut equally
spaced cutoffs are used covering the range of values in the
corresponding column of <code>x.train</code>.  If <code>usequants</code> is true, then
<code class="reqn">min(numcut, the number of unique values in the corresponding
   columns of x.train - 1)</code> values are used.  </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ndpost</code></td>
<td>

<p>The number of posterior draws returned.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nskip</code></td>
<td>

<p>Number of MCMC iterations to be treated as burn in.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>printevery</code></td>
<td>

<p>As the MCMC runs, a message is printed every printevery draws.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>keepevery</code></td>
<td>

<p>Every keepevery draw is kept to be returned to the user.<br></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>transposed</code></td>
<td>

<p>When running <code>gbart</code> in parallel, it is more memory-efficient
to transpose <code>x.train</code> and <code>x.test</code>, if any, prior to
calling <code>mc.gbart</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>hostname</code></td>
<td>

<p>When running on a cluster occasionally it is useful
to track on which node each chain is running; to do so
set this argument to <code>TRUE</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>

<p>Setting the seed required for reproducible MCMC.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mc.cores</code></td>
<td>

<p>Number of cores to employ in parallel.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nice</code></td>
<td>

<p>Set the job niceness.  The default
niceness is 19: niceness goes from 0 (highest) to 19 (lowest).
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>BART is a Bayesian MCMC method.
At each MCMC interation, we produce a draw from the joint posterior
<code class="reqn">(f,\sigma) | (x,y)</code> in the numeric <code class="reqn">y</code> case
and just <code class="reqn">f</code> in the binary <code class="reqn">y</code> case.
</p>
<p>Thus, unlike a lot of other modelling methods in R, we do not produce
a single model object from which fits and summaries may be extracted.
The output consists of values <code class="reqn">f^*(x)</code> (and
<code class="reqn">\sigma^*</code> in the numeric case) where * denotes a
particular draw.  The <code class="reqn">x</code> is either a row from the training data,
<code>x.train</code> or the test data, <code>x.test</code>.
</p>
<p>For <code>x.train</code>/<code>x.test</code> with missing data elements, <code>gbart</code>
will singly impute them with hot decking. For one or more missing
covariates, record-level hot-decking imputation <cite>deWaPann11</cite> is
employed that is biased towards the null, i.e., nonmissing values
from another record are randomly selected regardless of the
outcome. Since <code>mc.gbart</code> runs multiple <code>gbart</code> threads in
parallel, <code>mc.gbart</code> performs multiple imputation with hot
decking, i.e., a separate imputation for each thread.  This
record-level hot-decking imputation is biased towards the null, i.e.,
nonmissing values from another record are randomly selected
regardless of <code>y.train</code>.
</p>


<h3>Value</h3>







<p><code>gbart</code> returns an object of type <code>gbart</code> which is
essentially a list. 
In the numeric <code class="reqn">y</code> case, the list has components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>yhat.train</code></td>
<td>

<p>A matrix with ndpost rows and nrow(x.train) columns.
Each row corresponds to a draw <code class="reqn">f^*</code> from the posterior of <code class="reqn">f</code>
and each column corresponds to a row of x.train.
The <code class="reqn">(i,j)</code> value is <code class="reqn">f^*(x)</code> for the <code class="reqn">i^{th}</code> kept draw of <code class="reqn">f</code>
and the <code class="reqn">j^{th}</code> row of x.train.<br>
Burn-in is dropped.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>yhat.test</code></td>
<td>
<p>Same as yhat.train but now the x's are the rows of the test data.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>yhat.train.mean</code></td>
<td>
<p>train data fits = mean of yhat.train columns.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>yhat.test.mean</code></td>
<td>
<p>test data fits = mean of yhat.test columns.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sigma</code></td>
<td>
<p>post burn in draws of sigma, length = ndpost.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>first.sigma</code></td>
<td>
<p>burn-in draws of sigma.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>varcount</code></td>
<td>
<p>a matrix with ndpost rows and nrow(x.train) columns.
Each row is for a draw. For each variable (corresponding to the columns),
the total count of the number of times
that variable is used in a tree decision rule (over all trees) is given.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sigest</code></td>
<td>

<p>The rough error standard deviation (<code class="reqn">\sigma</code>) used in the prior.
</p>
</td>
</tr>
</table>
<h3>See Also</h3>

<p><code>pbart</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">##simulate data (example from Friedman MARS paper)
f = function(x){
10*sin(pi*x[,1]*x[,2]) + 20*(x[,3]-.5)^2+10*x[,4]+5*x[,5]
}
sigma = 1.0  #y = f(x) + sigma*z , z~N(0,1)
n = 100      #number of observations
set.seed(99)
x=matrix(runif(n*10),n,10) #10 variables, only first 5 matter
Ey = f(x)
y=Ey+sigma*rnorm(n)
lmFit = lm(y~.,data.frame(x,y)) #compare lm fit to BART later

##test BART with token run to ensure installation works
set.seed(99)
bartFit = wbart(x,y,nskip=5,ndpost=5)

## Not run: 
##run BART
set.seed(99)
bartFit = wbart(x,y)

##compare BART fit to linear matter and truth = Ey
fitmat = cbind(y,Ey,lmFit$fitted,bartFit$yhat.train.mean)
colnames(fitmat) = c('y','Ey','lm','bart')
print(cor(fitmat))

## End(Not run)
</code></pre>


</div>