<div class="container">

<table style="width: 100%;"><tr>
<td>predict</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Forecasting Multivariate Time Series</h2>

<h3>Description</h3>

<p>Forecasts multivariate time series using given model.
</p>


<h3>Usage</h3>

<pre><code class="language-R">## S3 method for class 'varlse'
predict(object, n_ahead, level = 0.05, ...)

## S3 method for class 'vharlse'
predict(object, n_ahead, level = 0.05, ...)

## S3 method for class 'bvarmn'
predict(object, n_ahead, n_iter = 100L, level = 0.05, num_thread = 1, ...)

## S3 method for class 'bvharmn'
predict(object, n_ahead, n_iter = 100L, level = 0.05, num_thread = 1, ...)

## S3 method for class 'bvarflat'
predict(object, n_ahead, n_iter = 100L, level = 0.05, num_thread = 1, ...)

## S3 method for class 'bvarssvs'
predict(object, n_ahead, level = 0.05, ...)

## S3 method for class 'bvharssvs'
predict(object, n_ahead, level = 0.05, ...)

## S3 method for class 'bvarhs'
predict(object, n_ahead, level = 0.05, ...)

## S3 method for class 'bvharhs'
predict(object, n_ahead, level = 0.05, ...)

## S3 method for class 'bvarldlt'
predict(
  object,
  n_ahead,
  level = 0.05,
  num_thread = 1,
  sparse = FALSE,
  warn = FALSE,
  ...
)

## S3 method for class 'bvharldlt'
predict(
  object,
  n_ahead,
  level = 0.05,
  num_thread = 1,
  sparse = FALSE,
  warn = FALSE,
  ...
)

## S3 method for class 'bvarsv'
predict(
  object,
  n_ahead,
  level = 0.05,
  num_thread = 1,
  use_sv = TRUE,
  sparse = FALSE,
  warn = FALSE,
  ...
)

## S3 method for class 'bvharsv'
predict(
  object,
  n_ahead,
  level = 0.05,
  num_thread = 1,
  use_sv = TRUE,
  sparse = FALSE,
  warn = FALSE,
  ...
)

## S3 method for class 'predbvhar'
print(x, digits = max(3L, getOption("digits") - 3L), ...)

is.predbvhar(x)

## S3 method for class 'predbvhar'
knit_print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>Model object</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_ahead</code></td>
<td>
<p>step to forecast</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>level</code></td>
<td>
<p>Specify alpha of confidence interval level 100(1 - alpha) percentage. By default, .05.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>not used</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_iter</code></td>
<td>
<p>Number to sample residual matrix from inverse-wishart distribution. By default, 100.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>num_thread</code></td>
<td>
<p>Number of threads</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sparse</code></td>
<td>
<p><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental"><img src="../help/figures/lifecycle-experimental.svg" alt="[Experimental]"></a> Apply restriction. By default, <code>FALSE</code>.
Give CI level (e.g. <code>.05</code>) instead of <code>TRUE</code> to use credible interval across MCMC for restriction.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>warn</code></td>
<td>
<p>Give warning for stability of each coefficients record. By default, <code>FALSE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>use_sv</code></td>
<td>
<p>Use SV term</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p><code>predbvhar</code> object</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>digits</code></td>
<td>
<p>digit option to print</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p><code>predbvhar</code> class with the following components:
</p>

<dl>
<dt>process</dt>
<dd>
<p>object$process</p>
</dd>
<dt>forecast</dt>
<dd>
<p>forecast matrix</p>
</dd>
<dt>se</dt>
<dd>
<p>standard error matrix</p>
</dd>
<dt>lower</dt>
<dd>
<p>lower confidence interval</p>
</dd>
<dt>upper</dt>
<dd>
<p>upper confidence interval</p>
</dd>
<dt>lower_joint</dt>
<dd>
<p>lower CI adjusted (Bonferroni)</p>
</dd>
<dt>upper_joint</dt>
<dd>
<p>upper CI adjusted (Bonferroni)</p>
</dd>
<dt>y</dt>
<dd>
<p>object$y</p>
</dd>
</dl>
<h3>n-step ahead forecasting VAR(p)</h3>

<p>See pp35 of Lütkepohl (2007).
Consider h-step ahead forecasting (e.g. n + 1, ... n + h).
</p>
<p>Let <code class="reqn">y_{(n)}^T = (y_n^T, ..., y_{n - p + 1}^T, 1)</code>.
Then one-step ahead (point) forecasting:
</p>
<p style="text-align: center;"><code class="reqn">\hat{y}_{n + 1}^T = y_{(n)}^T \hat{B}</code>
</p>

<p>Recursively, let <code class="reqn">\hat{y}_{(n + 1)}^T = (\hat{y}_{n + 1}^T, y_n^T, ..., y_{n - p + 2}^T, 1)</code>.
Then two-step ahead (point) forecasting:
</p>
<p style="text-align: center;"><code class="reqn">\hat{y}_{n + 2}^T = \hat{y}_{(n + 1)}^T \hat{B}</code>
</p>

<p>Similarly, h-step ahead (point) forecasting:
</p>
<p style="text-align: center;"><code class="reqn">\hat{y}_{n + h}^T = \hat{y}_{(n + h - 1)}^T \hat{B}</code>
</p>

<p>How about confident region?
Confidence interval at h-period is
</p>
<p style="text-align: center;"><code class="reqn">y_{k,t}(h) \pm z_(\alpha / 2) \sigma_k (h)</code>
</p>

<p>Joint forecast region of <code class="reqn">100(1-\alpha)</code>% can be computed by
</p>
<p style="text-align: center;"><code class="reqn">\{ (y_{k, 1}, y_{k, h}) \mid y_{k, n}(i) - z_{(\alpha / 2h)} \sigma_n(i) \le y_{n, i} \le y_{k, n}(i) + z_{(\alpha / 2h)} \sigma_k(i), i = 1, \ldots, h \}</code>
</p>

<p>See the pp41 of Lütkepohl (2007).
</p>
<p>To compute covariance matrix, it needs VMA representation:
</p>
<p style="text-align: center;"><code class="reqn">Y_{t}(h) = c + \sum_{i = h}^{\infty} W_{i} \epsilon_{t + h - i} = c + \sum_{i = 0}^{\infty} W_{h + i} \epsilon_{t - i}</code>
</p>

<p>Then
</p>
<p style="text-align: center;"><code class="reqn">\Sigma_y(h) = MSE [ y_t(h) ] = \sum_{i = 0}^{h - 1} W_i \Sigma_{\epsilon} W_i^T = \Sigma_y(h - 1) + W_{h - 1} \Sigma_{\epsilon} W_{h - 1}^T</code>
</p>



<h3>n-step ahead forecasting VHAR</h3>

<p>Let <code class="reqn">T_{HAR}</code> is VHAR linear transformation matrix.
Since VHAR is the linearly transformed VAR(22),
let <code class="reqn">y_{(n)}^T = (y_n^T, y_{n - 1}^T, ..., y_{n - 21}^T, 1)</code>.
</p>
<p>Then one-step ahead (point) forecasting:
</p>
<p style="text-align: center;"><code class="reqn">\hat{y}_{n + 1}^T = y_{(n)}^T T_{HAR} \hat{\Phi}</code>
</p>

<p>Recursively, let <code class="reqn">\hat{y}_{(n + 1)}^T = (\hat{y}_{n + 1}^T, y_n^T, ..., y_{n - 20}^T, 1)</code>.
Then two-step ahead (point) forecasting:
</p>
<p style="text-align: center;"><code class="reqn">\hat{y}_{n + 2}^T = \hat{y}_{(n + 1)}^T T_{HAR} \hat{\Phi}</code>
</p>

<p>and h-step ahead (point) forecasting:
</p>
<p style="text-align: center;"><code class="reqn">\hat{y}_{n + h}^T = \hat{y}_{(n + h - 1)}^T T_{HAR} \hat{\Phi}</code>
</p>



<h3>n-step ahead forecasting BVAR(p) with minnesota prior</h3>

<p>Point forecasts are computed by posterior mean of the parameters.
See Section 3 of Bańbura et al. (2010).
</p>
<p>Let <code class="reqn">\hat{B}</code> be the posterior MN mean
and let <code class="reqn">\hat{V}</code> be the posterior MN precision.
</p>
<p>Then predictive posterior for each step
</p>
<p style="text-align: center;"><code class="reqn">y_{n + 1} \mid \Sigma_e, y \sim N( vec(y_{(n)}^T A), \Sigma_e \otimes (1 + y_{(n)}^T \hat{V}^{-1} y_{(n)}) )</code>
</p>

<p style="text-align: center;"><code class="reqn">y_{n + 2} \mid \Sigma_e, y \sim N( vec(\hat{y}_{(n + 1)}^T A), \Sigma_e \otimes (1 + \hat{y}_{(n + 1)}^T \hat{V}^{-1} \hat{y}_{(n + 1)}) )</code>
</p>

<p>and recursively,
</p>
<p style="text-align: center;"><code class="reqn">y_{n + h} \mid \Sigma_e, y \sim N( vec(\hat{y}_{(n + h - 1)}^T A), \Sigma_e \otimes (1 + \hat{y}_{(n + h - 1)}^T \hat{V}^{-1} \hat{y}_{(n + h - 1)}) )</code>
</p>



<h3>n-step ahead forecasting BVHAR</h3>

<p>Let <code class="reqn">\hat\Phi</code> be the posterior MN mean
and let <code class="reqn">\hat\Psi</code> be the posterior MN precision.
</p>
<p>Then predictive posterior for each step
</p>
<p style="text-align: center;"><code class="reqn">y_{n + 1} \mid \Sigma_e, y \sim N( vec(y_{(n)}^T \tilde{T}^T \Phi), \Sigma_e \otimes (1 + y_{(n)}^T \tilde{T} \hat\Psi^{-1} \tilde{T} y_{(n)}) )</code>
</p>

<p style="text-align: center;"><code class="reqn">y_{n + 2} \mid \Sigma_e, y \sim N( vec(y_{(n + 1)}^T \tilde{T}^T \Phi), \Sigma_e \otimes (1 + y_{(n + 1)}^T \tilde{T} \hat\Psi^{-1} \tilde{T} y_{(n + 1)}) )</code>
</p>

<p>and recursively,
</p>
<p style="text-align: center;"><code class="reqn">y_{n + h} \mid \Sigma_e, y \sim N( vec(y_{(n + h - 1)}^T \tilde{T}^T \Phi), \Sigma_e \otimes (1 + y_{(n + h - 1)}^T \tilde{T} \hat\Psi^{-1} \tilde{T} y_{(n + h - 1)}) )</code>
</p>



<h3>n-step ahead forecasting VAR(p) with SSVS and Horseshoe</h3>

<p>The process of the computing point estimate is the same.
However, predictive interval is achieved from each Gibbs sampler sample.
</p>
<p style="text-align: center;"><code class="reqn">y_{n + 1} \mid A, \Sigma_e, y \sim N( vec(y_{(n)}^T A), \Sigma_e )</code>
</p>

<p style="text-align: center;"><code class="reqn">y_{n + h} \mid A, \Sigma_e, y \sim N( vec(\hat{y}_{(n + h - 1)}^T A), \Sigma_e )</code>
</p>



<h3>n-step ahead forecasting VHAR with SSVS and Horseshoe</h3>

<p>The process of the computing point estimate is the same.
However, predictive interval is achieved from each Gibbs sampler sample.
</p>
<p style="text-align: center;"><code class="reqn">y_{n + 1} \mid \Sigma_e, y \sim N( vec(y_{(n)}^T \tilde{T}^T \Phi), \Sigma_e \otimes (1 + y_{(n)}^T \tilde{T} \hat\Psi^{-1} \tilde{T} y_{(n)}) )</code>
</p>

<p style="text-align: center;"><code class="reqn">y_{n + h} \mid \Sigma_e, y \sim N( vec(y_{(n + h - 1)}^T \tilde{T}^T \Phi), \Sigma_e \otimes (1 + y_{(n + h - 1)}^T \tilde{T} \hat\Psi^{-1} \tilde{T} y_{(n + h - 1)}) )</code>
</p>



<h3>References</h3>

<p>Lütkepohl, H. (2007). <em>New Introduction to Multiple Time Series Analysis</em>. Springer Publishing.
</p>
<p>Corsi, F. (2008). <em>A Simple Approximate Long-Memory Model of Realized Volatility</em>. Journal of Financial Econometrics, 7(2), 174-196.
</p>
<p>Baek, C. and Park, M. (2021). <em>Sparse vector heterogeneous autoregressive modeling for realized volatility</em>. J. Korean Stat. Soc. 50, 495-510.
</p>
<p>Bańbura, M., Giannone, D., &amp; Reichlin, L. (2010). <em>Large Bayesian vector auto regressions</em>. Journal of Applied Econometrics, 25(1).
</p>
<p>Gelman, A., Carlin, J. B., Stern, H. S., &amp; Rubin, D. B. (2013). <em>Bayesian data analysis</em>. Chapman and Hall/CRC.
</p>
<p>Karlsson, S. (2013). <em>Chapter 15 Forecasting with Bayesian Vector Autoregression</em>. Handbook of Economic Forecasting, 2, 791-897.
</p>
<p>Litterman, R. B. (1986). <em>Forecasting with Bayesian Vector Autoregressions: Five Years of Experience</em>. Journal of Business &amp; Economic Statistics, 4(1), 25.
</p>
<p>Ghosh, S., Khare, K., &amp; Michailidis, G. (2018). <em>High-Dimensional Posterior Consistency in Bayesian Vector Autoregressive Models</em>. Journal of the American Statistical Association, 114(526).
</p>
<p>George, E. I., Sun, D., &amp; Ni, S. (2008). <em>Bayesian stochastic search for VAR model restrictions</em>. Journal of Econometrics, 142(1), 553-580.
</p>
<p>George, E. I., Sun, D., &amp; Ni, S. (2008). <em>Bayesian stochastic search for VAR model restrictions</em>. Journal of Econometrics, 142(1), 553-580.
</p>
<p>Korobilis, D. (2013). <em>VAR FORECASTING USING BAYESIAN VARIABLE SELECTION</em>. Journal of Applied Econometrics, 28(2).
</p>
<p>Korobilis, D. (2013). <em>VAR FORECASTING USING BAYESIAN VARIABLE SELECTION</em>. Journal of Applied Econometrics, 28(2).
</p>
<p>Huber, F., Koop, G., &amp; Onorante, L. (2021). <em>Inducing Sparsity and Shrinkage in Time-Varying Parameter Models</em>. Journal of Business &amp; Economic Statistics, 39(3), 669-683.
</p>


</div>