<div class="container">

<table style="width: 100%;"><tr>
<td>bark-deprecated</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>NonParametric Regression using Bayesian Additive Regression Kernels</h2>

<h3>Description</h3>

<p>BARK is a Bayesian <em>sum-of-kernels</em> model.<br>
For numeric response <code class="reqn">y</code>, we have
<code class="reqn">y = f(x) + \epsilon</code>,
where <code class="reqn">\epsilon \sim N(0,\sigma^2)</code>.<br>
For a binary response <code class="reqn">y</code>, <code class="reqn">P(Y=1 | x) = F(f(x))</code>,
where <code class="reqn">F</code>
denotes the standard normal cdf (probit link).
<br>
In both cases, <code class="reqn">f</code> is the sum of many Gaussian kernel functions.
The goal is to have very flexible inference for the unknown
function <code class="reqn">f</code>.
BARK uses an approximation to a Cauchy process as the prior distribution
for the unknown function <code class="reqn">f</code>.
</p>
<p>Feature selection can be achieved through the inference
on the scale parameters in the Gaussian kernels.
BARK accepts four different types of prior distributions,
<em>e</em>, <em>d</em>, enabling
either soft shrinkage or  <em>se</em>, <em>sd</em>, enabling hard shrinkage for the scale
parameters.
</p>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x.train</code></td>
<td>
<p>Explanatory variables for training (in sample) data.<br>
Must be a matrix of doubles,
with (as usual) rows corresponding to observations
and columns to variables.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y.train</code></td>
<td>
<p>Dependent variable for training (in sample) data.<br>
If y is numeric a continuous response model is fit (normal errors).<br>
If y is a logical (or just has values 0 and 1),
then a binary response model with a probit link is fit.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x.test</code></td>
<td>
<p>Explanatory variables for test (out of sample) data.<br>
Should have same structure as x.train.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p>BARK type, <em>e</em>, <em>d</em>, <em>se</em>, or <em>sd</em>, default
choice is <em>se</em>.<br><em>e</em>: BARK with equal weights.<br><em>d</em>: BARK with different weights.<br><em>se</em>: BARK with selection and equal weights.<br><em>sd</em>: BARK with selection and different weights.<br></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>classification</code></td>
<td>
<p>TRUE/FALSE logical variable,
indicating a classification or regression problem.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>keepevery</code></td>
<td>
<p>Every keepevery draw is kept to be returned to the user</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nburn</code></td>
<td>
<p>Number of MCMC iterations (nburn*keepevery)
to be treated as burn in.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nkeep</code></td>
<td>
<p>Number of MCMC iterations kept for the posterior inference.<br>
nkeep*keepevery iterations after the burn in.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>printevery</code></td>
<td>
<p>As the MCMC runs, a message is printed every printevery draws.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>keeptrain</code></td>
<td>
<p>Logical, whether to keep results for training samples.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fixed</code></td>
<td>
<p>A list of fixed hyperparameters, using the default values if not
specified.<br>
alpha = 1: stable index, must be 1 currently.<br>
eps = 0.5: approximation parameter.<br>
gam = 5: intensity parameter.<br>
la = 1: first argument of the gamma prior on kernel scales.<br>
lb = 2: second argument of the gamma prior on kernel scales.<br>
pbetaa = 1: first argument of the beta prior on plambda.<br>
pbetab = 1: second argument of the beta prior on plambda.<br>
n: number of training samples, automatically generates.<br>
p: number of explanatory variables, automatically generates.<br>
meanJ: the expected number of kernels, automatically generates.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tune</code></td>
<td>
<p>A list of tuning parameters, not expected to change.<br>
lstep: the stepsize of the lognormal random walk on lambda.<br>
frequL: the frequency to update L.<br>
dpow: the power on the death step.<br>
upow: the power on the update step.<br>
varphistep: the stepsize of the lognormal random walk on varphi.<br>
phistep: the stepsize of the lognormal random walk on phi.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>theta</code></td>
<td>
<p>A list of the starting values for the parameter theta,
use defaults if nothing is given.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>BARK is implemented using a Bayesian MCMC method.
At each MCMC interaction, we produce a draw from the joint posterior
distribution, i.e. a full configuration of regression coefficients,
kernel locations and kernel parameters etc.
</p>
<p>Thus, unlike a lot of other modelling methods in R,
we do not produce a single model object
from which fits and summaries may be extracted.
The output consists of values
<code class="reqn">f^*(x)</code> (and <code class="reqn">\sigma^*</code> in the numeric case)
where * denotes a particular draw.
The <code class="reqn">x</code> is either a row from the training data (x.train)
</p>


<h3>Value</h3>

<p><code>bark</code> returns a list, including:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>fixed</code></td>
<td>
<p>Fixed hyperparameters</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tune</code></td>
<td>
<p>Tuning parameters used</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>theta.last</code></td>
<td>
<p>The last set of parameters from the posterior draw</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>theta.nvec</code></td>
<td>
<p>A matrix with nrow(x.train)<code class="reqn">+1</code> rows and (nkeep) columns,
recording the  number of kernels at each training sample</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>theta.varphi</code></td>
<td>
<p> A matrix with nrow(x.train)
<code class="reqn">+1</code> rows and (nkeep) columns,
recording the precision in the normal gamma prior
distribution for the regression coefficients</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>theta.beta</code></td>
<td>
<p>A matrix with nrow(x.train)<code class="reqn">+1</code> rows and (nkeep) columns,
recording the regression coefficients</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>theta.lambda</code></td>
<td>
<p>A matrix with ncol(x.train) rows and (nkeep) columns,
recording the kernel scale parameters</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>thea.phi</code></td>
<td>
<p>The vector of length nkeep,
recording the precision in regression Gaussian noise
(1 for the classification case)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>yhat.train</code></td>
<td>
<p>A matrix with nrow(x.train) rows and (nkeep) columns.
Each column corresponds to a draw <code class="reqn">f^*</code> from
the posterior of <code class="reqn">f</code>
and each row corresponds to a row of x.train.
The <code class="reqn">(i,j)</code> value is <code class="reqn">f^*(x)</code> for
the <code class="reqn">j^{th}</code> kept draw of <code class="reqn">f</code>
and the <code class="reqn">i^{th}</code> row of x.train.<br>
For classification problems, this is the value
of the expectation for the underlying normal
random variable.<br>
Burn-in is dropped</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>yhat.test</code></td>
<td>
<p>Same as yhat.train but now the x's
are the rows of the test data</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>yhat.train.mean</code></td>
<td>
<p>train data fits = row mean of yhat.train</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>yhat.test.mean</code></td>
<td>
<p>test data fits = row mean of yhat.test</p>
</td>
</tr>
</table>
<h3>References</h3>

<p>Ouyang, Zhi (2008) Bayesian Additive Regression Kernels.
Duke University. PhD dissertation, page 58.
</p>


<h3>See Also</h3>

<p>Other bark deprecated functions: 
<code>bark-package-deprecated</code>,
<code>sim.Circle-deprecated</code>,
<code>sim.Friedman1-deprecated</code>,
<code>sim.Friedman2-deprecated</code>,
<code>sim.Friedman3-deprecated</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Simulate regression example
#  Friedman 2 data set, 200 noisy training, 1000 noise free testing
#  Out of sample MSE in SVM (default RBF): 6500 (sd. 1600)
#  Out of sample MSE in BART (default):    5300 (sd. 1000)
traindata &lt;- sim_Friedman2(200, sd=125)
testdata &lt;- sim_Friedman2(1000, sd=0)
# example with a very small number of iterations to illustrate the method
fit.bark.d &lt;- bark_mat(traindata$x, traindata$y, testdata$x,
                  nburn=10, nkeep=10, keepevery=10,
                  classification=FALSE, type="d")
boxplot(data.frame(fit.bark.d$theta.lambda))
mean((fit.bark.d$yhat.test.mean-testdata$y)^2)

 # Simulate classification example
 #  Circle 5 with 2 signals and three noisy dimensions
 #  Out of sample erorr rate in SVM (default RBF): 0.110 (sd. 0.02)
 #  Out of sample error rate in BART (default):    0.065 (sd. 0.02)
 traindata &lt;- sim_circle(200, dim=5)
 testdata &lt;- sim_circle(1000, dim=5)
 fit.bark.se &lt;- bark_mat(traindata$x, traindata$y, testdata$x, classification=TRUE, type="se")
 boxplot(data.frame(fit.bark.se$theta.lambda))
 mean((fit.bark.se$yhat.test.mean&gt;0)!=testdata$y)

</code></pre>


</div>