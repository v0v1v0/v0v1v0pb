<div class="container">

<table style="width: 100%;"><tr>
<td>predict.blockForest</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Prediction using Random Forest variants for block-structured covariate data</h2>

<h3>Description</h3>

<p>This function is to be applied to the entry 'forest' of the output of
<code>blockfor</code>. See the example section for illustration.
</p>


<h3>Usage</h3>

<pre><code class="language-R">## S3 method for class 'blockForest'
predict(
  object,
  data = NULL,
  predict.all = FALSE,
  num.trees = object$num.trees,
  type = "response",
  se.method = "infjack",
  quantiles = c(0.1, 0.5, 0.9),
  seed = NULL,
  num.threads = NULL,
  verbose = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p><code>blockForest</code> object.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>New test data of class <code>data.frame</code> or <code>gwaa.data</code> (GenABEL).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>predict.all</code></td>
<td>
<p>Return individual predictions for each tree instead of aggregated predictions for all trees. Return a matrix (sample x tree) for classification and regression, a 3d array for probability estimation (sample x class x tree) and survival (sample x time x tree).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>num.trees</code></td>
<td>
<p>Number of trees used for prediction. The first <code>num.trees</code> in the forest are used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p>Type of prediction. One of 'response', 'se', 'terminalNodes', 'quantiles' with default 'response'. See below for details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>se.method</code></td>
<td>
<p>Method to compute standard errors. One of 'jack', 'infjack' with default 'infjack'. Only applicable if type = 'se'. See below for details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>quantiles</code></td>
<td>
<p>Vector of quantiles for quantile prediction. Set <code>type = 'quantiles'</code> to use.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>
<p>Random seed. Default is <code>NULL</code>, which generates the seed from <code>R</code>. Set to <code>0</code> to ignore the <code>R</code> seed. The seed is used in case of ties in classification mode.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>num.threads</code></td>
<td>
<p>Number of threads. Default is number of CPUs available.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>Verbose output on or off.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>For <code>type = 'response'</code> (the default), the predicted classes (classification), predicted numeric values (regression), predicted probabilities (probability estimation) or survival probabilities (survival) are returned. 
For <code>type = 'se'</code>, the standard error of the predictions are returned (regression only). The jackknife-after-bootstrap or infinitesimal jackknife for bagging is used to estimate the standard errors based on out-of-bag predictions. See Wager et al. (2014) for details.
For <code>type = 'terminalNodes'</code>, the IDs of the terminal node in each tree for each observation in the given dataset are returned.
For <code>type = 'quantiles'</code>, the selected quantiles for each observation are estimated. See Meinshausen (2006) for details.
</p>
<p>If <code>type = 'se'</code> is selected, the method to estimate the variances can be chosen with <code>se.method</code>. Set <code>se.method = 'jack'</code> for jackknife-after-bootstrap and <code>se.method = 'infjack'</code> for the infinitesimal jackknife for bagging.
</p>
<p>For classification and <code>predict.all = TRUE</code>, a factor levels are returned as numerics.
To retrieve the corresponding factor levels, use <code>rf$forest$levels</code>, if <code>rf</code> is the ranger object.
</p>


<h3>Value</h3>

<p>Object of class <code>blockForest.prediction</code> with elements
</p>

<table>
<tr>
<td style="text-align: left;">
      <code>predictions</code>    </td>
<td style="text-align: left;"> Predicted classes/values (only for classification and regression)  </td>
</tr>
<tr>
<td style="text-align: left;">
      <code>unique.death.times</code> </td>
<td style="text-align: left;"> Unique death times (only for survival). </td>
</tr>
<tr>
<td style="text-align: left;">
      <code>chf</code> </td>
<td style="text-align: left;"> Estimated cumulative hazard function for each sample (only for survival). </td>
</tr>
<tr>
<td style="text-align: left;">
      <code>survival</code> </td>
<td style="text-align: left;"> Estimated survival function for each sample (only for survival). </td>
</tr>
<tr>
<td style="text-align: left;">
      <code>num.trees</code>   </td>
<td style="text-align: left;"> Number of trees. </td>
</tr>
<tr>
<td style="text-align: left;">
      <code>num.independent.variables</code> </td>
<td style="text-align: left;"> Number of independent variables. </td>
</tr>
<tr>
<td style="text-align: left;">
      <code>treetype</code>    </td>
<td style="text-align: left;"> Type of forest/tree. Classification, regression or survival. </td>
</tr>
<tr>
<td style="text-align: left;">
      <code>num.samples</code>     </td>
<td style="text-align: left;"> Number of samples.
  </td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Marvin N. Wright
</p>


<h3>References</h3>


<ul>
<li>
<p> Wright, M. N. &amp; Ziegler, A. (2017). ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R. J Stat Softw 77:1-17. <a href="https://doi.org/10.18637/jss.v077.i01">doi:10.18637/jss.v077.i01</a>.
</p>
</li>
<li>
<p> Wager, S., Hastie T., &amp; Efron, B. (2014). Confidence Intervals for Random Forests: The Jackknife and the Infinitesimal Jackknife. J Mach Learn Res 15:1625-1651. <a href="https://jmlr.org/papers/v15/wager14a.html">https://jmlr.org/papers/v15/wager14a.html</a>.
</p>
</li>
<li>
<p> Meinshausen (2006). Quantile Regression Forests. J Mach Learn Res 7:983-999. <a href="https://www.jmlr.org/papers/v7/meinshausen06a.html">https://www.jmlr.org/papers/v7/meinshausen06a.html</a>.  
</p>
</li>
</ul>
<h3>See Also</h3>

<p><code>blockForest</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R"># NOTE: There is no association between covariates and response for the
# simulated data below.
# Moreover, the input parameters of blockfor() are highly unrealistic
# (e.g., nsets = 10 is specified much too small).
# The purpose of the shown examples is merely to illustrate the
# application of predict.blockForest().


# Generate data:
################

set.seed(1234)

# Covariate matrix:
X &lt;- cbind(matrix(nrow=40, ncol=5, data=rnorm(40*5)), 
           matrix(nrow=40, ncol=30, data=rnorm(40*30, mean=1, sd=2)),
           matrix(nrow=40, ncol=100, data=rnorm(40*100, mean=2, sd=3)))
colnames(X) &lt;- paste("X", 1:ncol(X), sep="")

# Block variable (list):
block &lt;- rep(1:3, times=c(5, 30, 100))
block &lt;- lapply(1:3, function(x) which(block==x))

# Binary outcome:
ybin &lt;- factor(sample(c(0,1), size=40, replace=TRUE), levels=c(0,1))

# Survival outcome:
ysurv &lt;- cbind(rnorm(40), sample(c(0,1), size=40, replace=TRUE))



# Divide in training and test data:

Xtrain &lt;- X[1:30,]
Xtest &lt;- X[31:40,]

ybintrain &lt;- ybin[1:30]
ybintest &lt;- ybin[31:40]

ysurvtrain &lt;- ysurv[1:30,]
ysurvtest &lt;- ysurv[31:40,]




# Binary outcome: Apply algorithm to training data and obtain predictions
# for the test data:
#########################################################################

# Apply a variant to the training data:

blockforobj &lt;- blockfor(Xtrain, ybintrain, num.trees = 100, replace = TRUE, block=block,
                        nsets = 10, num.trees.pre = 50, splitrule="extratrees", 
                        block.method = "SplitWeights")
blockforobj$paramvalues


# Obtain prediction for the test data:

(predres &lt;- predict(blockforobj$forest, data = Xtest, block.method = "SplitWeights"))
predres$predictions



# Survival outcome: Apply algorithm to training data and obtain predictions
# for the test data:
###########################################################################

# Apply a variant to the training data:

blockforobj &lt;- blockfor(Xtrain, ysurvtrain, num.trees = 100, replace = TRUE, block=block,
                        nsets = 10, num.trees.pre = 50, splitrule="extratrees", 
                        block.method = "SplitWeights")
blockforobj$paramvalues


# Obtain prediction for the test data:

(predres &lt;- predict(blockforobj$forest, data = Xtest, block.method = "SplitWeights"))
rowSums(predres$chf)

</code></pre>


</div>