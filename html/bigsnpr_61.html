<div class="container">

<table style="width: 100%;"><tr>
<td>snp_lassosum2</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>lassosum2</h2>

<h3>Description</h3>

<p>lassosum2
</p>


<h3>Usage</h3>

<pre><code class="language-R">snp_lassosum2(
  corr,
  df_beta,
  delta = c(0.001, 0.01, 0.1, 1),
  nlambda = 30,
  lambda.min.ratio = 0.01,
  dfmax = 2e+05,
  maxiter = 1000,
  tol = 1e-05,
  ind.corr = cols_along(corr),
  ncores = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>corr</code></td>
<td>
<p>Sparse correlation matrix as an SFBM.
If <code>corr</code> is a dsCMatrix or a dgCMatrix, you can use <code>as_SFBM(corr)</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>df_beta</code></td>
<td>
<p>A data frame with 3 columns:
</p>

<ul>
<li> <p><code style="white-space: pre;">⁠$beta⁠</code>: effect size estimates
</p>
</li>
<li> <p><code style="white-space: pre;">⁠$beta_se⁠</code>: standard errors of effect size estimates
</p>
</li>
<li> <p><code style="white-space: pre;">⁠$n_eff⁠</code>: either GWAS sample size(s) when estimating <code>beta</code> for a
continuous trait, or in the case of a binary trait, this is
<code>4 / (1 / n_control + 1 / n_case)</code>; in the case of a meta-analysis, you
should sum the effective sample sizes of each study instead of using the
total numbers of cases and controls, see <a href="https://doi.org/10.1016/j.biopsych.2022.05.029">doi:10.1016/j.biopsych.2022.05.029</a>;
when using a mixed model, the effective sample size needs to be adjusted
as well, see <a href="https://doi.org/10.1016/j.xhgg.2022.100136">doi:10.1016/j.xhgg.2022.100136</a>.
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>delta</code></td>
<td>
<p>Vector of shrinkage parameters to try (L2-regularization).
Default is <code>c(0.001, 0.01, 0.1, 1)</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nlambda</code></td>
<td>
<p>Number of different lambdas to try (L1-regularization).
Default is <code>30</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda.min.ratio</code></td>
<td>
<p>Ratio between last and first lambdas to try.
Default is <code>0.01</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dfmax</code></td>
<td>
<p>Maximum number of non-zero effects in the model.
Default is <code>200e3</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxiter</code></td>
<td>
<p>Maximum number of iterations before convergence.
Default is <code>1000</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>
<p>Tolerance parameter for assessing convergence.
Default is <code>1e-5</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ind.corr</code></td>
<td>
<p>Indices to "subset" <code>corr</code>, as if this was run with
<code>corr[ind.corr, ind.corr]</code> instead. No subsetting by default.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ncores</code></td>
<td>
<p>Number of cores used. Default doesn't use parallelism.
You may use <code>bigstatsr::nb_cores()</code>.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A matrix of effect sizes, one vector (column) for each row in
<code style="white-space: pre;">⁠attr(&lt;res&gt;, "grid_param")⁠</code>. Missing values are returned when strong
divergence is detected.
</p>


</div>