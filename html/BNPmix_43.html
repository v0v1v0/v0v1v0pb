<div class="container">

<table style="width: 100%;"><tr>
<td>PYregression</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>MCMC for Pitman-Yor mixture of Gaussian regressions</h2>

<h3>Description</h3>

<p>The <code>PYregression</code> function generates a posterior sample
for mixtures of linear regression models inspired by the ANOVA-DDP model
introduced in De Iorio et al. (2004). See details below for model specification.
</p>


<h3>Usage</h3>

<pre><code class="language-R">PYregression(y, x, mcmc = list(), prior = list(), output = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>a vector of observations, univariate dependent variable;</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>a matrix of observations, multivariate independent variable;</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mcmc</code></td>
<td>
<p>a list of MCMC arguments:
</p>

<ul>
<li> <p><code>niter</code> (mandatory), number of iterations.
</p>
</li>
<li> <p><code>nburn</code> (mandatory), number of iterations to discard as burn-in.
</p>
</li>
<li> <p><code>method</code>, the MCMC sampling method to be used. Options are <code>'ICS'</code>, <code>'MAR'</code> and <code>'SLI'</code> (default is <code>'ICS'</code>). See details.
</p>
</li>
<li> <p><code>model</code> the type of model to be fitted (default is 'LS'). See details.
</p>
</li>
<li> <p><code>nupd</code>, argument controlling the number of iterations to be displayed on screen: the function reports
on standard output every time <code>nupd</code> new iterations have been carried out (default is <code>niter/10</code>).
</p>
</li>
<li> <p><code>print_message</code>, control option. If equal to <code>TRUE</code>, the status is printed
to standard output every <code>nupd</code> iterations (default is <code>TRUE</code>).
</p>
</li>
<li> <p><code>m_imp</code>, number of generated values for the importance sampling step of <code>method = 'ICS'</code> (default is 10). See details.
</p>
</li>
<li> <p><code>slice_type</code>, when <code>method = 'SLI'</code> it specifies the type of slice sampler. Options are <code>'DEP'</code> for dependent slice-efficient, and <code>'INDEP'</code> for independent slice-efficient (default is <code>'DEP'</code>). See details.
</p>
</li>
<li> <p><code>m_marginal</code>, number of generated values for the augmentation step needed, if <code>method = 'MAR'</code>, to implement Algorithm 8 of Neal, 2000. (Default is 100). See details.
</p>
</li>
<li> <p><code>hyper</code>, if equal to <code>TRUE</code>, hyperprior distributions on the base measure's
parameters are added, as specified in <code>prior</code> and explained in <code>details</code> (default is <code>TRUE</code>).
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prior</code></td>
<td>
<p>a list giving the prior information. The list includes
<code>strength</code> and <code>discount</code>, the strength and discount parameters of the Pitman-Yor process
(default are <code>strength = 1</code> and <code>discount = 0</code>, the latter leading to the Dirichlet process).
The remaining parameters specify the base measure: <code>m0</code> and <code>S0</code> are
the mean and covariance of normal base measure on the regression coefficients (default are a vector of zeroes, except for the first element equal
to <code>mean(y)</code>, and a diagonal matrix with each element equal to 100);
<code>a0</code> and <code>b0</code> are the shape and scale parameters of the inverse gamma base measure on the scale component
(default are 2 and var(y)).
If <code>hyper = TRUE</code>,  optional hyperpriors on the base measure's parameters are added:
specifically, <code>m1</code> and <code>k1</code> are the  mean parameter and scale factor defining the
normal hyperprior on <code>m0</code> (default are a vector of zeroes, except for the first element equal
to the sample mean of the dependent observed variable, and 1);
<code>tau1</code> and <code>zeta1</code> are the shape and rate parameters of the gamma hyperprior on
<code>b0</code> (default is 1 for both);
<code>n1</code> and <code>S1</code> are the parameters (degrees of freedom and scale) of the Wishart prior for <code>S0</code>
(default 4 and a diagonal matrix with each element equal to 100);  See details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>output</code></td>
<td>
<p>list of posterior summaries:
</p>

<ul>
<li> <p><code>grid_y</code>, a vector of points where to evaluate the estimated posterior mean density of
<code>y</code>, conditionally on each value of <code>x</code> in <code>grid_x</code>;
</p>
</li>
<li> <p><code>grid_x</code>, a matrix of points where to evaluate the realization of the posterior conditional densities of
<code>y</code> given <code>x</code>;
</p>
</li>
<li> <p><code>out_type</code>, if <code>out_type = "FULL"</code>, the function returns the estimated partitions and the realizations of the posterior density for each iteration;
If <code>out_type = "MEAN"</code>, return the estimated partitions and the mean of the densities sampled at each iteration;
If <code>out_type = "CLUST"</code>, return the estimated partitions. Default <code>out_type = "FULL"</code>;
</p>
</li>
<li> <p><code>out_param</code>, if equal to <code>TRUE</code>, the function returns the draws of the kernel's
parameters for each MCMC iteration, default is <code>FALSE</code>. See <code>value</code> for details.
</p>
</li>
</ul>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This function fits a Pitman-Yor process mixture of Gaussian linear regression models, i.e
</p>
<p style="text-align: center;"><code class="reqn">\tilde f(y) = \int \phi(y; x^T \beta, \sigma^2) \tilde p (d \beta, d \sigma^2)</code>
</p>

<p>where <code class="reqn">x</code> is a bivariate vector containing the dependent variable in <code>x</code> and a value of 1
for the intercept term.
The mixing measure <code class="reqn">\tilde p</code> has a Pitman-Yor process prior with strength <code class="reqn">\vartheta</code>,
discount parameter <code class="reqn">\alpha</code>. The location model assume a base measures <code class="reqn">P_0</code> specified as
</p>
<p style="text-align: center;"><code class="reqn">P_0(d \beta) = N(d \beta; m_0, S_0) .</code>
</p>

<p>while the location-scale model assume a base measures <code class="reqn">P_0</code> specified as
</p>
<p style="text-align: center;"><code class="reqn">P_0(d \beta, d \sigma^2) = N(d \beta; m_0, S_0) \times IGa(d \sigma^2; a_0, b_0).</code>
</p>

<p>Optional hyperpriors complete the model specification:
</p>
<p style="text-align: center;"><code class="reqn">m_0 \sim N(m_1, S_0 / k_1 ),\quad S_0 \sim IW(\nu_1, S_1),\quad b_0 \sim G(\tau_1, \zeta_1).</code>
</p>

<p><strong>Posterior simulation methods</strong>
</p>
<p>This generic function implements three types of MCMC algorithms for posterior simulation.
The default method is the importance conditional sampler <code>'ICS'</code> (Canale et al. 2019). Other options are
the marginal sampler <code>'MAR'</code> (algorithm 8 of Neal, 2000) and the slice sampler <code>'SLI'</code> (Kalli et al. 2011).
The importance conditional sampler performs an importance sampling step when updating the values of
individual parameters <code class="reqn">\theta</code>, which requires to sample <code>m_imp</code> values from a suitable
proposal. Large values of <code>m_imp</code> are known to improve the mixing of the posterior distribution
at the cost of increased running time (Canale et al. 2019). When updateing the individual parameter
<code class="reqn">\theta</code>, Algorithm 8 of Neal, 2000, requires to sample <code>m_marginal</code> values from the base
measure. <code>m_marginal</code> can be chosen arbitrarily. Two options are available for the slice sampler,
namely the dependent slice-efficient sampler (<code>slice_type = 'DEP'</code>), which is set as default, and the
independent slice-efficient sampler (<code>slice_type = 'INDEP'</code>) (Kalli et al. 2011). See Corradin et al. (to appear)
for more details.
</p>


<h3>Value</h3>

<p>A <code>BNPdens</code> class object containing the estimated density and
the cluster allocations for each iterations. The output contains also the data and
the grids. If <code>out_param = TRUE</code> the output
contains also the kernel specific parameters for each iteration. If <code>mcmc_dens = TRUE</code>, the
function returns also a realization from the posterior density for each iteration.
If <code>mean_dens = TRUE</code>, the output contains just the mean of the densities sampled at each iteration.
The output retuns also the number of iterations,
the number of burn-in iterations, the computational time and the type of model.
</p>


<h3>References</h3>

<p>Canale, A., Corradin, R., Nipoti, B. (2019), Importance conditional sampling for Bayesian nonparametric mixtures,
arXiv preprint, arXiv:1906.08147
</p>
<p>Corradin, R., Canale, A., Nipoti, B. (2021), BNPmix: An R Package for Bayesian Nonparametric Modeling via Pitman-Yor Mixtures,
Journal of Statistical Software, doi:10.18637/jss.v100.i15
</p>
<p>De Iorio, M., Mueller, P., Rosner, G.L., and MacEachern, S. (2004), An ANOVA Model for Dependent Random Measures,
Journal of the American Statistical Association 99, 205-215, doi:10.1198/016214504000000205
</p>
<p>Kalli, M., Griffin, J. E., and Walker, S. G. (2011), Slice sampling mixture models.
Statistics and Computing 21, 93-105, doi:10.1007/s11222-009-9150-y
</p>
<p>Neal, R. M. (2000), Markov Chain Sampling Methods for Dirichlet Process Mixture Models,
Journal of Computational and Graphical Statistics 9, 249-265, doi:10.2307/1390653
</p>


<h3>Examples</h3>

<pre><code class="language-R">x_toy &lt;- c(rnorm(100, 3, 1), rnorm(100, 3, 1))
y_toy &lt;- c(x_toy[1:100] * 2 + 1, x_toy[101:200] * 6 + 1) + rnorm(200, 0, 1)
grid_x &lt;- c(0, 1, 2, 3, 4, 5)
grid_y &lt;- seq(0, 35, length.out = 50)
est_model &lt;- PYregression(y = y_toy, x = x_toy,
mcmc = list(niter = 200, nburn = 100),
output = list(grid_x = grid_x, grid_y = grid_y))
summary(est_model)
plot(est_model)

</code></pre>


</div>