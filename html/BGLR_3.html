<div class="container">

<table style="width: 100%;"><tr>
<td>BLR</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Bayesian Linear Regression</h2>

<h3>Description</h3>

<p>The BLR (‘Bayesian Linear Regression’) function
was designed to fit parametric regression models using different
types of shrinkage methods. An earlier version of this program was presented in de los Campos <em>et al.</em> (2009).
</p>


<h3>Usage</h3>

<pre><code class="language-R">  BLR(y, XF, XR, XL, GF, prior, nIter, burnIn, thin,thin2,saveAt,
      minAbsBeta,weights)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>(numeric, <code class="reqn">n</code>) the data-vector (NAs allowed).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>XF</code></td>
<td>
<p>(numeric, <code class="reqn">n \times pF</code>) incidence matrix for <code class="reqn">\boldsymbol \beta_F</code>, may be NULL.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>XR</code></td>
<td>
<p>(numeric, <code class="reqn">n \times pR</code>) incidence matrix for <code class="reqn">\boldsymbol \beta_R</code>, may be NULL.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>XL</code></td>
<td>
<p>(numeric, <code class="reqn">n \times pL</code>) incidence matrix for <code class="reqn">\boldsymbol \beta_L</code>, may be NULL.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>GF</code></td>
<td>
<p>(list) providing an <code class="reqn">\$</code>ID (integer, <code class="reqn">n</code>) linking observations to groups
(e.g., lines or sires) and a (co)variance structure (<code class="reqn">\$</code>A, numeric, <code class="reqn">pU \times pU</code>) between effects of the grouping factor
(e.g., line or sire effects). Note: ID must be an integer taking values from 1 to <code class="reqn">pU</code>; ID[i]=<code class="reqn">q</code> indicates that
the ith observation in <code class="reqn">\boldsymbol y</code> belongs to cluster <code class="reqn">q</code> whose (co)variance function is in the qth row (column) of <code class="reqn">\boldsymbol A</code>.
GF may be NULL.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>(numeric, <code class="reqn">n</code>) a vector of weights, may be NULL.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nIter,burnIn, thin</code></td>
<td>
<p>(integer) the number of iterations, burn-in and thinning.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>saveAt</code></td>
<td>
<p>(string) this may include a path and a pre-fix that will be added to the name of the files that are saved as the program runs.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prior</code></td>
<td>
<p>(list) containing the following elements,
</p>

<ul>
<li>
<p> prior$varE, prior$varBR, prior$varU: (list) each providing degree of freedom ($df) and scale ($S). These are the parameters of the scaled inverse-<code class="reqn">\chi^2</code>
distributions assigned to variance components, see Eq. (2) below. In the parameterization used by BLR() the prior expectation of variance parameters is <code class="reqn">S/(df-2)</code>.
</p>
</li>
<li>
<p> prior$lambda: (list) providing $value (initial value for <code class="reqn">\lambda</code>); $type (‘random’ or ‘fixed’) this argument specifies
whether <code class="reqn">\lambda</code> should be kept fixed at the value provided by $value or updated with samples from the posterior
distribution; and, either $shape and $rate (this when a Gamma prior is desired on <code class="reqn">\lambda^2</code>) or $shape1, $shape2 and
$max, in this case <code class="reqn">p(\lambda |\max, \alpha_1, \alpha_2) \propto Beta \left(\frac{\lambda}{\max} | \alpha_1, \alpha_2 \right)</code>. For detailed description of these priors see de los Campos <em>et al.</em> (2009).
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>thin2</code></td>
<td>
<p>This value controls wether the running means are saved to disk or not. If thin2 is greater than nIter the running 
means are not saved (default, thin2=<code class="reqn">1 \times 10^{10}</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>minAbsBeta</code></td>
<td>
<p>The minimum absolute value of the components of <code class="reqn">\boldsymbol \beta_L</code> to avoid numeric problems when sampling from <code class="reqn">\boldsymbol \tau^2</code>, default <code class="reqn">1 \times 10^{-9}</code> </p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The program runs a Gibbs sampler for the Bayesian regression model described below.
</p>
<p><b>Likelihood</b>. The equation for the data is:
</p>
<p style="text-align: center;"><code class="reqn"> 
 \begin{array}{lr}
 \boldsymbol y= \boldsymbol 1 \mu + \boldsymbol X_F \boldsymbol \beta_F + \boldsymbol X_R \boldsymbol \beta_R + \boldsymbol X_L \boldsymbol \beta_L + \boldsymbol{Zu}  + \boldsymbol \varepsilon &amp; (1) 
 \end{array}
     </code>
</p>

<p>where <code class="reqn">\boldsymbol y</code>, the response is a <code class="reqn">n \times 1</code> vector (NAs allowed); <code class="reqn">\mu</code> is
an intercept; <code class="reqn">\boldsymbol X_F, \boldsymbol X_R, \boldsymbol X_L</code> and <code class="reqn">\boldsymbol Z</code> are incidence matrices 
used to accommodate different 
types of effects (see below), and; <code class="reqn">\boldsymbol \varepsilon</code> is a vector of model residuals assumed to be 
distributed as <code class="reqn">\boldsymbol \varepsilon \sim N(\boldsymbol 0,Diag(\sigma_{\boldsymbol \varepsilon}^2/w_i^2))</code>, 
here <code class="reqn">\sigma_{\boldsymbol \varepsilon}^2</code> is an (unknown) 
variance parameter and <code class="reqn">w_i</code> are (known) weights that allow for heterogeneous-residual variances.
</p>
<p>Any of the elements in the right-hand side of the linear predictor, except <code class="reqn">\mu</code> and <code class="reqn">\boldsymbol \varepsilon</code> , can be omitted; 
by default the program runs an intercept model.
</p>
<p><b>Prior</b>. The residual variance is assigned a scaled inverse-<code class="reqn">\chi^2</code> prior with degree of freedom and scale parameter
provided by the user, that is, <code class="reqn">\sigma_{\boldsymbol \varepsilon}^2 \sim \chi^{-2} (\sigma_{\boldsymbol \varepsilon}^2 | df_{\boldsymbol \varepsilon}, S_{\boldsymbol \varepsilon})</code>. The regression coefficients <code class="reqn">\left\{\mu, \boldsymbol \beta_F, \boldsymbol \beta_R, \boldsymbol \beta_L, \boldsymbol u \right\}</code> are assigned priors
that yield different type of shrinkage. The intercept and the vector of regression coefficients <code class="reqn">\boldsymbol \beta_F</code> are assigned flat priors
(i.e., estimates are not shrunk). The vector of regression coefficients <code class="reqn">\boldsymbol \beta_R</code> is assigned a
Gaussian prior with variance common to all effects, that is,
<code class="reqn">\beta_{R,j} \mathop \sim \limits^{iid} N(0, \sigma_{\boldsymbol \beta_R}^2)</code>. This prior is 
the Bayesian counterpart of Ridge Regression. The variance parameter <code class="reqn">\sigma_{\boldsymbol \beta_R}^2</code>, 
is treated as unknown and it is assigned a scaled inverse-<code class="reqn">\chi^2</code> prior, that is,
<code class="reqn">\sigma_{\boldsymbol \beta_R}^2 \sim \chi^{-2} (\sigma_{\boldsymbol \beta_R}^2 | df_{\boldsymbol \beta_R}, S_{\boldsymbol \beta_R})</code> with degrees of freedom 
<code class="reqn">df_{\boldsymbol \beta_R}</code>, and scale <code class="reqn">S_{\boldsymbol \beta_R}</code> provided by the user.
</p>
<p>The vector of regression coefficients <code class="reqn">\boldsymbol \beta_L</code> is treated as in
the Bayesian LASSO of Park and Casella (2008). Specifically,
</p>
<p style="text-align: center;"><code class="reqn">p(\boldsymbol \beta_L, \boldsymbol \tau^2, \lambda | \sigma_{\boldsymbol \varepsilon}^2) = \left\{\prod_k N(\beta_{L,k} | 0, \sigma_{\boldsymbol \varepsilon}^2 \tau_k^2) Exp\left(\tau_k^2 | \lambda^2\right) \right\} p(\lambda),</code>
</p>

<p>where, <code class="reqn">Exp(\cdot|\cdot)</code> is an exponential prior and <code class="reqn">p(\lambda)</code> can either be: (a)
a mass-point at some value (i.e., fixed <code class="reqn">\lambda</code>); (b) <code class="reqn">p(\lambda^2) \sim Gamma(r,\delta)</code> this 
is the prior suggested by Park and Casella (2008); or, (c) <code class="reqn">p(\lambda | \max, \alpha_1, \alpha_2) \propto Beta\left(\frac{\lambda}{\max} | \alpha_1,\alpha_2 \right)</code>, see de los Campos <em>et al</em>. (2009) for details. It can be shown that the marginal prior of regression coefficients <code class="reqn">\beta_{L,k}, \int N(\beta_{L,k} | 0, \sigma_{\boldsymbol \varepsilon}^2 \tau_k^2) Exp\left(\tau_k^2 | \lambda^2\right) \partial \tau_k^2</code>, is Double-Exponential. This prior has thicker tails and higher peak of mass at zero than the Gaussian prior used for <code class="reqn">\boldsymbol \beta_R</code>, inducing a different type of shrinkage.
</p>
<p>The vector <code class="reqn">\boldsymbol u </code> is used to model the so called ‘infinitesimal effects’, and is assigned a prior <code class="reqn">\boldsymbol u \sim N(\boldsymbol 0, \boldsymbol A\sigma_{\boldsymbol u}^2)</code>, 
where, <code class="reqn">\boldsymbol A</code> is a positive-definite matrix (usually a relationship matrix computed from a pedigree) and <code class="reqn">\sigma_{\boldsymbol u}^2</code> is an unknow variance, whose prior is
<code class="reqn">\sigma_{\boldsymbol u}^2 \sim \chi^{-2} (\sigma_{\boldsymbol u}^2 | df_{\boldsymbol u}, S_{\boldsymbol u})</code>.
</p>
<p>Collecting the above mentioned assumptions, the posterior distribution of model unknowns, 
<code class="reqn">\boldsymbol \theta= \left\{\mu, \boldsymbol \beta_F, \boldsymbol \beta_R, \sigma_{\boldsymbol \beta_R}^2, \boldsymbol \beta_L, \boldsymbol \tau^2, \lambda, \boldsymbol u, \sigma_{\boldsymbol u}^2, \sigma_{\boldsymbol \varepsilon}^2, \right\}</code>, is, 
</p>
<p style="text-align: center;"><code class="reqn">\begin{array}{lclr}
        p(\boldsymbol \theta | \boldsymbol y) &amp; \propto &amp; N\left( \boldsymbol y | \boldsymbol 1 \mu + \boldsymbol X_F \boldsymbol \beta_F + \boldsymbol X_R \boldsymbol \beta_R + \boldsymbol X_L \boldsymbol \beta_L + \boldsymbol{Zu}; Diag\left\{ \frac{\sigma_\varepsilon^2}{w_i^2}\right\}\right) &amp; \\
                                              &amp;         &amp; \times  \left\{ \prod\limits_j N\left(\beta_{R,j} | 0, \sigma_{\boldsymbol \beta_R}^2 \right) \right\} \chi^{-2} \left(\sigma_{\boldsymbol \beta_R}^2  | df_{\boldsymbol \beta_R}, S_{\boldsymbol \beta_R}\right)  &amp; \\
                                              &amp;         &amp; \times \left\{ \prod\limits_k N \left( \beta_{L,k} |0,\sigma_{\boldsymbol \varepsilon}^2 \tau_k^2 \right) Exp \left(\tau_k^2 | \lambda^2\right)\right\} p(\lambda) &amp; (2)\\
                                              &amp;         &amp; \times N(\boldsymbol u | \boldsymbol 0,\boldsymbol A\sigma_{\boldsymbol u}^2) \chi^{-2} (\sigma_{\boldsymbol u}^2 | df_{\boldsymbol u}, S_{\boldsymbol u}) \chi^{-2} (\sigma_{\boldsymbol \varepsilon}^2 | df_{\boldsymbol \varepsilon}, S_{\boldsymbol \varepsilon}) &amp; 
      \end{array}
     </code>
</p>



<h3>Value</h3>

<p>A list with posterior means, posterior standard deviations, and the parameters used to fit the model:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>$yHat</code></td>
<td>
<p>the posterior mean of <code class="reqn">\boldsymbol 1 \mu + \boldsymbol X_F \boldsymbol \beta_F + \boldsymbol X_R \boldsymbol \beta_R + \boldsymbol X_L \boldsymbol \beta_L + \boldsymbol{Zu}  + \boldsymbol\varepsilon</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>$SD.yHat</code></td>
<td>
<p>the corresponding posterior standard deviation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>$mu</code></td>
<td>
<p>the posterior mean of the intercept.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>$varE</code></td>
<td>
<p>the posterior mean of <code class="reqn">\sigma_{\boldsymbol \varepsilon}^2</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>$bR</code></td>
<td>
<p>the posterior mean of <code class="reqn">\boldsymbol \beta_R</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>$SD.bR</code></td>
<td>
<p>the corresponding posterior standard deviation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>$varBr</code></td>
<td>
<p>the posterior mean of <code class="reqn">\sigma_{\boldsymbol \beta_R}^2</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>$bL</code></td>
<td>
<p>the posterior mean of <code class="reqn">\boldsymbol \beta_L</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>$SD.bL</code></td>
<td>
<p>the corresponding posterior standard deviation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>$tau2</code></td>
<td>
<p>the posterior mean of <code class="reqn">\boldsymbol \tau^2</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>$lambda</code></td>
<td>
<p>the posterior mean of <code class="reqn">\lambda</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>$u</code></td>
<td>
<p>the posterior mean of <code class="reqn">\boldsymbol u</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>$SD.u</code></td>
<td>
<p>the corresponding posterior standard deviation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>$varU</code></td>
<td>
<p>the posterior mean of <code class="reqn">\sigma_{\boldsymbol u}^2</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>$fit</code></td>
<td>
<p>a list with evaluations of effective number of parameters and DIC (Spiegelhalter <em>et al.</em>, 2002).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>$whichNa</code></td>
<td>
<p>a vector indicating which entries in <code class="reqn">\boldsymbol y</code> were missing.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>$prior</code></td>
<td>
<p>a list containig the priors used during the analysis.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>$weights</code></td>
<td>
<p>vector of weights.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>$fit</code></td>
<td>
<p>list containing the following elements,
</p>

<ul>
<li>
<p> $logLikAtPostMean: log-likelihood evaluated at posterior mean.
</p>
</li>
<li>
<p> $postMeanLogLik: the posterior mean of the Log-Likelihood.
</p>
</li>
<li>
<p> $pD: estimated effective number of parameters, Spiegelhalter <em>et al.</em> (2002).
</p>
</li>
<li>
<p> $DIC: the deviance information criterion, Spiegelhalter <em>et al.</em> (2002).                        
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>$nIter</code></td>
<td>
<p>the number of iterations made in the Gibbs sampler.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>$burnIn</code></td>
<td>
<p>the nuber of iteratios used as burn-in.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>$thin</code></td>
<td>
<p>the thin used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>$y</code></td>
<td>
<p>original data-vector.</p>
</td>
</tr>
</table>
<p>The posterior means returned by BLR are calculated after burnIn is
passed and at a thin as specified by the user.
</p>
<p><b>Save</b>. The routine will save samples of <code class="reqn">\mu</code>, variance components and <code class="reqn">\lambda</code> and running means
(rm*.dat). Running means are computed using the thinning specified by
the user (see argument thin above); however these running means are
saved at a thinning specified by argument thin2 (by default, thin2=<code class="reqn">1 \times 10^{10}</code> so that running means are computed 
as the sampler runs but not saved to the disc).
</p>


<h3>Author(s)</h3>

<p>Gustavo de los Campos, Paulino Perez Rodriguez,
</p>


<h3>References</h3>

<p>de los Campos G., H. Naya, D. Gianola, J. Crossa, A. Legarra, E. Manfredi, K. Weigel and J. Cotes. 2009.
Predicting Quantitative Traits with Regression Models for Dense Molecular Markers and Pedigree. <em>Genetics</em> <b>182</b>: 375-385.
</p>
<p>Park T. and G. Casella. 2008. The Bayesian LASSO. <em>Journal of the American Statistical Association</em> <b>103</b>: 681-686.
</p>
<p>Spiegelhalter, D.J., N.G. Best, B.P. Carlin and A. van der Linde. 2002. Bayesian measures of model complexity and 
fit (with discussion). <em>Journal of the Royal Statistical Society</em>, Series B (Statistical Methodology) <b>64</b> (4): 583-639.
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
########################################################################
##Example 1:
########################################################################

rm(list=ls())
setwd(tempdir())
library(BGLR)
data(wheat)     #Loads the wheat dataset

y=wheat.Y[,1]
### Creates a testing set with 100 observations
whichNa&lt;-sample(1:length(y),size=100,replace=FALSE)
yNa&lt;-y
yNa[whichNa]&lt;-NA

### Runs the Gibbs sampler
fm&lt;-BLR(y=yNa,XL=wheat.X,GF=list(ID=1:nrow(wheat.A),A=wheat.A),
                           prior=list(varE=list(df=3,S=0.25),
                           varU=list(df=3,S=0.63),
                           lambda=list(shape=0.52,rate=1e-4,
                           type='random',value=30)),
                           nIter=5500,burnIn=500,thin=1)

MSE.tst&lt;-mean((fm$yHat[whichNa]-y[whichNa])^2)
MSE.tst
MSE.trn&lt;-mean((fm$yHat[-whichNa]-y[-whichNa])^2)
MSE.trn
COR.tst&lt;-cor(fm$yHat[whichNa],y[whichNa])
COR.tst
COR.trn&lt;-cor(fm$yHat[-whichNa],y[-whichNa])
COR.trn

plot(fm$yHat~y,xlab="Phenotype",
     ylab="Pred. Gen. Value" ,cex=.8)
points(x=y[whichNa],y=fm$yHat[whichNa],col=2,cex=.8,pch=19)

x11()
plot(scan('varE.dat'),type="o",
        ylab=expression(paste(sigma[epsilon]^2)))

########################################################################
#Example 2: Ten fold, Cross validation, environment 1,
########################################################################

rm(list=ls())
setwd(tempdir())
library(BGLR)
data(wheat)     #Loads the wheat dataset
nIter&lt;-1500     #For real data sets more samples are needed
burnIn&lt;-500     
thin&lt;-10
folds&lt;-10
y&lt;-wheat.Y[,1]
A&lt;-wheat.A

priorBL&lt;-list(
               varE=list(df=3,S=2.5),
               varU=list(df=3,S=0.63),
               lambda = list(shape=0.52,rate=1e-5,value=20,type='random')
             )
             
set.seed(123)  #Set seed for the random number generator
sets&lt;-rep(1:10,60)[-1]
sets&lt;-sets[order(runif(nrow(A)))]
COR.CV&lt;-rep(NA,times=(folds+1))
names(COR.CV)&lt;-c(paste('fold=',1:folds,sep=''),'Pooled')
w&lt;-rep(1/nrow(A),folds) ## weights for pooled correlations and MSE
yHatCV&lt;-numeric()

for(fold in 1:folds)
{
   yNa&lt;-y
   whichNa&lt;-which(sets==fold)
   yNa[whichNa]&lt;-NA
   prefix&lt;-paste('PM_BL','_fold_',fold,'_',sep='')
   fm&lt;-BLR(y=yNa,XL=wheat.X,GF=list(ID=(1:nrow(wheat.A)),A=wheat.A),prior=priorBL,
               nIter=nIter,burnIn=burnIn,thin=thin)
   yHatCV[whichNa]&lt;-fm$yHat[fm$whichNa]
   w[fold]&lt;-w[fold]*length(fm$whichNa)
   COR.CV[fold]&lt;-cor(fm$yHat[fm$whichNa],y[whichNa])
}

COR.CV[11]&lt;-mean(COR.CV[1:10])
COR.CV

########################################################################

## End(Not run)
</code></pre>


</div>