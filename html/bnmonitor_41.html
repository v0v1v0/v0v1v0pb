<div class="container">

<table style="width: 100%;"><tr>
<td>KL_bounds</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Bounds for the KL-divergence</h2>

<h3>Description</h3>

<p>Computation of the bounds of the KL-divergence for variations of each parameter of a <code>CI</code> object.
</p>


<h3>Usage</h3>

<pre><code class="language-R">KL_bounds(ci, delta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>ci</code></td>
<td>
<p>object of class <code>CI</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>delta</code></td>
<td>
<p>multiplicative variation coefficient for the entry of the covariance matrix given in <code>entry</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Let <code class="reqn">\Sigma</code> be the covariance matrix of a Gaussian Bayesian network with <code class="reqn">n</code> vertices.
Let <code class="reqn">D</code> and <code class="reqn">\Delta</code> be  variation matrices acting additively on <code class="reqn">\Sigma</code>. Let also <code class="reqn">\tilde\Delta</code> be a model-preserving co-variation matrix.
Denote with <code class="reqn">Y</code> and <code class="reqn">\tilde{Y}</code> the original and the perturbed random vectors. Then for a standard sensitivity analysis
</p>
<p style="text-align: center;"><code class="reqn">KL(\tilde{Y}||Y)\leq 0.5n\max\left\{f(\lambda_{\max}(D\Sigma^{-1})),f(\lambda_{\min}(D\Sigma^{-1}))\right\}</code>
</p>

<p>whilst for a model-preserving one
</p>
<p style="text-align: center;"><code class="reqn">KL(\tilde{Y}||Y)\leq 0.5n\max\left\{f(\lambda_{\max}(\tilde\Delta\circ\Delta)),f(\lambda_{\min}(\tilde\Delta\circ\Delta))\right\}</code>
</p>

<p>where <code class="reqn">\lambda_{\max}</code> and <code class="reqn">\lambda_{\min}</code> are the largest and the smallest eigenvalues, respectively, <code class="reqn">f(x)=\ln(1+x)-x/(1+x)</code> and <code class="reqn">\circ</code> denotes the Schur or element-wise product.
</p>


<h3>Value</h3>

<p>A dataframe including the KL-divergence bound for each co-variation scheme (model-preserving and standard) and every entry of the covariance matrix. For variations leading to non-positive semidefinite matrix, the dataframe includes a <code>NA</code>.
</p>


<h3>References</h3>

<p>C. GÃ¶rgen &amp; M. Leonelli (2020), Model-preserving sensitivity analysis for families of Gaussian distributions.  Journal of Machine Learning Research, 21: 1-32.
</p>


<h3>See Also</h3>

<p><code>KL.CI</code>, <code>KL.CI</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">KL_bounds(synthetic_ci,1.05)


</code></pre>


</div>