<div class="container">

<table style="width: 100%;"><tr>
<td>sample_bpr</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Fitting Bayesian Poisson Regression</h2>

<h3>Description</h3>

<p>The function generates draws from the posterior distribution of the coefficients of Poisson regression models. 
The method allows for Gaussian and horseshoe (Carvalho et al, 2010) prior distributions, 
and relies on a Metropolis-Hastings or importance sampler algorithm.
</p>


<h3>Usage</h3>

<pre><code class="language-R">sample_bpr(
  formula = NULL,
  data = NULL,
  iter,
  burnin = NULL,
  prior = list(type = "gaussian", b = NULL, B = NULL, tau = NULL),
  pars = list(method = "MH", max_dist = 50, max_r = NULL, max_dist_burnin = 1e+06),
  state = NULL,
  thin = 1,
  verbose = TRUE,
  seed = NULL,
  nchains = 1,
  perc_burnin = 0.25
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p>an object of class "formula": a symbolic description of the model to be fitted.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>data frame or matrix containing the variables in the model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iter</code></td>
<td>
<p>number of algorithm iterations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>burnin</code></td>
<td>
<p>(optional) a positive integer specifying the length of the burn-in. 
If a value &gt; 1 is provided, the first <code>burnin</code> iterations use a different tuning parameter in order to better explore the parameter space.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prior</code></td>
<td>
<p>a named list of parameters to select prior type and parameters, with arguments:
</p>

<ul>
<li> <p><code>type</code> : string specifying whether an informative Gaussian (<code>"gaussian"</code>) or a horseshoe (<code>"horseshoe"</code>) prior should be used. 
Default is <code>"gaussian"</code>.
</p>
</li>
<li> <p><code>b, B</code> : (optional) if a Gaussian prior is used, the mean and covariance matrix passed as prior parameters. 
If not specified, the prior on the regression parameters is centered at zero, with independent N(0,2) components.
</p>
</li>
<li> <p><code>tau</code> : if a horseshoe prior is used, the global shrinkage parameter tau has to be fixed. 
This can be seen as an empirical Bayes approach, and allows to speed convergence and avoid potential convergence issues that often occur when it is sampled.
In general, the parameter can be interpreted as a measure of sparsity, and it should be fixed to small values. See van der Pas et al. (2017) for a discussion.
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pars</code></td>
<td>
<p>a named list of parameters to select algorithm type and tuning parameters, with arguments:
</p>

<ul>
<li> <p><code>method</code> : the type of algorithm used. Default is a Metropolis-Hastings algorithm (<code>"MH"</code>), 
the alternative is an importance sampler algorithm (<code>"IS"</code>).
</p>
</li>
<li> <p><code>max_dist</code> : tuning parameter controlling the "distance" of the approximation to the true target posterior. 
For the Metropolis-Hastings algorithm, it can be used to balance acceptance rate and autocorrelation of the chains. 
As a general indication, larger values are needed for increasing size/dimension of the data to obtain good results.
#' </p>
</li>
<li> <p><code>max_r</code> : (optional) additional tuning parameter which sets an upper bound for the parameters r controlling the approximation.
</p>
</li>
<li> <p><code>max_dist_burnin</code> : if <code>burnin</code> is specified, the tuning parameter used for the first part of the chain. 
A very large value is sometimes useful to explore the parameter space (especially if the chains are initialized very far from their stationary distribution).
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>state</code></td>
<td>
<p>optional vector providing the starting points of the chains.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>thin</code></td>
<td>
<p>a positive integer specifying the period for saving samples. The default is 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>logical (default = TRUE) indicating whether to print messages on the progress of the algorithm and possible convergence issues.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>
<p>(optional) positive integer: the seed of random number generator.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nchains</code></td>
<td>
<p>(optional) positive integer specifying the number of Markov chains. The default is 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>perc_burnin</code></td>
<td>
<p>(default = 0.25) percentage of the chain to be discarded to perform inference. If both burnin and perc_burnin are specified, the most conservative burn-in is considered.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This function fits a Bayesian Poisson regression model with Gaussian prior distributions on the regression coefficients:
</p>
<p style="text-align: center;"><code class="reqn"> Y ~ Poisson(\lambda) , \lambda = exp{X \beta} </code>
</p>

<p>where <code class="reqn">Y</code> is a size <code class="reqn">n</code> vector of counts and <code class="reqn">X</code> is a <code class="reqn">n x p</code> matrix of coefficients; and <code class="reqn">(\beta | - )</code> 
has a Gaussian distribution (possibly conditionally on some parameters).
</p>
<p>Specifically, the function allows for informative Gaussian prior distribution on the parameters, 
i.e. <code class="reqn">(\beta_1,...,\beta_p) ~ N_p(b, B)</code>, and for a horseshoe prior distribution (Carvalho et al, 2010). 
The horseshoe prior is a scale mixture of normals, which is typically used in high-dimension settings to induce sparsity and
regularization of the coefficients.
</p>
<p>The implemented Metropolis-Hastings and importance sampler exploit as proposal density a multivariate Gaussian approximation of the 
posterior distribution. Such proposal is based on the convergence of the negative binomial distribution to the Poisson distribution and on
the Polya-gamma data augmentation of Polson et al. (2013).
</p>
<p>The output of the sampling is an object of class <code>poisreg</code> and admits class-specific methods to perform inference.<br>
The function <code>summary.poisreg</code> can be used to obtain or print a summary of the results and of the algorithm diagnostics. <br>
The function <code>mcmc_diagnostics</code> can be used to obtain or print convergence diagnostics for the sampled chains.  <br>
The function <code>plot.poisreg</code> prints the trace of the sampled values and a density estimate of the regression coefficients. 
See <code>plot.mcmc</code>.<br>
The function <code>posterior_predictive</code> can be used to compute the posterior predictive distributions to check the model. 
See also the related function <code>plot.ppc</code>.
</p>


<h3>Value</h3>

<p>An object of S3 class <code>poisreg</code> containing the results of the sampling. <br><code>poisreg</code> is a list containing at least the following elements:
</p>
<p><code>sim</code> : list of the results of the sampling. It contains the following elements: </p>

<ul>
<li> <p><code>beta</code> : <code>mcmc</code> object of posterior draws of the regression coefficients.
</p>
</li>
<li> <p><code>r</code> : the sequence of adaptive tuning parameters used in each iteration. 
</p>
</li>
<li> <p><code>time</code> : the total amount of time to perform the simulation. 
</p>
</li>
</ul>
<p><code>formula</code>  : the <code>formula</code> object used.
</p>
<p><code>data</code>  : list with elements the matrix of covariates <code>X</code> and response variable <code>y</code>.
</p>
<p><code>state</code>  : the starting points of the chain.
</p>
<p><code>burnin</code>  : length of the used burn-in.
</p>
<p><code>prior</code>  : whether a Gaussian or horseshoe prior was used.
</p>
<p><code>prior_pars</code>  : prior parameters.
</p>
<p><code>thin</code>  : thinning frequency passed to the <code>thin</code> parameter.
</p>
<p><code>nchains</code>  : number of chains. If <code>nchains</code> was chosen &gt;1, the output list will also include additional 
numbered <code>sim</code> elements, one for each sampled chain.
</p>
<p><code>perc_burnin</code> : percentage of the chain used as burn-in.
</p>


<h3>References</h3>

<p>Carvalho, C., Polson, N., &amp; Scott, J. (2010). The horseshoe estimator for sparse signals. Biometrika, 97(2), 465-480.<br><br>
van der Pas, S., Szabo, B. and van der Vaart, A. (2017), Adaptive posterior contractionrates for the horseshoe, Electronic Journal of Statistics, 11(2), 3196-3225.
</p>


<h3>See Also</h3>

<p><code>summary.poisreg</code> , <code>mcmc_diagnostics</code> , <code>plot.poisreg</code> ,
<code>merge_sim</code> , <code>posterior_predictive</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">require(MASS) # load the data set
head(epil)

fit = sample_bpr( y ~  lbase*trt + lage + V4, data = epil, 
                   iter = 1000)
                   
summary(fit)    # summary of posterior inference
mcmc_diagnostics(fit)    # summary of MCMC convergence diagnostics

plot(fit)    


## Examples with different options
# Select prior parameters and set tuning parameter 
fit2 = sample_bpr( y ~  lbase*trt + lage + V4, data = epil, 
                    iter = 1000, 
                    prior = list( type = "gaussian", b = rep(0, 6), 
                                  B = diag(6) * 3 ),
                    pars = list( max_dist = 10 ))
                    
# Simulate multiple chains and merge outputs after checking convergence
fit3 = sample_bpr( y ~  lbase*trt + lage + V4, data = epil, 
                    iter = 1000, 
                    nchains = 4, thin = 2)
# fit3 now contains additional elements ($sim2, $sim3, $sim4)

mcmc_diagnostics(fit3)     
# the Gelman-Rubin diagnostics confirms convergence of the 4 
# independent chains to the same stationary distribution

fit3b = merge_sim(fit3) 
str(fit3b$sim)    
# fit 3b contains only one MCMC chain of length 1500 
# (after thinning and burn-in)


## introduce more variables and use regularization
epil2 &lt;- epil[epil$period == 1, ]
epil2["period"] &lt;- rep(0, 59); epil2["y"] &lt;- epil2["base"]
epil["time"] &lt;- 1; epil2["time"] &lt;- 4
epil2 &lt;- rbind(epil, epil2)
epil2$pred &lt;- unclass(epil2$trt) * (epil2$period &gt; 0) 
epil2$subject &lt;- factor(epil2$subject)
epil3 &lt;- aggregate(epil2, list(epil2$subject, epil2$period &gt; 0),
                   function(x) if(is.numeric(x)) sum(x) else x[1])
                   epil3$pred &lt;- factor(epil3$pred,
                   labels = c("base", "placebo", "drug"))
contrasts(epil3$pred) &lt;- structure(contr.sdif(3),
                         dimnames = list(NULL, c("placebo-base", "drug-placebo")))
                         
fit4 = sample_bpr(y ~ pred + factor(subject), data = epil3,
                pars = list(max_dist = 0.3),
                prior = list(type = "horseshoe", tau = 2),
                iter = 3000, burnin = 1000)
summary(fit4)
mcmc_diagnostics(fit4)
plot(posterior_predictive(fit4), stats = c("mean", "sd", "max"))


</code></pre>


</div>