<div class="container">

<table style="width: 100%;"><tr>
<td>bark-package</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>bark:  Bayesian Additive Regression Trees</h2>

<h3>Description</h3>

<p>Implementation of Bayesian Additive Regression Kernels
with Feature Selection for  Nonparametric Regression for
Gaussian regression
and classification for binary Probit models
</p>
<p>_PACKAGE
</p>


<h3>Details</h3>

<p>BARK is a Bayesian <em>sum-of-kernels</em> model or because of the
Bayesian priors is a Bayesian Additive Regression Kernel model. <br>
For numeric response <code class="reqn">y</code>, we have
<code class="reqn">y = f(x) + \epsilon</code>,
where <code class="reqn">\epsilon \sim N(0,\sigma^2)</code>.<br>
For a binary response <code class="reqn">y</code>, <code class="reqn">P(Y=1 | x) = F(f(x))</code>, where <code class="reqn">F</code>
denotes the standard normal cdf (probit link).
In both cases, <code class="reqn">f</code> is the sum of many Gaussian kernel functions.
The goal is to have very flexible inference for the unknown
function <code class="reqn">f</code>.
bark  uses an approximated Cauchy process as the prior distribution
for the unknown function <code class="reqn">f</code>.
Feature selection can be achieved through the inference
on the scale parameters in the Gaussian kernels.
BARK accepts four different types of prior distributions through setting
values for <code>selection</code> (TRUE or FALSE), which allows scale parameters
for some variables to be set to zero, removing the variables from the
kernels <code>selection = TRUE</code>; this enables either soft shrinkage or hard 
shrinkage for the scale
parameters. The input <code>common_lambdas</code> (TRUE or FALSE) specifies whether
a common scale parameter should be used for all predictors (TRUE) or
if FALSE allows the scale parameters to differ across all variables
in the kernel.
</p>


<h3>References</h3>

<p>Ouyang, Zhi (2008) Bayesian Additive Regression Kernels.
Duke University. PhD dissertation, Chapter 3.
</p>


<h3>See Also</h3>

<p>Other bark functions: 
<code>bark()</code>,
<code>bark-package-deprecated</code>,
<code>sim_Friedman1()</code>,
<code>sim_Friedman2()</code>,
<code>sim_Friedman3()</code>,
<code>sim_circle()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
 # Simulate regression example
 # Friedman 2 data set, 200 noisy training, 1000 noise free testing
 # Out of sample MSE in SVM (default RBF): 6500 (sd. 1600)
 # Out of sample MSE in BART (default):    5300 (sd. 1000)
 traindata &lt;- sim_Friedman2(200, sd=125)
 testdata &lt;- sim_Friedman2(1000, sd=0)
 fit.bark.d &lt;- bark(y ~ ., data = data.frame(traindata),
                    testdata = data.frame(testdata), 
                    classification = FALSE,
                    selection = FALSE,
                    common_lambdas = TRUE)
 boxplot(as.data.frame(fit.bark.d$theta.lambda))
 mean((fit.bark.d$yhat.test.mean-testdata$y)^2)
 # Simulate classification example
 # Circle 5 with 2 signals and three noisy dimensions
 # Out of sample erorr rate in SVM (default RBF): 0.110 (sd. 0.02)
 # Out of sample error rate in BART (default):    0.065 (sd. 0.02)
 traindata &lt;- sim_circle(200, dim=5)
 testdata &lt;- sim_circle(1000, dim=5)
 fit.bark.se &lt;- bark(y ~ ., data= data.frame(traindata), 
                     testdata= data.frame(testdata),
                     classification=TRUE, 
                     selection=TRUE,
                     common_lambdas = FALSE)
                   
 boxplot(as.data.frame(fit.bark.se$theta.lambda))
 mean((fit.bark.se$yhat.test.mean&gt;0)!=testdata$y)


</code></pre>


</div>