<div class="container">

<table style="width: 100%;"><tr>
<td>brulee_linear_reg</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Fit a linear regression model</h2>

<h3>Description</h3>

<p><code>brulee_linear_reg()</code> fits a linear regression model.
</p>


<h3>Usage</h3>

<pre><code class="language-R">brulee_linear_reg(x, ...)

## Default S3 method:
brulee_linear_reg(x, ...)

## S3 method for class 'data.frame'
brulee_linear_reg(
  x,
  y,
  epochs = 20L,
  penalty = 0.001,
  mixture = 0,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 1,
  momentum = 0,
  batch_size = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)

## S3 method for class 'matrix'
brulee_linear_reg(
  x,
  y,
  epochs = 20L,
  penalty = 0.001,
  mixture = 0,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 1,
  momentum = 0,
  batch_size = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)

## S3 method for class 'formula'
brulee_linear_reg(
  formula,
  data,
  epochs = 20L,
  penalty = 0.001,
  mixture = 0,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 1,
  momentum = 0,
  batch_size = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)

## S3 method for class 'recipe'
brulee_linear_reg(
  x,
  data,
  epochs = 20L,
  penalty = 0.001,
  mixture = 0,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 1,
  momentum = 0,
  batch_size = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>Depending on the context:
</p>

<ul>
<li>
<p> A <strong>data frame</strong> of predictors.
</p>
</li>
<li>
<p> A <strong>matrix</strong> of predictors.
</p>
</li>
<li>
<p> A <strong>recipe</strong> specifying a set of preprocessing steps
created from <code>recipes::recipe()</code>.
</p>
</li>
</ul>
<p>The predictor data should be standardized (e.g. centered or scaled).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Options to pass to the learning rate schedulers via
<code>set_learn_rate()</code>. For example, the <code>reduction</code> or <code>steps</code> arguments to
<code>schedule_step()</code> could be passed here.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>When <code>x</code> is a <strong>data frame</strong> or <strong>matrix</strong>, <code>y</code> is the outcome
specified as:
</p>

<ul>
<li>
<p> A <strong>data frame</strong> with 1 numeric column.
</p>
</li>
<li>
<p> A <strong>matrix</strong> with 1 numeric column.
</p>
</li>
<li>
<p> A numeric <strong>vector</strong>.
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epochs</code></td>
<td>
<p>An integer for the number of epochs of training.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>penalty</code></td>
<td>
<p>The amount of weight decay (i.e., L2 regularization).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mixture</code></td>
<td>
<p>Proportion of Lasso Penalty (type: double, default: 0.0). A
value of mixture = 1 corresponds to a pure lasso model, while mixture = 0
indicates ridge regression (a.k.a weight decay).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>validation</code></td>
<td>
<p>The proportion of the data randomly assigned to a
validation set.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>optimizer</code></td>
<td>
<p>The method used in the optimization procedure. Possible choices
are 'LBFGS' and 'SGD'. Default is 'LBFGS'.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>learn_rate</code></td>
<td>
<p>A positive number that controls the initial rapidity that
the model moves along the descent path. Values around 0.1 or less are
typical.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>momentum</code></td>
<td>
<p>A positive number usually on <code style="white-space: pre;">⁠[0.50, 0.99]⁠</code> for the momentum
parameter in gradient descent.  (<code>optimizer = "SGD"</code> only)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>batch_size</code></td>
<td>
<p>An integer for the number of training set points in each
batch. (<code>optimizer = "SGD"</code> only)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stop_iter</code></td>
<td>
<p>A non-negative integer for how many iterations with no
improvement before stopping.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>A logical that prints out the iteration history.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p>A formula specifying the outcome term(s) on the left-hand side,
and the predictor term(s) on the right-hand side.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>When a <strong>recipe</strong> or <strong>formula</strong> is used, <code>data</code> is specified as:
</p>

<ul><li>
<p> A <strong>data frame</strong> containing both the predictors and the outcome.
</p>
</li></ul>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This function fits a linear combination of coefficients and predictors to
model the numeric outcome. The training process optimizes the
mean squared error loss function.
</p>
<p>The function internally standardizes the outcome data to have mean zero and
a standard deviation of one. The prediction function creates predictions on
the original scale.
</p>
<p>By default, training halts when the validation loss increases for at least
<code>step_iter</code> iterations. If <code>validation = 0</code> the training set loss is used.
</p>
<p>The <em>predictors</em> data should all be numeric and encoded in the same units (e.g.
standardized to the same range or distribution). If there are factor
predictors, use a recipe or formula to create indicator variables (or some
other method) to make them numeric. Predictors should be in the same units
before training.
</p>
<p>The model objects are saved for each epoch so that the number of epochs can
be efficiently tuned. Both the <code>coef()</code> and <code>predict()</code> methods for this
model have an <code>epoch</code> argument (which defaults to the epoch with the best
loss value).
</p>
<p>The use of the L1 penalty (a.k.a. the lasso penalty) does <em>not</em> force
parameters to be strictly zero (as it does in packages such as <span class="pkg">glmnet</span>).
The zeroing out of parameters is a specific feature the optimization method
used in those packages.
</p>


<h3>Value</h3>

<p>A <code>brulee_linear_reg</code> object with elements:
</p>

<ul>
<li> <p><code>models_obj</code>: a serialized raw vector for the torch module.
</p>
</li>
<li> <p><code>estimates</code>: a list of matrices with the model parameter estimates per
epoch.
</p>
</li>
<li> <p><code>best_epoch</code>: an integer for the epoch with the smallest loss.
</p>
</li>
<li> <p><code>loss</code>: A vector of loss values (MSE) at each epoch.
</p>
</li>
<li> <p><code>dim</code>: A list of data dimensions.
</p>
</li>
<li> <p><code>y_stats</code>: A list of summary statistics for numeric outcomes.
</p>
</li>
<li> <p><code>parameters</code>: A list of some tuning parameter values.
</p>
</li>
<li> <p><code>blueprint</code>: The <code>hardhat</code> blueprint data.
</p>
</li>
</ul>
<h3>See Also</h3>

<p><code>predict.brulee_linear_reg()</code>, <code>coef.brulee_linear_reg()</code>,
<code>autoplot.brulee_linear_reg()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
if (torch::torch_is_installed()) {

 ## -----------------------------------------------------------------------------

 data(ames, package = "modeldata")

 ames$Sale_Price &lt;- log10(ames$Sale_Price)

 set.seed(122)
 in_train &lt;- sample(1:nrow(ames), 2000)
 ames_train &lt;- ames[ in_train,]
 ames_test  &lt;- ames[-in_train,]


 # Using matrices
 set.seed(1)
 brulee_linear_reg(x = as.matrix(ames_train[, c("Longitude", "Latitude")]),
                    y = ames_train$Sale_Price,
                    penalty = 0.10, epochs = 1, batch_size = 64)

 # Using recipe
 library(recipes)

 ames_rec &lt;-
  recipe(Sale_Price ~ Bldg_Type + Neighborhood + Year_Built + Gr_Liv_Area +
         Full_Bath + Year_Sold + Lot_Area + Central_Air + Longitude + Latitude,
         data = ames_train) %&gt;%
    # Transform some highly skewed predictors
    step_BoxCox(Lot_Area, Gr_Liv_Area) %&gt;%
    # Lump some rarely occurring categories into "other"
    step_other(Neighborhood, threshold = 0.05)  %&gt;%
    # Encode categorical predictors as binary.
    step_dummy(all_nominal_predictors(), one_hot = TRUE) %&gt;%
    # Add an interaction effect:
    step_interact(~ starts_with("Central_Air"):Year_Built) %&gt;%
    step_zv(all_predictors()) %&gt;%
    step_normalize(all_numeric_predictors())

 set.seed(2)
 fit &lt;- brulee_linear_reg(ames_rec, data = ames_train,
                           epochs = 5, batch_size = 32)
 fit

 autoplot(fit)

 library(ggplot2)

 predict(fit, ames_test) %&gt;%
   bind_cols(ames_test) %&gt;%
   ggplot(aes(x = .pred, y = Sale_Price)) +
   geom_abline(col = "green") +
   geom_point(alpha = .3) +
   lims(x = c(4, 6), y = c(4, 6)) +
   coord_fixed(ratio = 1)

 library(yardstick)
 predict(fit, ames_test) %&gt;%
   bind_cols(ames_test) %&gt;%
   rmse(Sale_Price, .pred)

 }


</code></pre>


</div>