<div class="container">

<table style="width: 100%;"><tr>
<td>blockfor</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Random Forest variants for block-structured covariate data</h2>

<h3>Description</h3>

<p>Implements five Random Forest variants for prediction
of binary, survival and metric outcomes using block-structured covariate
data, for example, clinical covariates plus measurements of a certain omics data type
or multi-omics data, that is, data for which measurements of different types of omics data
and/or clinical data for each patient exist. For example, for the task of predicting
survival for each patient there might be available
clinical covariates, gene expression measurements, mutation data,
and copy number variation measurements. <br>
The group of covariates corresponding to one specific data type is denoted as a 'block'. <br>
NOTE: We strongly recommend using the variant "BlockForest" (or "block forest")
in applications. The other four variants performed worse than "BlockForest"
in the analysis by Hornung &amp; Wright (2019). Using 20 real multi-omics data sets Hornung &amp; Wright (2019) compared all
five variants with each other and with alternatives, in particular with Random Survival Forest as existing
reference method. The ranking of the performances of the five variants was as follows
in the comparison study by Hornung &amp; Wright (2019): 1) "BlockForest", 2) "RandomBlock",
3) "BlockVarSel", 4) "VarProb", 5) "SplitWeights". <br>
Each of the five variants uses a different split selection algorithm.
For details, see Hornung &amp; Wright (2019). <br>
Note that this R package is a fork of the R package ranger. <br>
NOTE ALSO: Including the clinical block mandatorily in the split point selection can considerably improve the prediction performance.
Whether or not this is the case, depends on the level of predictive information contained in the clinical block.
We recommend trying out including the clinical block mandatorily to see, whether this improves prediction
performance in the particular application. Note that in the case of including the clinical block mandatorily
and having more than only one omics block, "RandomBlock" performed (slightly) better than "BlockForest" in the comparison study by Hornung &amp; Wright (2019). 
Including the clinical block mandatorily can be performed by setting the function argument 'always.select.block'
of 'blockfor()' to the index of the clinical block (e.g., if the clinical block would be the second block in order, we would 
set always.select.block=2).
</p>


<h3>Usage</h3>

<pre><code class="language-R">blockfor(
  X,
  y,
  blocks,
  block.method = "BlockForest",
  num.trees = 2000,
  mtry = NULL,
  nsets = 300,
  num.trees.pre = 1500,
  splitrule = "extratrees",
  always.select.block = 0,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>Covariate matrix. observations in rows, variables in columns.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>Target variable. If the outcome is binary, this is a factor with
two levels. If the outcome is metric, this is a numeric vector. If the outcome
is a survival outcome, this is a matrix with two columns, where the first column
contains the vector of survival/censoring times (one for each observation) and the second column contains
the status variable, that has the value '1' if the corresponding time is
a survival time and '0' if that time is a censoring time.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>blocks</code></td>
<td>
<p>A list of length equal to the number M of blocks considered. Each
entry contains the vector of column indices in 'X' of the covariates in one of the M blocks.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>block.method</code></td>
<td>
<p>Forest variant to use. One of the following: "BlockForest" (default), "RandomBlock", "BlockVarSel", "VarProb", "SplitWeights".
The latter listing is ordered according to the performances of these variants in the comparison study by Hornung &amp; Wright (2019),
with the best variant being listed first.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>num.trees</code></td>
<td>
<p>Number of trees in the forest.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mtry</code></td>
<td>
<p>This is either a number specifying the number of variables sampled for each
split from all variables (for variants "VarProb" and "SplitWeights")
or a vector of length equal to the number of blocks, where the m-th entry of the
vector gives the number of variables to sample from block m (for variants "BlockForest", "RandomBlock", and "BlockVarSel").
The default values are sqrt(p_1) + sqrt(p_2) + ... sqrt(p_M) and (sqrt(p_1), sqrt(p_2), ..., sqrt(p_M)), respectively,
where p_m denotes the number of variables in the m-th block (m = 1, ..., M) and sqrt() denoted the square root function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nsets</code></td>
<td>
<p>Number of sets of tuning parameter values generated randomly in the optimization of the tuning parameters.
Each variant has a tuning parameter for each block, that is, there are M tuning parameters for each variant.
These tuning parameters are optimized in the following way: 1. Generate random sets of tuning parameter values
and measure there adequateness: For j = 1,..., nsets: a) Generate a random set of tuning parameter values;
b) Construct a forest (with num.trees.pre trees) using the set of tuning parameter values generated in a);
c) Record the out-of-bag (OOB) estimated prediction error of the forest constructed in b); 2. Use the set of tuning 
parameter values generated in 1. that is associated with the smallest OOB estimated prediction error.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>num.trees.pre</code></td>
<td>
<p>Number of trees in each forest constructed during the optimization of the tuning
parameter values, see 'nsets' for details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>splitrule</code></td>
<td>
<p>Splitting rule. Default "extratrees" (for computational efficiency). For other options see <code>blockForest</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>always.select.block</code></td>
<td>
<p>Number of block to make always available for splitting (e.g. clinical covariates).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Parameters passed to <code>blockForest</code>, such as <code>num.threads</code>, etc. See <code>blockForest</code> for details.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p><code>blockfor</code> returns a list containing the following components: 
</p>
<table>
<tr style="vertical-align: top;">
<td><code>forest</code></td>
<td>
<p> object of class <code>"blockForest"</code>. Constructed forest.  </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>paramvalues</code></td>
<td>
<p> vector of length M. Optimized tuning parameter value for each block. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>biased_oob_error_donotuse</code></td>
<td>
<p> numeric. OOB estimated prediction error. NOTE: This estimate should not be used, because it is (highly) optimistic (i.e, too small), because the data set was used twice - for optimizing the tuning parameter values and for estimating the prediction error. Instead, cross-validation should be used to estimate the prediction error. </p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Roman Hornung, Marvin N. Wright
</p>


<h3>References</h3>


<ul>
<li>
<p> Hornung, R. &amp; Wright, M. N. (2019) Block Forests: random forests for blocks of clinical and omics covariate data. BMC Bioinformatics 20:358. <a href="https://doi.org/10.1186/s12859-019-2942-y">doi:10.1186/s12859-019-2942-y</a>.
</p>
</li>
<li>
<p> Breiman, L. (2001). Random forests. Mach Learn, 45(1), 5-32. <a href="https://doi.org/10.1023/A%3A1010933404324">doi:10.1023/A:1010933404324</a>. 
</p>
</li>
<li>
<p> Wright, M. N. &amp; Ziegler, A. (2017). ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R. J Stat Softw 77:1-17. <a href="https://doi.org/10.18637/jss.v077.i01">doi:10.18637/jss.v077.i01</a>.
</p>
</li>
</ul>
<h3>Examples</h3>

<pre><code class="language-R"># NOTE: There is no association between covariates and response for the
# simulated data below.
# Moreover, the input parameters of blockfor() are highly unrealistic
# (e.g., nsets = 10 is specified much too small).
# The purpose of the shown examples is merely to illustrate the
# application of blockfor().


# Generate data:
################

set.seed(1234)

# Covariate matrix:
X &lt;- cbind(matrix(nrow=40, ncol=5, data=rnorm(40*5)), 
           matrix(nrow=40, ncol=30, data=rnorm(40*30, mean=1, sd=2)),
           matrix(nrow=40, ncol=100, data=rnorm(40*100, mean=2, sd=3)))

# Block variable (list):
blocks &lt;- rep(1:3, times=c(5, 30, 100))
blocks &lt;- lapply(1:3, function(x) which(blocks==x))

# Binary outcome:
ybin &lt;- factor(sample(c(0,1), size=40, replace=TRUE), levels=c(0,1))

# Survival outcome:
ysurv &lt;- cbind(rnorm(40), sample(c(0,1), size=40, replace=TRUE))

# Application to binary outcome:
################################

blockforobj &lt;- blockfor(X, ybin, num.trees = 100, replace = TRUE, blocks=blocks,
                        nsets = 10, num.trees.pre = 50, splitrule="extratrees", 
                        block.method = "BlockForest")
# Tuning parameter estimates (see Hornung &amp; Wright (2019)):
blockforobj$paramvalues

# Application to survival outcome:
##################################

blockforobj &lt;- blockfor(X, ysurv, num.trees = 100, replace = TRUE, blocks=blocks,
                        nsets = 10, num.trees.pre = 50, splitrule="extratrees", 
                        block.method = "BlockForest")
blockforobj$paramvalues

</code></pre>


</div>