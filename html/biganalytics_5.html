<div class="container">

<table style="width: 100%;"><tr>
<td>bigkmeans</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Memory-efficient k-means cluster analysis</h2>

<h3>Description</h3>

<p>k-means cluster analysis without the memory overhead, and 
possibly in parallel using shared memory.
</p>


<h3>Usage</h3>

<pre><code class="language-R">bigkmeans(x, centers, iter.max = 10, nstart = 1, dist = "euclid")
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>a <code>big.matrix</code> object.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>centers</code></td>
<td>
<p>a scalar denoting the number of clusters, or for k clusters, 
a k by <code>ncol(x)</code> matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iter.max</code></td>
<td>
<p>the maximum number of iterations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nstart</code></td>
<td>
<p>number of random starts, to be done in parallel if there 
is a registered backend (see below).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dist</code></td>
<td>
<p>the distance function. Can be "euclid" or "cosine".</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The real benefit is the lack of memory overhead compared to the 
standard <code>kmeans</code> function.  Part of the overhead from 
<code>kmeans()</code> stems from the way it looks for unique starting 
centers, and could be improved upon.  The <code>bigkmeans()</code> function 
works on either regular <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> <code>matrix</code> objects, or on <code>big.matrix</code> 
objects.  In either case, it requires no extra memory (beyond the data, 
other than recording the cluster memberships), whereas <code>kmeans()</code> 
makes at least two extra copies of the data.  And <code>kmeans()</code> is even 
worse if multiple starts (<code>nstart&gt;1</code>) are used.  If <code>nstart&gt;1</code> 
and you are using <code>bigkmeans()</code> in parallel, a vector of cluster 
memberships will need to be stored for each worker, which could be 
memory-intensive for large data.  This isn't a problem if you use are running
the multiple starts sequentially.
</p>
<p>Unless you have a really big data set (where a single run of 
<code>kmeans</code> not only burns memory but takes more than a few 
seconds), use of parallel computing for multiple random starts is unlikely 
to be much faster than running iteratively.
</p>
<p>Only the algorithm by MacQueen is used here.
</p>


<h3>Value</h3>

<p>An object of class <code>kmeans</code>, just as produced by 
<code>kmeans</code>.
</p>


<h3>Note</h3>

<p>A comment should be made about the excellent package <span class="pkg">foreach</span>.  By
default, it provides <code>foreach</code>, which is used
much like a <code>for</code> loop, here over the <code>nstart</code>
and doing a final comparison of all results).
</p>
<p>When a parallel backend has been registered (see packages <span class="pkg">doSNOW</span>, 
<span class="pkg">doMC</span>, and <span class="pkg">doMPI</span>, for example), <code>bigkmeans()</code> automatically 
distributes the <code>nstart</code> random starting points across the available 
workers.  This is done in shared memory on an SMP, but is distributed on 
a cluster *IF* the <code>big.matrix</code> is file-backed.  If used on a cluster 
with an in-RAM <code>big.matrix</code>, it will fail horribly.  We're considering 
an extra option as an alternative to the current behavior.
</p>


</div>