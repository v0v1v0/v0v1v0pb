<div class="container">

<table style="width: 100%;"><tr>
<td>Detroit</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Detroit homicide data for 1961-73 used in the book Subset Regression by
A.J. Miller</h2>

<h3>Description</h3>

<p>For convenience we have labelled the input variables 1 through 11 to be 
consistent with the notation used in Miller (2002).
Only the first 11 variables were used in Miller's analyses.
The best fitting subset regression with these 11 variables, uses only 3 inputs 
and has a residual sum of squares of
6.77 while using forward selection produces a best fit with 3 inputs with 
residual sum of squares 21.19.  
Backward selection and stagewise methods produce similar results.
It is remarkable that there is such a big difference.
Note that the usual forward and backward selection algorithms may fail since
the linear regression using 11 variables gives essentially a perfect fit.
</p>


<h3>Usage</h3>

<pre><code class="language-R">data(Detroit)</code></pre>


<h3>Format</h3>

<p>A data frame with 13 observations on the following 14 variables.
</p>

<dl>
<dt><code>FTP.1</code></dt>
<dd>
<p>Full-time police per 100,000 population</p>
</dd>
<dt><code>UEMP.2</code></dt>
<dd>
<p>Percent unemployed in the population</p>
</dd>
<dt><code>MAN.3</code></dt>
<dd>
<p>Number of manufacturing workers in thousands</p>
</dd>
<dt><code>LIC.4</code></dt>
<dd>
<p>Number of handgun licences per 100,000 population</p>
</dd>
<dt><code>GR.5</code></dt>
<dd>
<p>Number of handgun registrations per 100,000 population</p>
</dd>
<dt><code>CLEAR.6</code></dt>
<dd>
<p>Percent homicides cleared by arrests</p>
</dd>
<dt><code>WM.7</code></dt>
<dd>
<p>Number of white males in the population</p>
</dd>
<dt><code>NMAN.8</code></dt>
<dd>
<p>Number of non-manufacturing workers in thousands</p>
</dd>
<dt><code>GOV.9</code></dt>
<dd>
<p>Number of government workers in thousands</p>
</dd>
<dt><code>HE.10</code></dt>
<dd>
<p>Average hourly earnings</p>
</dd>
<dt><code>WE.11</code></dt>
<dd>
<p>Average weekly earnings</p>
</dd>
<dt><code>ACC</code></dt>
<dd>
<p>Death rate in accidents per 100,000 population</p>
</dd>
<dt><code>ASR</code></dt>
<dd>
<p>Number of assaults per 100,000 population</p>
</dd>
<dt><code>HOM</code></dt>
<dd>
<p>Number of homicides per 100,000 of population</p>
</dd>
</dl>
<h3>Details</h3>

<p>The data were orginally collected and discussed by Fisher (1976) but
the complete dataset first appeared in Gunst and Mason (1980, Appendix A).
Miller (2002) discusses this dataset throughout his book.
The data were obtained from StatLib.
</p>


<h3>Source</h3>

<p><a href="http://lib.stat.cmu.edu/datasets/detroit">http://lib.stat.cmu.edu/datasets/detroit</a>
</p>


<h3>References</h3>

<p>Fisher, J.C. (1976).  Homicide in Detroit: The Role of Firearms. Criminology, 
vol.14, 387-400.
</p>
<p>Gunst, R.F. and Mason, R.L. (1980). 
Regression analysis and its application: A data-oriented approach. 
Marcel Dekker.
</p>
<p>Miller, A. J. (2002). Subset Selection in Regression. 2nd Ed. 
Chapman &amp; Hall/CRC. Boca Raton.
</p>


<h3>Examples</h3>

<pre><code class="language-R">#Detroit data example
data(Detroit)
#As in Miller (2002) columns 1-11 are used as inputs
p&lt;-11
#For possible comparison with other algorithms such as LARS
#  it is preferable to work with the scaled inputs.
#From Miller (2002, Table 3.14), we see that the
#best six inputs are: 1, 2, 4, 6, 7, 11
X&lt;-as.data.frame(scale(Detroit[,c(1,2,4,6,7,11)]))
y&lt;-Detroit[,ncol(Detroit)]
Xy&lt;-cbind(X,HOM=y)
#Use backward stepwise regression with BIC selects full model
out &lt;- lm(HOM~., data=Xy)
step(out, k=log(nrow(Xy)))
#
#Same story with exhaustive search algorithm
out&lt;-bestglm(Xy, IC="BIC")
out
#But many coefficients have p-values that are quite large considering
#  the selection bias. Note: 1, 6 and 7 are all about 5% only.
#We can use BICq to reduce the number of variables.
#The qTable let's choose q for other possible models,
out$qTable
#This suggest we try q=0.05 or q=0.0005 
bestglm(Xy,IC="BICq", t=0.05)
bestglm(Xy,IC="BICq", t=0.00005)
#It is interesting that the subset model of size 2 is not a subset
# itself of the size 3 model. These results agree with 
#Miller (2002, Table 3.14).
#
#Using delete-d CV with d=4 suggests variables 2,4,6,11
set.seed(1233211)
bestglm(Xy, IC="CV", CVArgs=list(Method="d", K=4, REP=50))
</code></pre>


</div>