<div class="container">

<table style="width: 100%;"><tr>
<td>bmixnorm</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>	Sampling algorithm for mixture of Normal distributions </h2>

<h3>Description</h3>

<p>This function consists of several sampling algorithms for Bayesian estimation for finite a mixture of Normal distributions. 
</p>


<h3>Usage</h3>

<pre><code class="language-R">bmixnorm( data, k = "unknown", iter = 1000, burnin = iter / 2, lambda = 1, 
          k.start = NULL, mu.start = NULL, sig.start = NULL, pi.start = NULL, 
          k.max = 30, trace = TRUE )
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>data     </code></td>
<td>
<p> vector of data with size <code>n</code>.	</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k        </code></td>
<td>
<p> number of components of mixture distribution. It can take an integer values. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iter     </code></td>
<td>
<p> number of iteration for the sampling algorithm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>burnin   </code></td>
<td>
<p> number of burn-in iteration for the sampling algorithm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda   </code></td>
<td>
<p> For the case <code>k = "unknown"</code>, it is the parameter of the prior distribution of number of components <code>k</code>. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k.start  </code></td>
<td>
<p> For the case <code>k = "unknown"</code>, initial value for number of components of mixture distribution.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mu.start </code></td>
<td>
<p> Initial value for parameter of mixture distribution.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sig.start</code></td>
<td>
<p> Initial value for parameter of mixture distribution.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pi.start </code></td>
<td>
<p> Initial value for parameter of mixture distribution.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k.max    </code></td>
<td>
<p> For the case <code>k = "unknown"</code>, maximum value for the number of components of mixture distribution.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trace    </code></td>
<td>
<p> Logical: if TRUE (default), tracing information is printed.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Sampling from finite mixture of Normal distribution, with density:
</p>
<p style="text-align: center;"><code class="reqn">Pr(x|k, \underline{\pi}, \underline{\mu}, \underline{\sigma}) = \sum_{i=1}^{k} \pi_{i} N(x|\mu_{i}, \sigma^2_{i}),</code>
</p>

<p>where <code>k</code> is the number of components of mixture distribution (as a defult we assume is <code>unknown</code>).
The prior distributions are defined as below
</p>
<p style="text-align: center;"><code class="reqn"> P(K=k) \propto \frac{\lambda^k}{k!}, \ \ \ k=1,...,k_{max},</code>
</p>

<p style="text-align: center;"><code class="reqn"> \pi_{i} | k \sim Dirichlet( 1,..., 1 ), </code>
</p>

<p style="text-align: center;"><code class="reqn"> \mu_{i} | k  \sim N( \epsilon, \kappa ), </code>
</p>

<p style="text-align: center;"><code class="reqn"> \sigma_i | k \sim IG( g, h ), </code>
</p>

<p>where <code>IG</code> denotes an inverted gamma distribution. For more details see for more details see Stephens, M. (2000), doi: <a href="https://doi.org/10.1214/aos/1016120364">10.1214/aos/1016120364</a>.
</p>


<h3>Value</h3>

<p>An object with <code>S3</code> class <code>"bmixnorm"</code> is returned:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>all_k      </code></td>
<td>
<p> a vector which includes the waiting times for all iterations. It is needed for monitoring the convergence of the BD-MCMC algorithm. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>all_weights</code></td>
<td>
<p> a vector which includes the waiting times for all iterations. It is needed for monitoring the convergence of the BD-MCMC algorithm. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pi_sample  </code></td>
<td>
<p> a vector which includes the MCMC samples after burn-in from parameter <code>pi</code> of mixture distribution. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mu_sample  </code></td>
<td>
<p> a vector which includes the MCMC samples after burn-in from parameter <code>mu</code> of mixture distribution. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sig_sample </code></td>
<td>
<p> a vector which includes the MCMC samples after burn-in from parameter <code>sig</code> of mixture distribution. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data       </code></td>
<td>
<p> original data. </p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p> Reza Mohammadi <a href="mailto:a.mohammadi@uva.nl">a.mohammadi@uva.nl</a></p>


<h3>References</h3>

<p>Stephens, M. (2000) Bayesian analysis of mixture models with an unknown number of components-an alternative to reversible jump methods. <em>Annals of statistics</em>, 28(1):40-74, doi: <a href="https://doi.org/10.1214/aos/1016120364">10.1214/aos/1016120364</a>
</p>
<p>Richardson, S. and Green, P. J. (1997) On Bayesian analysis of mixtures with an unknown number of components. <em>Journal of the Royal Statistical Society: series B</em>, 59(4):731-792, doi: <a href="https://doi.org/10.1111/1467-9868.00095">10.1111/1467-9868.00095</a>
</p>
<p>Green, P. J. (1995) Reversible jump Markov chain Monte Carlo computation and Bayesian model determination. <em>Biometrika</em>, 82(4):711-732, doi: <a href="https://doi.org/10.1093/biomet/82.4.711">10.1093/biomet/82.4.711</a>
</p>
<p>Cappe, O., Christian P. R., and Tobias, R. (2003) Reversible jump, birth and death and more general continuous time Markov chain Monte Carlo samplers. <em>Journal of the Royal Statistical Society: Series B</em>, 65(3):679-700
</p>
<p>Mohammadi, A., Salehi-Rad, M. R., and Wit, E. C. (2013) Using mixture of Gamma distributions for Bayesian analysis in an M/G/1 queue with optional second service. <em>Computational Statistics</em>, 28(2):683-700, doi: <a href="https://doi.org/10.1007/s00180-012-0323-3">10.1007/s00180-012-0323-3</a>
</p>
<p>Mohammadi, A., and Salehi-Rad, M. R. (2012) Bayesian inference and prediction in an M/G/1 with optional second service. <em>Communications in Statistics-Simulation and Computation</em>, 41(3):419-435, doi: <a href="https://doi.org/10.1080/03610918.2011.588358">10.1080/03610918.2011.588358</a>
</p>
<p>Wade, S. and Ghahramani, Z. (2018) Bayesian Cluster Analysis: Point Estimation and Credible Balls (with Discussion). <em>Bayesian Analysis</em>, 13(2):559-626, doi: <a href="https://doi.org/10.1214/17-BA1073">10.1214/17-BA1073</a>
</p>


<h3>See Also</h3>

 <p><code>bmixt</code>, <code>bmixgamma</code>, <code>rmixnorm</code> </p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
data( galaxy )

set.seed( 70 )
# Runing bdmcmc algorithm for the galaxy dataset      
mcmc_sample = bmixnorm( data = galaxy )

summary( mcmc_sample ) 
plot( mcmc_sample )
print( mcmc_sample)

# simulating data from mixture of Normal with 3 components
n      = 500
weight = c( 0.3, 0.5, 0.2 )
mean   = c( 0  , 10 , 3   )
sd     = c( 1  , 1  , 1   )
    
data = rmixnorm( n = n, weight = weight, mean = mean, sd = sd )
   
# plot for simulation data      
hist( data, prob = TRUE, nclass = 30, col = "gray" )
    
x           = seq( -20, 20, 0.05 )
densmixnorm = dmixnorm( x, weight, mean, sd )
      
lines( x, densmixnorm, lwd = 2 )  
     
# Runing bdmcmc algorithm for the above simulation data set      
bmixnorm.obj = bmixnorm( data, k = 3, iter = 1000 )
    
summary( bmixnorm.obj ) 

## End(Not run)
</code></pre>


</div>