<div class="container">

<table style="width: 100%;"><tr>
<td>BTM</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Construct a Biterm Topic Model on Short Text</h2>

<h3>Description</h3>

<p>The Biterm Topic Model (BTM) is a word co-occurrence based topic model that learns topics by modeling word-word co-occurrences patterns (e.g., biterms)
</p>

<ul>
<li>
<p> A biterm consists of two words co-occurring in the same context, for example, in the same short text window. 
</p>
</li>
<li>
<p> BTM models the biterm occurrences in a corpus (unlike LDA models which model the word occurrences in a document). 
</p>
</li>
<li>
<p> It's a generative model. In the generation procedure, a biterm is generated by drawing two words independently from a same topic z. 
In other words, the distribution of a biterm <code class="reqn">b=(wi,wj)</code> is defined as: <code class="reqn">P(b) = \sum_k{P(wi|z)*P(wj|z)*P(z)}</code> 
where k is the number of topics you want to extract.
</p>
</li>
<li>
<p> Estimation of the topic model is done with the Gibbs sampling algorithm. Where estimates are provided for <code class="reqn">P(w|k)=phi</code> and <code class="reqn">P(z)=theta</code>.
</p>
</li>
</ul>
<h3>Usage</h3>

<pre><code class="language-R">BTM(
  data,
  k = 5,
  alpha = 50/k,
  beta = 0.01,
  iter = 1000,
  window = 15,
  background = FALSE,
  trace = FALSE,
  biterms,
  detailed = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>a tokenised data frame containing one row per token with 2 columns 
</p>

<ul>
<li>
<p> the first column is a context identifier (e.g. a tweet id, a document id, a sentence id, an identifier of a survey answer, an identifier of a part of a text)
</p>
</li>
<li>
<p> the second column is a column called of type character containing the sequence of words occurring within the context identifier 
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>integer with the number of topics to identify</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>numeric, indicating the symmetric dirichlet prior probability of a topic P(z). Defaults to 50/k.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta</code></td>
<td>
<p>numeric, indicating the symmetric dirichlet prior probability of a word given the topic P(w|z). Defaults to 0.01.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iter</code></td>
<td>
<p>integer with the number of iterations of Gibbs sampling</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>window</code></td>
<td>
<p>integer with the window size for biterm extraction. Defaults to 15.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>background</code></td>
<td>
<p>logical if set to <code>TRUE</code>, the first topic is set to a background topic that 
equals to the empirical word distribution. This can be used to filter out common words. Defaults to FALSE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trace</code></td>
<td>
<p>logical indicating to print out evolution of the Gibbs sampling iterations. Defaults to FALSE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>biterms</code></td>
<td>
<p>optionally, your own set of biterms to use for modelling.<br> 
This argument should be a data.frame with column names doc_id, term1, term2 and cooc, indicating how many times each biterm (as indicated by terms term1 and term2) 
is occurring within a certain doc_id. The field cooc indicates how many times this biterm happens with the doc_id. <br>
Note that doc_id's which are not in <code>data</code> are not allowed, as well as terms (in term1 and term2) which are not also in <code>data</code>.
See the examples.<br> 
If provided, the <code>window</code> argument is ignored and the <code>data</code> argument will only be used to calculate the background word frequency distribution.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>detailed</code></td>
<td>
<p>logical indicating to return detailed output containing as well the vocabulary and the biterms used to construct the model. Defaults to FALSE.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>an object of class BTM which is a list containing
</p>

<ul>
<li>
<p>model: a pointer to the C++ BTM model
</p>
</li>
<li>
<p>K: the number of topics
</p>
</li>
<li>
<p>W: the number of tokens in the data
</p>
</li>
<li>
<p>alpha: the symmetric dirichlet prior probability of a topic P(z)
</p>
</li>
<li>
<p>beta: the symmetric dirichlet prior probability of a word given the topic P(w|z)
</p>
</li>
<li>
<p>iter: the number of iterations of Gibbs sampling
</p>
</li>
<li>
<p>background: indicator if the first topic is set to the background topic that equals the empirical word distribution.
</p>
</li>
<li>
<p>theta: a vector with the topic probability p(z) which is determinated by the overall proportions of biterms in it
</p>
</li>
<li>
<p>phi: a matrix of dimension W x K with one row for each token in the data. This matrix contains the probability of the token given the topic P(w|z).
the rownames of the matrix indicate the token w
</p>
</li>
<li>
<p>vocab: a data.frame with columns token and freq indicating the frequency of occurrence of the tokens in <code>data</code>. Only provided in case argument <code>detailed</code> is set to <code>TRUE</code>
</p>
</li>
<li>
<p>biterms: the result of a call to <code>terms</code> with type set to biterms, containing all the biterms used in the model. Only provided in case argument <code>detailed</code> is set to <code>TRUE</code>
</p>
</li>
</ul>
<h3>Note</h3>

<p>A biterm is defined as a pair of words co-occurring in the same text window. 
If you have as an example a document with sequence of words <code>'A B C B'</code>, and assuming the window size is set to 3, 
that implies there are two text windows which can generate biterms namely 
text window <code>'A B C'</code> with biterms <code>'A B', 'B C', 'A C'</code> and text window <code>'B C B'</code> with biterms <code>'B C', 'C B', 'B B'</code>
A biterm is an unorder word pair where <code>'B C' = 'C B'</code>. Thus, the document <code>'A B C B'</code> will have the following biterm frequencies: <br></p>

<ul>
<li>
<p> 'A B': 1 
</p>
</li>
<li>
<p> 'B C': 3
</p>
</li>
<li>
<p> 'A C': 1
</p>
</li>
<li>
<p> 'B B': 1
</p>
</li>
</ul>
<p>These biterms are used to create the model.
</p>


<h3>References</h3>

<p>Xiaohui Yan, Jiafeng Guo, Yanyan Lan, Xueqi Cheng. A Biterm Topic Model For Short Text. WWW2013,
<a href="https://github.com/xiaohuiyan/BTM">https://github.com/xiaohuiyan/BTM</a>, <a href="https://github.com/xiaohuiyan/xiaohuiyan.github.io/blob/master/paper/BTM-WWW13.pdf">https://github.com/xiaohuiyan/xiaohuiyan.github.io/blob/master/paper/BTM-WWW13.pdf</a>
</p>


<h3>See Also</h3>

<p><code>predict.BTM</code>, <code>terms.BTM</code>, <code>logLik.BTM</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
library(udpipe)
data("brussels_reviews_anno", package = "udpipe")
x &lt;- subset(brussels_reviews_anno, language == "nl")
x &lt;- subset(x, xpos %in% c("NN", "NNP", "NNS"))
x &lt;- x[, c("doc_id", "lemma")]
model  &lt;- BTM(x, k = 5, alpha = 1, beta = 0.01, iter = 10, trace = TRUE)
model
terms(model)
scores &lt;- predict(model, newdata = x)

## Another small run with first topic the background word distribution
set.seed(123456)
model &lt;- BTM(x, k = 5, beta = 0.01, iter = 10, background = TRUE)
model
terms(model)

##
## You can also provide your own set of biterms to cluster upon
## Example: cluster nouns and adjectives in the neighbourhood of one another
##
library(data.table)
library(udpipe)
x &lt;- subset(brussels_reviews_anno, language == "nl")
x &lt;- head(x, 5500) # take a sample to speed things up on CRAN
biterms &lt;- as.data.table(x)
biterms &lt;- biterms[, cooccurrence(x = lemma, 
                                  relevant = xpos %in% c("NN", "NNP", "NNS", "JJ"),
                                  skipgram = 2), 
                   by = list(doc_id)]
head(biterms)
set.seed(123456)
x &lt;- subset(x, xpos %in% c("NN", "NNP", "NNS", "JJ"))
x &lt;- x[, c("doc_id", "lemma")]
model &lt;- BTM(x, k = 5, beta = 0.01, iter = 10, background = TRUE, 
             biterms = biterms, trace = 10, detailed = TRUE)
model
terms(model)
bitermset &lt;- terms(model, "biterms")
head(bitermset$biterms, 100)

bitermset$n
sum(biterms$cooc)


## Not run: 
##
## Visualisation either using the textplot or the LDAvis package
##
library(textplot)
library(ggraph)
library(concaveman)
plot(model, top_n = 4)

library(LDAvis)
docsize &lt;- table(x$doc_id)
scores  &lt;- predict(model, x)
scores  &lt;- scores[names(docsize), ]
json &lt;- createJSON(
  phi = t(model$phi), 
  theta = scores, 
  doc.length = as.integer(docsize),
  vocab = model$vocabulary$token, 
  term.frequency = model$vocabulary$freq)
serVis(json)

## End(Not run)
</code></pre>


</div>