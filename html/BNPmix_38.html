<div class="container">

<table style="width: 100%;"><tr>
<td>partition.BNPdens</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Estimate the partition of the data</h2>

<h3>Description</h3>

<p>The <code>partition</code> method estimates the partition of the data based on the output generated by a Bayesian nonparametric mixture
model, according to a specified criterion, for a <code>BNPdens</code> class object.
</p>


<h3>Usage</h3>

<pre><code class="language-R">## S3 method for class 'BNPdens'
partition(object, dist = "VI", max_k = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>an object of class <code>BNPdens</code>;</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dist</code></td>
<td>
<p>a loss function defined on the space of partitions;
it can be variation of information  (<code>"VI"</code>) or <code>"Binder"</code>, default <code>"VI"</code>. See details;</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_k</code></td>
<td>
<p>maximum number of clusters passed to the <code>cutree</code> function. See value below;</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>additional arguments to be passed.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This method returns point estimates for the clustering of the data induced by a nonparametric mixture model.
This result is achieved exploiting two different loss fuctions on the space of partitions: variation of information
(<code>dist = 'VI'</code>) and Binder's loss (<code>dist = 'Binder'</code>). The function is based on the <code>mcclust.ext</code>
code by Sara Wade (Wade and Ghahramani, 2018).
</p>


<h3>Value</h3>

<p>The method returns a list containing a matrix with <code>nrow(data)</code> columns and 3 rows. Each row reports
the cluster labels for each observation according to three different approaches, one per row. The first and second rows
are the output of an agglomerative clustering procedure obtained by applying the function <code>hclust</code>
to the dissimilarity matrix, and by using the complete or average linkage,
respectively. The number of clusters is between 1 and <code>max_k</code> and is choosen according to a lower bound
on the expected loss, as described in Wade and Ghahramani (2018).
The third row reports the partition visited by the MCMC with the minimum distance <code>dist</code> from the dissimilarity matrix.
</p>
<p>In addition, the list reports a vector with three scores representing the lower bound on the expected loss
for the three partitions.
</p>


<h3>References</h3>

<p>Wade, S.,  Ghahramani, Z. (2018). Bayesian cluster analysis: Point estimation and credible balls.
Bayesian Analysis, 13, 559-626.
</p>


<h3>Examples</h3>

<pre><code class="language-R">data_toy &lt;- c(rnorm(10, -3, 1), rnorm(10, 3, 1))
grid &lt;- seq(-7, 7, length.out = 50)
fit &lt;- PYdensity(y = data_toy, mcmc = list(niter = 100,
                      nburn = 10, nupd = 100), output = list(grid = grid))
class(fit)
partition(fit)

</code></pre>


</div>