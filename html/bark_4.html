<div class="container">

<table style="width: 100%;"><tr>
<td>bark</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Nonparametric Regression using Bayesian Additive Regression Kernels</h2>

<h3>Description</h3>

<p>BARK is a Bayesian <em>sum-of-kernels</em> model.<br>
For numeric response <code class="reqn">y</code>, we have
<code class="reqn">y = f(x) + \epsilon</code>,
where <code class="reqn">\epsilon \sim N(0,\sigma^2)</code>.<br>
For a binary response <code class="reqn">y</code>, <code class="reqn">P(Y=1 | x) = F(f(x))</code>,
where <code class="reqn">F</code>
denotes the standard normal cdf (probit link).
<br>
In both cases, <code class="reqn">f</code> is the sum of many Gaussian kernel functions.
The goal is to have very flexible inference for the unknown
function <code class="reqn">f</code>.
BARK uses an approximation to a Cauchy process as the prior distribution
for the unknown function <code class="reqn">f</code>.
</p>
<p>Feature selection can be achieved through the inference
on the scale parameters in the Gaussian kernels.
BARK accepts four different types of prior distributions,
<em>e</em>, <em>d</em>, enabling
either soft shrinkage or  <em>se</em>, <em>sd</em>, enabling hard shrinkage for the scale
parameters.
</p>


<h3>Usage</h3>

<pre><code class="language-R">bark(
  formula,
  data,
  subset,
  na.action = na.omit,
  testdata = NULL,
  selection = TRUE,
  common_lambdas = TRUE,
  classification = FALSE,
  keepevery = 100,
  nburn = 100,
  nkeep = 100,
  printevery = 1000,
  keeptrain = FALSE,
  verbose = FALSE,
  fixed = list(),
  tune = list(lstep = 0.5, frequL = 0.2, dpow = 1, upow = 0, varphistep = 0.5, phistep =
    1),
  theta = list()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p>model formula for the model with all predictors,
Y ~ X.  The X variables will be centered and scaled as part of model fitting.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>a data frame.  Factors will be converted to numerical vectors based on
the using 'model.matrix'.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>subset</code></td>
<td>
<p>an optional vector specifying a subset of observations to be
used in the fitting process.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>na.action</code></td>
<td>
<p>a function which indicates what should happen when the data
contain NAs. The default is "na.omit".</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>testdata</code></td>
<td>
<p>Dataframe with test data for out of sample prediction.<br>
Should have same structure as data.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>selection</code></td>
<td>
<p>Logical variable indicating whether variable 
dependent kernel parameters <code class="reqn">\lambda</code> may be set to zero in the MCMC; 
default is TRUE. <br></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>common_lambdas</code></td>
<td>
<p>Logical variable indicating whether
kernel parameters <code class="reqn">\lambda</code> should be predictor specific or common across
predictors;  default is TRUE.   Note if  <em>common_lambdas = TRUE</em> and 
<em>selection = TRUE</em> this applies just to the non-zero <code class="reqn">lambda_j</code>. <br></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>classification</code></td>
<td>
<p>TRUE/FALSE logical variable,
indicating a classification or regression problem.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>keepevery</code></td>
<td>
<p>Every keepevery draw is kept to be returned to the user</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nburn</code></td>
<td>
<p>Number of MCMC iterations (nburn*keepevery)
to be treated as burn in.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nkeep</code></td>
<td>
<p>Number of MCMC iterations kept for the posterior inference.<br>
nkeep*keepevery iterations after the burn in.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>printevery</code></td>
<td>
<p>As the MCMC runs, a message is printed every printevery draws.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>keeptrain</code></td>
<td>
<p>Logical, whether to keep results for training samples.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>Logical, whether to print out messages</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fixed</code></td>
<td>
<p>A list of fixed hyperparameters, using the default values if not
specified.<br>
alpha = 1: stable index, must be 1 currently.<br>
eps = 0.5: approximation parameter.<br>
gam = 5: intensity parameter.<br>
la = 1: first argument of the gamma prior on kernel scales.<br>
lb = 2: second argument of the gamma prior on kernel scales.<br>
pbetaa = 1: first argument of the beta prior on plambda.<br>
pbetab = 1: second argument of the beta prior on plambda.<br>
n: number of training samples, automatically generates.<br>
p: number of explanatory variables, automatically generates.<br>
meanJ: the expected number of kernels, automatically generates.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tune</code></td>
<td>
<p>A list of tuning parameters, not expected to change.<br>
lstep: the stepsize of the lognormal random walk on lambda.<br>
frequL: the frequency to update L.<br>
dpow: the power on the death step.<br>
upow: the power on the update step.<br>
varphistep: the stepsize of the lognormal random walk on varphi.<br>
phistep: the stepsize of the lognormal random walk on phi.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>theta</code></td>
<td>
<p>A list of the starting values for the parameter theta,
use defaults if nothing is given.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>BARK is implemented using a Bayesian MCMC method.
At each MCMC interaction, we produce a draw from the joint posterior
distribution, i.e. a full configuration of regression coefficients,
kernel locations and kernel parameters.
</p>
<p>Thus, unlike a lot of other modelling methods in R,
we do not produce a single model object
from which fits and summaries may be extracted.
The output consists of values
<code class="reqn">f^*(x)</code> (and <code class="reqn">\sigma^*</code> in the numeric case)
where * denotes a particular draw.
The <code class="reqn">x</code> is either a row from the training data (x.train)
</p>


<h3>Value</h3>

<p><code>bark</code> returns an object of class 'bark' with a list, including:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>call</code></td>
<td>
<p>the matched call</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fixed</code></td>
<td>
<p>Fixed hyperparameters</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tune</code></td>
<td>
<p>Tuning parameters used</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>theta.last</code></td>
<td>
<p>The last set of parameters from the posterior draw</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>theta.nvec</code></td>
<td>
<p>A matrix with nrow(x.train)<code class="reqn">+1</code> rows and (nkeep) columns,
recording the  number of kernels at each training sample</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>theta.varphi</code></td>
<td>
<p> A matrix with nrow(x.train)
<code class="reqn">+1</code> rows and (nkeep) columns,
recording the precision in the normal gamma prior
distribution for the regression coefficients</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>theta.beta</code></td>
<td>
<p>A matrix with nrow(x.train)<code class="reqn">+1</code> rows and (nkeep) columns,
recording the regression coefficients</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>theta.lambda</code></td>
<td>
<p>A matrix with ncol(x.train) rows and (nkeep) columns,
recording the kernel scale parameters</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>thea.phi</code></td>
<td>
<p>The vector of length nkeep,
recording the precision in regression Gaussian noise
(1 for the classification case)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>yhat.train</code></td>
<td>
<p>A matrix with nrow(x.train) rows and (nkeep) columns.
Each column corresponds to a draw <code class="reqn">f^*</code> from
the posterior of <code class="reqn">f</code>
and each row corresponds to a row of x.train.
The <code class="reqn">(i,j)</code> value is <code class="reqn">f^*(x)</code> for
the <code class="reqn">j^{th}</code> kept draw of <code class="reqn">f</code>
and the <code class="reqn">i^{th}</code> row of x.train.<br>
For classification problems, this is the value
of the expectation for the underlying normal
random variable.<br>
Burn-in is dropped</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>yhat.test</code></td>
<td>
<p>Same as yhat.train but now the x's
are the rows of the test data;  NULL if testdata are not provided</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>yhat.train.mean</code></td>
<td>
<p>train data fits = row mean of yhat.train</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>yhat.test.mean</code></td>
<td>
<p>test data fits = row mean of yhat.test</p>
</td>
</tr>
</table>
<h3>References</h3>

<p>Ouyang, Zhi (2008) Bayesian Additive Regression Kernels.
Duke University. PhD dissertation, page 58.
</p>


<h3>See Also</h3>

<p>Other bark functions: 
<code>bark-package</code>,
<code>bark-package-deprecated</code>,
<code>sim_Friedman1()</code>,
<code>sim_Friedman2()</code>,
<code>sim_Friedman3()</code>,
<code>sim_circle()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">##Simulated regression example
# Friedman 2 data set, 200 noisy training, 1000 noise free testing
# Out of sample MSE in SVM (default RBF): 6500 (sd. 1600)
# Out of sample MSE in BART (default):    5300 (sd. 1000)
traindata &lt;- data.frame(sim_Friedman2(200, sd=125))
testdata &lt;- data.frame(sim_Friedman2(1000, sd=0))
# example with a very small number of iterations to illustrate usage
fit.bark.d &lt;- bark(y ~ ., data=traindata, testdata= testdata,
                   nburn=10, nkeep=10, keepevery=10,
                   classification=FALSE, 
                   common_lambdas = FALSE,
                   selection = FALSE)
boxplot(data.frame(fit.bark.d$theta.lambda))
mean((fit.bark.d$yhat.test.mean-testdata$y)^2)

 ##Simulate classification example
 # Circle 5 with 2 signals and three noisy dimensions
 # Out of sample erorr rate in SVM (default RBF): 0.110 (sd. 0.02)
 # Out of sample error rate in BART (default):    0.065 (sd. 0.02)
 traindata &lt;- sim_circle(200, dim=5)
 testdata &lt;- sim_circle(1000, dim=5)
 fit.bark.se &lt;- bark(y ~ ., 
                     data=data.frame(traindata), 
                     testdata= data.frame(testdata), 
                     classification=TRUE,
                     nburn=100, nkeep=200, )
 boxplot(as.data.frame(fit.bark.se$theta.lambda))
 mean((fit.bark.se$yhat.test.mean&gt;0)!=testdata$y)

</code></pre>


</div>