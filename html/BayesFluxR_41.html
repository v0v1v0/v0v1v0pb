<div class="container">

<table style="width: 100%;"><tr>
<td>sampler.HMC</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Standard Hamiltonian Monte Carlo (Hybrid Monte Carlo).</h2>

<h3>Description</h3>

<p>Allows for the use of stochastic gradients, but the validity of doing so is not clear.
</p>


<h3>Usage</h3>

<pre><code class="language-R">sampler.HMC(
  l,
  path_len,
  sadapter = sadapter.DualAverage(1000),
  madapter = madapter.FixedMassMatrix()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>l</code></td>
<td>
<p>stepsize</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>path_len</code></td>
<td>
<p>number of leapfrog steps</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sadapter</code></td>
<td>
<p>Stepsize adapter</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>madapter</code></td>
<td>
<p>Mass adapter</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This is motivated by parts of the discussion in
Neal, R. M. (1996). Bayesian Learning for Neural Networks (Vol. 118). Springer
New York. https://doi.org/10.1007/978-1-4612-0745-0
</p>


<h3>Value</h3>

<p>a list with 'juliavar', 'juliacode', and all given arguments
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(Dense(5, 1))
  like &lt;- likelihood.feedforward_normal(net, Gamma(2.0, 0.5))
  prior &lt;- prior.gaussian(net, 0.5)
  init &lt;- initialise.allsame(Normal(0, 0.5), like, prior)
  x &lt;- matrix(rnorm(5*100), nrow = 5)
  y &lt;- rnorm(100)
  bnn &lt;- BNN(x, y, like, prior, init)
  sadapter &lt;- sadapter.DualAverage(100)
  sampler &lt;- sampler.HMC(1e-3, 3, sadapter = sadapter)
  ch &lt;- mcmc(bnn, 10, 1000, sampler)

## End(Not run)

</code></pre>


</div>