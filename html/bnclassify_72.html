<div class="container">

<table style="width: 100%;"><tr>
<td>learn_params</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Learn the parameters of a Bayesian network structure.</h2>

<h3>Description</h3>

<p>Learn parameters with maximum likelihood or Bayesian estimation, the 
weighting attributes to alleviate naive bayes' independence assumption (WANBIA), 
attribute weighted naive Bayes (AWNB), or the model averaged naive Bayes 
(MANB) methods. Returns a <code>bnc_bn</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">lp(
  x,
  dataset,
  smooth,
  awnb_trees = NULL,
  awnb_bootstrap = NULL,
  manb_prior = NULL,
  wanbia = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>The <code>bnc_dag</code> object. The Bayesian network classifier
structure.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dataset</code></td>
<td>
<p>The data frame from which to learn network parameters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>smooth</code></td>
<td>
<p>A numeric. The smoothing value (<code class="reqn">\alpha</code>) for Bayesian 
parameter estimation. Nonnegative.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>awnb_trees</code></td>
<td>
<p>An integer. The number (<code class="reqn">M</code>) of bootstrap samples to 
generate.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>awnb_bootstrap</code></td>
<td>
<p>A numeric. The size of the bootstrap subsample, 
relative to the size of <code>dataset</code> (given in [0,1]).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>manb_prior</code></td>
<td>
<p>A numeric. The prior probability for an arc between the 
class and any feature.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>wanbia</code></td>
<td>
<p>A logical. If <code>TRUE</code>, WANBIA feature weighting is
performed.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><code>lp</code> learns the parameters of each local distribution <code class="reqn">\theta_{ijk} 
= P(X_i = k \mid \mathbf{Pa}(X_i) = j)</code> as </p>
<p style="text-align: center;"><code class="reqn">\theta_{ijk} = \frac{N_{ijk} + \alpha}{N_{ ij \cdot } + r_i 
\alpha},</code>
</p>
<p> where
<code class="reqn">N_{ijk}</code> is the number of instances in <code>dataset</code> in which 
<code class="reqn">X_i = k</code> and <code class="reqn">\mathbf{Pa}(X_i) = j</code>, 
<code class="reqn">N_{ ij \cdot} = \sum_{k=1}^{r_i} N_{ijk}</code>, <code class="reqn">r_i</code> is the cardinality of <code class="reqn">X_i</code>, and all 
hyperparameters of the Dirichlet prior equal to <code class="reqn">\alpha</code>. <code class="reqn">\alpha = 
0</code> corresponds to maximum likelihood estimation. Returns a uniform 
distribution when <code class="reqn">N_{ i j \cdot } + r_i \alpha = 0</code>. With partially observed data, the above amounts to 
<em>available case analysis</em>.
</p>
<p>WANBIA learns a unique exponent 'weight' per feature. They are 
computed by optimizing conditional log-likelihood, and are bounded with
all <code class="reqn">w_i \in [0, 1]</code>. For WANBIA estimates, set <code>wanbia</code> to <code>TRUE</code>.
</p>
<p>In order to get the AWNB parameter estimate, provide either the 
<code>awnb_bootstrap</code> and/or the <code>awnb_trees</code> argument. The estimate is:
</p>
<p style="text-align: center;"><code class="reqn">\theta_{ijk}^{AWNB} = \frac{\theta_{ijk}^{w_i}}{\sum_{k=1}^{r_i} 
\theta_{ijk}^{w_i}},</code>
</p>
<p> while the weights <code class="reqn">w_i</code> are
computed as </p>
<p style="text-align: center;"><code class="reqn">w_i = \frac{1}{M}\sum_{t=1}^M \sqrt{\frac{1}{d_{ti}}},</code>
</p>
<p> where <code class="reqn">M</code> is the number of 
bootstrap samples from <code>dataset</code> and <code class="reqn">d_{ti}</code> the minimum 
testing depth of <code class="reqn">X_i</code> in an unpruned classification tree learned 
from the <code class="reqn">t</code>-th subsample (<code class="reqn">d_{ti} = 0</code> if <code class="reqn">X_i</code> 
is omitted from <code class="reqn">t</code>-th tree).
</p>
<p>The MANB parameters correspond to Bayesian model averaging over the naive 
Bayes models obtained from all <code class="reqn">2^n</code> subsets over the <code class="reqn">n</code> 
features. To get MANB parameters, provide the <code>manb_prior</code> argument.
</p>


<h3>Value</h3>

<p>A <code>bnc_bn</code> object.
</p>


<h3>References</h3>

<p>Hall M (2004). A decision tree-based attribute weighting filter 
for naive Bayes. <em>Knowledge-based Systems</em>, <b>20</b>(2), 120-126.
</p>
<p>Dash D and Cooper GF (2002). Exact model averaging with naive Bayesian 
classifiers. <em>19th International Conference on Machine Learning 
(ICML-2002)</em>, 91-98.
</p>
<p>Pigott T D (2001) A review of methods for missing data. <em>Educational 
research and evaluation</em>, <b>7</b>(4), 353-383.
</p>


<h3>Examples</h3>

<pre><code class="language-R">data(car)
nb &lt;- nb('class', car)
# Maximum likelihood estimation
mle &lt;- lp(nb, car, smooth = 0)
# Bayesian estimaion
bayes &lt;- lp(nb, car, smooth = 0.5)
# MANB
manb &lt;- lp(nb, car, smooth = 0.5, manb_prior = 0.5)
# AWNB
awnb &lt;- lp(nb, car, smooth = 0.5, awnb_trees = 10)
</code></pre>


</div>