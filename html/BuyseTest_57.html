<div class="container">

<table style="width: 100%;"><tr>
<td>performance</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Assess Performance of a Classifier</h2>

<h3>Description</h3>

<p>Assess the performance in term of AUC and brier score of one or several binary classifiers.
Currently limited to logistic regressions and random forest.
</p>


<h3>Usage</h3>

<pre><code class="language-R">performance(
  object,
  data = NULL,
  newdata = NA,
  individual.fit = FALSE,
  impute = "none",
  name.response = NULL,
  fold.size = 1/10,
  fold.repetition = 0,
  fold.balance = FALSE,
  null = c(brier = NA, AUC = 0.5),
  conf.level = 0.95,
  se = TRUE,
  transformation = TRUE,
  auc.type = "classical",
  simplify = TRUE,
  trace = TRUE,
  seed = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>a <code>glm</code> or <code>range</code> object, or a list of such object.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>[data.frame] the training data.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>newdata</code></td>
<td>
<p>[data.frame] an external data used to assess the performance.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>individual.fit</code></td>
<td>
<p>[logical] if <code>TRUE</code> the predictive model is refit for each individual using only the predictors with non missing values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>impute</code></td>
<td>
<p>[character] in presence of missing value in the regressors of the training dataset, should a complete case analysis be performed (<code>"none"</code>)
or should the median/mean (<code>"median"</code>/<code>"mean"</code>) value be imputed. For categorical variables, the most frequent value is imputed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>name.response</code></td>
<td>
<p>[character] the name of the response variable (i.e. the one containing the categories).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fold.size</code></td>
<td>
<p>[double, &gt;0] either the size of the test dataset (when &gt;1) or the fraction of the dataset (when &lt;1) to be used for testing when using cross-validation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fold.repetition</code></td>
<td>
<p>[integer] when strictly positive, the number of folds used in the cross-validation. If 0 then no cross validation is performed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fold.balance</code></td>
<td>
<p>[logical] should the outcome distribution in the folds of the cross-validation be similar to the one of the original dataset?</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>null</code></td>
<td>
<p>[numeric vector of length 2] the right-hand side of the null hypothesis relative to each metric.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>conf.level</code></td>
<td>
<p>[numeric] confidence level for the confidence intervals.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>se</code></td>
<td>
<p>[logical] should the uncertainty about AUC/brier be computed?
When <code>TRUE</code> adapt the method of LeDell et al. (2015) to repeated cross-validation for the AUC and the brier score.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>transformation</code></td>
<td>
<p>[logical]  should the CI be computed on the logit scale / log scale for the net benefit / win ratio and backtransformed.
Otherwise they are computed without any transformation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>auc.type</code></td>
<td>
<p>[character] should the auc be computed approximating the predicted probability by a dirac (<code>"classical"</code>, usual AUC formula)
or approximating the predicted probability by a normal distribution.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>simplify</code></td>
<td>
<p>[logical] should the number of fold and the size of the fold used for the cross validation be removed from the output?</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trace</code></td>
<td>
<p>[logical] Should the execution of the function be traced.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>
<p>[integer, &gt;0] Random number generator (RNG) state used when starting data spliting.
If <code>NULL</code> no state is set.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>An S3 object of class <code>performance</code>.
</p>


<h3>References</h3>

<p>LeDell E, Petersen M, van der Laan M. Computationally efficient confidence intervals for cross-validated area under the ROC curve estimates. Electron J Stat. 2015;9(1):1583-1607. doi:10.1214/15-EJS1035
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Simulate data
set.seed(10)
n &lt;- 100
df.train &lt;- data.frame(Y = rbinom(n, prob = 0.5, size = 1), X1 = rnorm(n), X2 = rnorm(n))
df.test &lt;- data.frame(Y = rbinom(n, prob = 0.5, size = 1), X1 = rnorm(n), X2 = rnorm(n))

## fit logistic model
e.null &lt;- glm(Y~1, data = df.train, family = binomial(link="logit"))
e.logit1 &lt;- glm(Y~X1, data = df.train, family = binomial(link="logit"))
e.logit2 &lt;- glm(Y~X1+X2, data = df.train, family = binomial(link="logit"))

## assess performance on the training set (biased)
## and external dataset
performance(e.logit1, newdata = df.test)
e.perf &lt;- performance(list(null = e.null, p1 = e.logit1, p2 = e.logit2),
                      newdata = df.test)
e.perf
summary(e.perf, order.model = c("null","p2","p1"))

## assess performance using cross validation
## Not run: 
set.seed(10)
performance(e.logit1, fold.repetition = 10, se = FALSE)
set.seed(10)
performance(list(null = e.null, prop = e.logit1), fold.repetition = 10)
performance(e.logit1, fold.repetition = c(50,20,10))

## End(Not run)
</code></pre>


</div>