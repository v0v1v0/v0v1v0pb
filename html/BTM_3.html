<div class="container">

<table style="width: 100%;"><tr>
<td>predict.BTM</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Predict function for a Biterm Topic Model</h2>

<h3>Description</h3>

<p>Classify new text alongside the biterm topic model.<br></p>
<p>To infer the topics in a document, it is assumed that the topic proportions of a document 
is driven by the expectation of the topic proportions of biterms generated from the document.
</p>


<h3>Usage</h3>

<pre><code class="language-R">## S3 method for class 'BTM'
predict(object, newdata, type = c("sum_b", "sub_w", "mix"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>an object of class BTM as returned by <code>BTM</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>newdata</code></td>
<td>
<p>a tokenised data frame containing one row per token with 2 columns 
</p>

<ul>
<li>
<p> the first column is a context identifier (e.g. a tweet id, a document id, a sentence id, an identifier of a survey answer, an identifier of a part of a text)
</p>
</li>
<li>
<p> the second column is a column called of type character containing the sequence of words occurring within the context identifier 
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p>character string with the type of prediction. 
Either one of 'sum_b', 'sub_w' or 'mix'. Default is set to 'sum_b' as indicated in the paper, 
indicating to sum over the the expectation of the topic proportions of biterms generated from the document. For the other approaches, please inspect the paper.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>not used</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>a matrix containing containing P(z|d) - the probability of the topic given the biterms.<br>
The matrix has one row for each unique doc_id (context identifier)
which contains words part of the dictionary of the BTM model and has K columns, 
one for each topic.
</p>


<h3>References</h3>

<p>Xiaohui Yan, Jiafeng Guo, Yanyan Lan, Xueqi Cheng. A Biterm Topic Model For Short Text. WWW2013,
<a href="https://github.com/xiaohuiyan/BTM">https://github.com/xiaohuiyan/BTM</a>, <a href="https://github.com/xiaohuiyan/xiaohuiyan.github.io/blob/master/paper/BTM-WWW13.pdf">https://github.com/xiaohuiyan/xiaohuiyan.github.io/blob/master/paper/BTM-WWW13.pdf</a>
</p>


<h3>See Also</h3>

<p><code>BTM</code>, <code>terms.BTM</code>, <code>logLik.BTM</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
library(udpipe)
data("brussels_reviews_anno", package = "udpipe")
x &lt;- subset(brussels_reviews_anno, language == "nl")
x &lt;- subset(x, xpos %in% c("NN", "NNP", "NNS"))
x &lt;- x[, c("doc_id", "lemma")]
model  &lt;- BTM(x, k = 5, iter = 5, trace = TRUE)
scores &lt;- predict(model, newdata = x, type = "sum_b")
scores &lt;- predict(model, newdata = x, type = "sub_w")
scores &lt;- predict(model, newdata = x, type = "mix")
head(scores)

</code></pre>


</div>