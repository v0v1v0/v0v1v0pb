<div class="container">

<table style="width: 100%;"><tr>
<td>ComputeKLDs</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Compute signed and symmetric Kullback-Leibler divergence</h2>

<h3>Description</h3>

<p>Compute signed and symmetric Kullback-Leibler divergence of variables over a spectrum of evidence
</p>


<h3>Usage</h3>

<pre><code class="language-R">ComputeKLDs(
  tree,
  var0,
  vars,
  seq,
  pbar = TRUE,
  method = "gaussian",
  epsilon = 10^-6
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>tree</code></td>
<td>
<p>a <code>ClusterTree</code> object</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>var0</code></td>
<td>
<p>the variable to have evidence absrobed</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>vars</code></td>
<td>
<p>the variables to have divergence computed</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seq</code></td>
<td>
<p>a <code>vector</code> of numeric values as the evidences</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pbar</code></td>
<td>
<p><code>logical(1)</code> whether to show progress bar</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>method for divergence computation:
<code>gaussian</code> for Gaussian approximation,  for Monte Carlo integration</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsilon</code></td>
<td>
<p><code>numeric(1)</code> the KL divergence is undefined if certain states of a discrete variable 
have probabilities of 0. In this case, a small positive number epsilon is assigned as their probabilities for
calculating the divergence. The probabilities of other states are shrunked proportionally to ensure they sum up to 1.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Compute signed and symmetric Kullback-Leibler divergence of variables over a spectrum of evidence.
The signed and symmetric Kullback-Leibler divergence is also known as Jeffery's signed information (JSI) for
continuous variables.
</p>


<h3>Value</h3>

<p>a <code>data.frame</code> of the divergence
</p>


<h3>Author(s)</h3>

<p>Han Yu
</p>


<h3>References</h3>

<p>Cowell, R. G. (2005). Local propagation in conditional Gaussian Bayesian networks.
Journal of Machine Learning Research, 6(Sep), 1517-1550. <br><br>
Yu H, Moharil J, Blair RH (2020). BayesNetBP: An R Package for Probabilistic Reasoning in Bayesian
Networks. Journal of Statistical Software, 94(3), 1-31. &lt;doi:10.18637/jss.v094.i03&gt;.
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
data(liver)
tree.init.p &lt;- Initializer(dag=liver$dag, data=liver$data,
                           node.class=liver$node.class,
                           propagate = TRUE)
klds &lt;- ComputeKLDs(tree=tree.init.p, var0="Nr1i3",
                    vars=setdiff(tree.init.p@node, "Nr1i3"),
                    seq=seq(-3,3,0.5))
head(klds)

## End(Not run)
</code></pre>


</div>