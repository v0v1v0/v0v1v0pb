<div class="container">

<table style="width: 100%;"><tr>
<td>brulee_multinomial_reg</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Fit a multinomial regression model</h2>

<h3>Description</h3>

<p><code>brulee_multinomial_reg()</code> fits a model.
</p>


<h3>Usage</h3>

<pre><code class="language-R">brulee_multinomial_reg(x, ...)

## Default S3 method:
brulee_multinomial_reg(x, ...)

## S3 method for class 'data.frame'
brulee_multinomial_reg(
  x,
  y,
  epochs = 20L,
  penalty = 0.001,
  mixture = 0,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 1,
  momentum = 0,
  batch_size = NULL,
  class_weights = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)

## S3 method for class 'matrix'
brulee_multinomial_reg(
  x,
  y,
  epochs = 20L,
  penalty = 0.001,
  mixture = 0,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 1,
  momentum = 0,
  batch_size = NULL,
  class_weights = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)

## S3 method for class 'formula'
brulee_multinomial_reg(
  formula,
  data,
  epochs = 20L,
  penalty = 0.001,
  mixture = 0,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 1,
  momentum = 0,
  batch_size = NULL,
  class_weights = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)

## S3 method for class 'recipe'
brulee_multinomial_reg(
  x,
  data,
  epochs = 20L,
  penalty = 0.001,
  mixture = 0,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 1,
  momentum = 0,
  batch_size = NULL,
  class_weights = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>Depending on the context:
</p>

<ul>
<li>
<p> A <strong>data frame</strong> of predictors.
</p>
</li>
<li>
<p> A <strong>matrix</strong> of predictors.
</p>
</li>
<li>
<p> A <strong>recipe</strong> specifying a set of preprocessing steps
created from <code>recipes::recipe()</code>.
</p>
</li>
</ul>
<p>The predictor data should be standardized (e.g. centered or scaled).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Options to pass to the learning rate schedulers via
<code>set_learn_rate()</code>. For example, the <code>reduction</code> or <code>steps</code> arguments to
<code>schedule_step()</code> could be passed here.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>When <code>x</code> is a <strong>data frame</strong> or <strong>matrix</strong>, <code>y</code> is the outcome
specified as:
</p>

<ul>
<li>
<p> A <strong>data frame</strong> with 1 factor column (with three or more levels).
</p>
</li>
<li>
<p> A <strong>matrix</strong> with 1 factor column (with three or more levels).
</p>
</li>
<li>
<p> A factor <strong>vector</strong> (with three or more levels).
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epochs</code></td>
<td>
<p>An integer for the number of epochs of training.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>penalty</code></td>
<td>
<p>The amount of weight decay (i.e., L2 regularization).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mixture</code></td>
<td>
<p>Proportion of Lasso Penalty (type: double, default: 0.0). A
value of mixture = 1 corresponds to a pure lasso model, while mixture = 0
indicates ridge regression (a.k.a weight decay).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>validation</code></td>
<td>
<p>The proportion of the data randomly assigned to a
validation set.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>optimizer</code></td>
<td>
<p>The method used in the optimization procedure. Possible choices
are 'LBFGS' and 'SGD'. Default is 'LBFGS'.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>learn_rate</code></td>
<td>
<p>A positive number that controls the rapidity that the model
moves along the descent path. Values around 0.1 or less are typical.
(<code>optimizer = "SGD"</code> only)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>momentum</code></td>
<td>
<p>A positive number usually on <code style="white-space: pre;">⁠[0.50, 0.99]⁠</code> for the momentum
parameter in gradient descent.  (<code>optimizer = "SGD"</code> only)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>batch_size</code></td>
<td>
<p>An integer for the number of training set points in each
batch. (<code>optimizer = "SGD"</code> only)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>class_weights</code></td>
<td>
<p>Numeric class weights (classification only). The value
can be:
</p>

<ul>
<li>
<p> A named numeric vector (in any order) where the names are the outcome
factor levels.
</p>
</li>
<li>
<p> An unnamed numeric vector assumed to be in the same order as the outcome
factor levels.
</p>
</li>
<li>
<p> A single numeric value for the least frequent class in the training data
and all other classes receive a weight of one.
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stop_iter</code></td>
<td>
<p>A non-negative integer for how many iterations with no
improvement before stopping.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>A logical that prints out the iteration history.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p>A formula specifying the outcome term(s) on the left-hand side,
and the predictor term(s) on the right-hand side.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>When a <strong>recipe</strong> or <strong>formula</strong> is used, <code>data</code> is specified as:
</p>

<ul><li>
<p> A <strong>data frame</strong> containing both the predictors and the outcome.
</p>
</li></ul>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This function fits a linear combination of coefficients and predictors to
model the log of the class probabilities. The training process optimizes the
cross-entropy loss function.
</p>
<p>By default, training halts when the validation loss increases for at least
<code>step_iter</code> iterations. If <code>validation = 0</code> the training set loss is used.
</p>
<p>The <em>predictors</em> data should all be numeric and encoded in the same units (e.g.
standardized to the same range or distribution). If there are factor
predictors, use a recipe or formula to create indicator variables (or some
other method) to make them numeric. Predictors should be in the same units
before training.
</p>
<p>The model objects are saved for each epoch so that the number of epochs can
be efficiently tuned. Both the <code>coef()</code> and <code>predict()</code> methods for this
model have an <code>epoch</code> argument (which defaults to the epoch with the best
loss value).
</p>
<p>The use of the L1 penalty (a.k.a. the lasso penalty) does <em>not</em> force
parameters to be strictly zero (as it does in packages such as <span class="pkg">glmnet</span>).
The zeroing out of parameters is a specific feature the optimization method
used in those packages.
</p>


<h3>Value</h3>

<p>A <code>brulee_multinomial_reg</code> object with elements:
</p>

<ul>
<li> <p><code>models_obj</code>: a serialized raw vector for the torch module.
</p>
</li>
<li> <p><code>estimates</code>: a list of matrices with the model parameter estimates per
epoch.
</p>
</li>
<li> <p><code>best_epoch</code>: an integer for the epoch with the smallest loss.
</p>
</li>
<li> <p><code>loss</code>: A vector of loss values (MSE for regression, negative log-
likelihood for classification) at each epoch.
</p>
</li>
<li> <p><code>dim</code>: A list of data dimensions.
</p>
</li>
<li> <p><code>parameters</code>: A list of some tuning parameter values.
</p>
</li>
<li> <p><code>blueprint</code>: The <code>hardhat</code> blueprint data.
</p>
</li>
</ul>
<h3>See Also</h3>

<p><code>predict.brulee_multinomial_reg()</code>, <code>coef.brulee_multinomial_reg()</code>,
<code>autoplot.brulee_multinomial_reg()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
if (torch::torch_is_installed()) {

  library(recipes)
  library(yardstick)

  data(penguins, package = "modeldata")

  penguins &lt;- penguins %&gt;% na.omit()

  set.seed(122)
  in_train &lt;- sample(1:nrow(penguins), 200)
  penguins_train &lt;- penguins[ in_train,]
  penguins_test  &lt;- penguins[-in_train,]

  rec &lt;- recipe(island ~ ., data = penguins_train) %&gt;%
    step_dummy(species, sex) %&gt;%
    step_normalize(all_predictors())

  set.seed(3)
  fit &lt;- brulee_multinomial_reg(rec, data = penguins_train, epochs = 5)
  fit

  predict(fit, penguins_test) %&gt;%
    bind_cols(penguins_test) %&gt;%
    conf_mat(island, .pred_class)
}

</code></pre>


</div>