<div class="container">

<table style="width: 100%;"><tr>
<td>brulee_mlp</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Fit neural networks</h2>

<h3>Description</h3>

<p><code>brulee_mlp()</code> fits neural network models using stochastic gradient
descent. Multiple layers can be used.
</p>


<h3>Usage</h3>

<pre><code class="language-R">brulee_mlp(x, ...)

## Default S3 method:
brulee_mlp(x, ...)

## S3 method for class 'data.frame'
brulee_mlp(
  x,
  y,
  epochs = 100L,
  hidden_units = 3L,
  activation = "relu",
  penalty = 0.001,
  mixture = 0,
  dropout = 0,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 0.01,
  rate_schedule = "none",
  momentum = 0,
  batch_size = NULL,
  class_weights = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)

## S3 method for class 'matrix'
brulee_mlp(
  x,
  y,
  epochs = 100L,
  hidden_units = 3L,
  activation = "relu",
  penalty = 0.001,
  mixture = 0,
  dropout = 0,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 0.01,
  rate_schedule = "none",
  momentum = 0,
  batch_size = NULL,
  class_weights = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)

## S3 method for class 'formula'
brulee_mlp(
  formula,
  data,
  epochs = 100L,
  hidden_units = 3L,
  activation = "relu",
  penalty = 0.001,
  mixture = 0,
  dropout = 0,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 0.01,
  rate_schedule = "none",
  momentum = 0,
  batch_size = NULL,
  class_weights = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)

## S3 method for class 'recipe'
brulee_mlp(
  x,
  data,
  epochs = 100L,
  hidden_units = 3L,
  activation = "relu",
  penalty = 0.001,
  mixture = 0,
  dropout = 0,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 0.01,
  rate_schedule = "none",
  momentum = 0,
  batch_size = NULL,
  class_weights = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>Depending on the context:
</p>

<ul>
<li>
<p> A <strong>data frame</strong> of predictors.
</p>
</li>
<li>
<p> A <strong>matrix</strong> of predictors.
</p>
</li>
<li>
<p> A <strong>recipe</strong> specifying a set of preprocessing steps
created from <code>recipes::recipe()</code>.
</p>
</li>
</ul>
<p>The predictor data should be standardized (e.g. centered or scaled).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Options to pass to the learning rate schedulers via
<code>set_learn_rate()</code>. For example, the <code>reduction</code> or <code>steps</code> arguments to
<code>schedule_step()</code> could be passed here.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>When <code>x</code> is a <strong>data frame</strong> or <strong>matrix</strong>, <code>y</code> is the outcome
specified as:
</p>

<ul>
<li>
<p> A <strong>data frame</strong> with 1 column (numeric or factor).
</p>
</li>
<li>
<p> A <strong>matrix</strong> with numeric column  (numeric or factor).
</p>
</li>
<li>
<p> A  <strong>vector</strong>  (numeric or factor).
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epochs</code></td>
<td>
<p>An integer for the number of epochs of training.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>hidden_units</code></td>
<td>
<p>An integer for the number of hidden units, or a vector
of integers. If a vector of integers, the model will have <code>length(hidden_units)</code>
layers each with <code>hidden_units[i]</code> hidden units.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>activation</code></td>
<td>
<p>A character vector for the activation function )such as
"relu", "tanh", "sigmoid", and so on). See <code>brulee_activations()</code> for
a list of possible values. If <code>hidden_units</code> is a vector, <code>activation</code>
can be a character vector with length equals to <code>length(hidden_units)</code>
specifying the activation for each hidden layer.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>penalty</code></td>
<td>
<p>The amount of weight decay (i.e., L2 regularization).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mixture</code></td>
<td>
<p>Proportion of Lasso Penalty (type: double, default: 0.0). A
value of mixture = 1 corresponds to a pure lasso model, while mixture = 0
indicates ridge regression (a.k.a weight decay).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dropout</code></td>
<td>
<p>The proportion of parameters set to zero.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>validation</code></td>
<td>
<p>The proportion of the data randomly assigned to a
validation set.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>optimizer</code></td>
<td>
<p>The method used in the optimization procedure. Possible choices
are 'LBFGS' and 'SGD'. Default is 'LBFGS'.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>learn_rate</code></td>
<td>
<p>A positive number that controls the initial rapidity that
the model moves along the descent path. Values around 0.1 or less are
typical.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rate_schedule</code></td>
<td>
<p>A single character value for how the learning rate
should change as the optimization proceeds. Possible values are
<code>"none"</code> (the default), <code>"decay_time"</code>, <code>"decay_expo"</code>, <code>"cyclic"</code> and
<code>"step"</code>. See <code>schedule_decay_time()</code> for more details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>momentum</code></td>
<td>
<p>A positive number usually on <code style="white-space: pre;">⁠[0.50, 0.99]⁠</code> for the momentum
parameter in gradient descent.  (<code>optimizer = "SGD"</code> only)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>batch_size</code></td>
<td>
<p>An integer for the number of training set points in each
batch. (<code>optimizer = "SGD"</code> only)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>class_weights</code></td>
<td>
<p>Numeric class weights (classification only). The value
can be:
</p>

<ul>
<li>
<p> A named numeric vector (in any order) where the names are the outcome
factor levels.
</p>
</li>
<li>
<p> An unnamed numeric vector assumed to be in the same order as the outcome
factor levels.
</p>
</li>
<li>
<p> A single numeric value for the least frequent class in the training data
and all other classes receive a weight of one.
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stop_iter</code></td>
<td>
<p>A non-negative integer for how many iterations with no
improvement before stopping.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>A logical that prints out the iteration history.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p>A formula specifying the outcome term(s) on the left-hand side,
and the predictor term(s) on the right-hand side.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>When a <strong>recipe</strong> or <strong>formula</strong> is used, <code>data</code> is specified as:
</p>

<ul><li>
<p> A <strong>data frame</strong> containing both the predictors and the outcome.
</p>
</li></ul>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This function fits feed-forward neural network models for regression (when
the outcome is a number) or classification (a factor). For regression, the
mean squared error is optimized and cross-entropy is the loss function for
classification.
</p>
<p>When the outcome is a number, the function internally standardizes the
outcome data to have mean zero and a standard deviation of one. The prediction
function creates predictions on the original scale.
</p>
<p>By default, training halts when the validation loss increases for at least
<code>step_iter</code> iterations. If <code>validation = 0</code> the training set loss is used.
</p>
<p>The <em>predictors</em> data should all be numeric and encoded in the same units (e.g.
standardized to the same range or distribution). If there are factor
predictors, use a recipe or formula to create indicator variables (or some
other method) to make them numeric. Predictors should be in the same units
before training.
</p>
<p>The model objects are saved for each epoch so that the number of epochs can
be efficiently tuned. Both the <code>coef()</code> and <code>predict()</code> methods for this
model have an <code>epoch</code> argument (which defaults to the epoch with the best
loss value).
</p>
<p>The use of the L1 penalty (a.k.a. the lasso penalty) does <em>not</em> force
parameters to be strictly zero (as it does in packages such as <span class="pkg">glmnet</span>).
The zeroing out of parameters is a specific feature the optimization method
used in those packages.
</p>


<h4>Learning Rates</h4>

<p>The learning rate can be set to constant (the default) or dynamically set
via a learning rate scheduler (via the <code>rate_schedule</code>). Using
<code>rate_schedule = 'none'</code> uses the <code>learn_rate</code> argument. Otherwise, any
arguments to the schedulers can be passed via <code>...</code>.
</p>



<h3>Value</h3>

<p>A <code>brulee_mlp</code> object with elements:
</p>

<ul>
<li> <p><code>models_obj</code>: a serialized raw vector for the torch module.
</p>
</li>
<li> <p><code>estimates</code>: a list of matrices with the model parameter estimates per
epoch.
</p>
</li>
<li> <p><code>best_epoch</code>: an integer for the epoch with the smallest loss.
</p>
</li>
<li> <p><code>loss</code>: A vector of loss values (MSE for regression, negative log-
likelihood for classification) at each epoch.
</p>
</li>
<li> <p><code>dim</code>: A list of data dimensions.
</p>
</li>
<li> <p><code>y_stats</code>: A list of summary statistics for numeric outcomes.
</p>
</li>
<li> <p><code>parameters</code>: A list of some tuning parameter values.
</p>
</li>
<li> <p><code>blueprint</code>: The <code>hardhat</code> blueprint data.
</p>
</li>
</ul>
<h3>See Also</h3>

<p><code>predict.brulee_mlp()</code>, <code>coef.brulee_mlp()</code>, <code>autoplot.brulee_mlp()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
if (torch::torch_is_installed()) {

 ## -----------------------------------------------------------------------------
 # regression examples (increase # epochs to get better results)

 data(ames, package = "modeldata")

 ames$Sale_Price &lt;- log10(ames$Sale_Price)

 set.seed(122)
 in_train &lt;- sample(1:nrow(ames), 2000)
 ames_train &lt;- ames[ in_train,]
 ames_test  &lt;- ames[-in_train,]


 # Using matrices
 set.seed(1)
 fit &lt;-
   brulee_mlp(x = as.matrix(ames_train[, c("Longitude", "Latitude")]),
               y = ames_train$Sale_Price, penalty = 0.10)

 # Using recipe
 library(recipes)

 ames_rec &lt;-
  recipe(Sale_Price ~ Bldg_Type + Neighborhood + Year_Built + Gr_Liv_Area +
         Full_Bath + Year_Sold + Lot_Area + Central_Air + Longitude + Latitude,
         data = ames_train) %&gt;%
   # Transform some highly skewed predictors
   step_BoxCox(Lot_Area, Gr_Liv_Area) %&gt;%
   # Lump some rarely occurring categories into "other"
   step_other(Neighborhood, threshold = 0.05)  %&gt;%
   # Encode categorical predictors as binary.
   step_dummy(all_nominal_predictors(), one_hot = TRUE) %&gt;%
   # Add an interaction effect:
   step_interact(~ starts_with("Central_Air"):Year_Built) %&gt;%
   step_zv(all_predictors()) %&gt;%
   step_normalize(all_numeric_predictors())

 set.seed(2)
 fit &lt;- brulee_mlp(ames_rec, data = ames_train, hidden_units = 20,
                    dropout = 0.05, rate_schedule = "cyclic", step_size = 4)
 fit

 autoplot(fit)

 library(ggplot2)

 predict(fit, ames_test) %&gt;%
   bind_cols(ames_test) %&gt;%
   ggplot(aes(x = .pred, y = Sale_Price)) +
   geom_abline(col = "green") +
   geom_point(alpha = .3) +
   lims(x = c(4, 6), y = c(4, 6)) +
   coord_fixed(ratio = 1)

 library(yardstick)
 predict(fit, ames_test) %&gt;%
   bind_cols(ames_test) %&gt;%
   rmse(Sale_Price, .pred)


 # ------------------------------------------------------------------------------
 # classification

 library(dplyr)
 library(ggplot2)

 data("parabolic", package = "modeldata")

 set.seed(1)
 in_train &lt;- sample(1:nrow(parabolic), 300)
 parabolic_tr &lt;- parabolic[ in_train,]
 parabolic_te &lt;- parabolic[-in_train,]

 set.seed(2)
 cls_fit &lt;- brulee_mlp(class ~ ., data = parabolic_tr, hidden_units = 2,
                        epochs = 200L, learn_rate = 0.1, activation = "elu",
                        penalty = 0.1, batch_size = 2^8, optimizer = "SGD")
 autoplot(cls_fit)

 grid_points &lt;- seq(-4, 4, length.out = 100)

 grid &lt;- expand.grid(X1 = grid_points, X2 = grid_points)

 predict(cls_fit, grid, type = "prob") %&gt;%
  bind_cols(grid) %&gt;%
  ggplot(aes(X1, X2)) +
  geom_contour(aes(z = .pred_Class1), breaks = 1/2, col = "black") +
  geom_point(data = parabolic_te, aes(col = class))

 }

</code></pre>


</div>