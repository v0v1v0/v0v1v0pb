<div class="container">

<table style="width: 100%;"><tr>
<td>Bvs</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Bayesian Variable Selection for linear regression models</h2>

<h3>Description</h3>

<p>Exact computation of summaries of the posterior distribution using
sequential computation.
</p>


<h3>Usage</h3>

<pre><code class="language-R">Bvs(
  formula,
  data,
  null.model = paste(as.formula(formula)[[2]], " ~ 1", sep = ""),
  prior.betas = "Robust",
  prior.models = "ScottBerger",
  n.keep = 10,
  time.test = TRUE,
  priorprobs = NULL,
  parallel = FALSE,
  n.nodes = detectCores()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p>Formula defining the most complex (full) regression model in the
analysis. See details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>data frame containing the data.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>null.model</code></td>
<td>
<p>A formula defining which is the simplest (null) model.
It should be nested in the full model. By default, the null model is defined
to be the one with just the intercept.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prior.betas</code></td>
<td>
<p>Prior distribution for regression parameters within each
model (to be literally specified). Possible choices include "Robust", "Robust.G", "Liangetal", "gZellner",
"ZellnerSiow", "FLS", "intrinsic.MGC" and "IHG" (see details).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prior.models</code></td>
<td>
<p>Prior distribution over the model space (to be literally specified). Possible
choices are "Constant", "ScottBerger" and "User" (see details).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.keep</code></td>
<td>
<p>How many of the most probable models are to be kept? By
default is set to 10, which is automatically adjusted if 10 is greater than
the total number of models.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>time.test</code></td>
<td>
<p>If TRUE and the number of variables is moderately large
(&gt;=18) a preliminary test to estimate computational time is performed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>priorprobs</code></td>
<td>
<p>A p+1 (p is the number of non-fixed covariates)
dimensional vector defining the prior probabilities Pr(M_i) (should be used
in the case where <code>prior.models</code>= "User"; see details.)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>parallel</code></td>
<td>
<p>A logical parameter specifying whether parallel computation
must be used (if set to TRUE)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.nodes</code></td>
<td>
<p>The number of cores to be used if parallel computation is used.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The model space is the set of all models, Mi, that contain the intercept and
are nested in that specified by <code>formula</code>. The simplest of such models,
M0, contains only the intercept. Then <code>Bvs</code> provides exact summaries of
the posterior distribution over this model space, that is, summaries of the
discrete distribution which assigns to each model Mi its probability given
the data:
</p>
<p>Pr(Mi | <code>data</code>)=Pr(Mi)*Bi/C,
</p>
<p>where Bi is the Bayes factor of Mi to M0, Pr(Mi) is the prior probability of
Mi and C is the normalizing constant.
</p>
<p>The Bayes factor B_i depends on the prior assigned for the parameters in the regression
models Mi and <code>Bvs</code> implements a number of popular choices. The "Robust"
prior by Bayarri, Berger, Forte and Garcia-Donato (2012) is the recommended (and default) choice.
This prior prior can be implemented in a more stable way using the derivations in Greenaway (2019) 
and that are available in BayesVarSel since version 2.2.x setting the argument to "Robust.G". 
</p>
<p>Additional options are "gZellner" a prior which 
corresponds to the
prior in Zellner (1986) with g=n. Also "Liangetal" prior is the
hyper-g/n with a=3 (see the original paper Liang et al 2008, for details).
"ZellnerSiow" is the multivariate Cauchy prior proposed by Zellner and Siow
(1980, 1984), further studied by Bayarri and Garcia-Donato (2007).
"FLS" is the (benchmark) prior recommended by Fernandez, Ley and Steel (2001) which is
the prior in Zellner (1986) with g=max(n, p*p) p being the number of
covariates to choose from (the most complex model has p+number of fixed
covariates).
"intrinsic.MGC" is the intrinsic prior derived by Moreno, Giron, Casella (2015)
and "IHG" corresponds to the intrinsic hyper-g prior derived in Berger, Garcia-Donato, 
Moreno and Pericchi (2022).
</p>
<p>With respect to the prior over the model space Pr(Mi) three possibilities
are implemented: "Constant", under which every model has the same prior
probability, "ScottBerger" under which Pr(Mi) is inversely proportional to
the number of models of that dimension, and "User" (see below). The
"ScottBerger" prior was studied by Scott and Berger (2010) and controls for
multiplicity (default choice since version 1.7.0).
</p>
<p>When the parameter <code>prior.models</code>="User", the prior probabilities are
defined through the p+1 dimensional parameter vector <code>priorprobs</code>. Let
k be the number of explanatory variables in the simplest model (the one
defined by <code>fixed.cov</code>) then except for the normalizing constant, the
first component of <code>priorprobs</code> must contain the probability of each
model with k covariates (there is only one); the second component of
<code>priorprobs</code> should contain the probability of each model with k+1
covariates and so on. Finally, the p+1 component in <code>priorprobs</code>
defined the probability of the most complex model (that defined by
<code>formula</code>. That is
</p>
<p><code>priorprobs</code>[j]=Cprior*Pr(M_i such that M_i has j-1+k explanatory variables)
</p>
<p>where Cprior is the normalizing constant for the prior, i.e
<code>Cprior=1/sum(priorprobs*choose(p,0:p)</code>.
</p>
<p>Note that <code>prior.models</code>="Constant" is equivalent to the combination
<code>prior.models</code>="User" and <code>priorprobs=rep(1,(p+1))</code> but the
internal functions are not the same and you can obtain small variations in
results due to these differences in the implementation.
</p>
<p>Similarly, <code>prior.models</code> = "ScottBerger" is equivalent to the
combination <code>prior.models</code>= "User" and <code>priorprobs</code> =
<code>1/choose(p,0:p)</code>.
</p>
<p>The case where n&lt;p is handled assigning to the Bayes factors of models with k regressors
with n&lt;k a value of 1. This should be interpreted as a generalization of the
null predictive matching in Bayarri et al (2012). Use <code>GibbsBvs</code> for cases where
p&gt;&gt;.
</p>
<p>Limitations: about the error "A Bayes factor is infinite.". Bayes factors can be
extremely big numbers if i) the sample size is large or if
ii) a competing model is much better (in terms of fit) than the model taken as the
null model. If you see this error, try to use the more stable version of the
robust prior "Robust.g" and/or reconisder using more accurate and realistic definitions
of the simplest model (via the <code>null.model</code> argument).
</p>


<h3>Value</h3>

<p><code>Bvs</code> returns an object of class <code>Bvs</code> with the following
elements: </p>
<table>
<tr style="vertical-align: top;">
<td><code>time </code></td>
<td>
<p>The internal time consumed in solving the problem</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lmfull </code></td>
<td>
<p>The <code>lm</code> class object that results when the model
defined by <code>formula</code> is fitted by <code>lm</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lmnull </code></td>
<td>
<p>The
<code>lm</code> class object that results when the model defined by
<code>null.model</code> is fitted by <code>lm</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>variables </code></td>
<td>
<p>The name of all
the potential explanatory variables (the set of variables to select from).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n </code></td>
<td>
<p>Number of observations</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>p </code></td>
<td>
<p>Number of explanatory variables
to select from</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k </code></td>
<td>
<p>Number of fixed variables</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>HPMbin </code></td>
<td>
<p>The
binary expression of the Highest Posterior Probability model</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>modelsprob </code></td>
<td>
<p>A <code>data.frame</code> which summaries the <code>n.keep</code>
most probable, a posteriori models, and their associated probability.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>inclprob </code></td>
<td>
<p>A named vector with the inclusion probabilities of all
the variables.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>jointinclprob </code></td>
<td>
<p>A <code>data.frame</code> with the joint
inclusion probabilities of all the variables.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>postprobdim </code></td>
<td>
<p>Posterior
probabilities of the dimension of the true model</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>call </code></td>
<td>
<p>The
<code>call</code> to the function</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>C</code></td>
<td>
<p>The value of the normalizing constant (C=sum BiPr(Mi), for Mi in the model space)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method </code></td>
<td>
<p><code>full</code> or <code>parallel</code> in case of
parallel computation</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prior.betas</code></td>
<td>
<p>prior.betas</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prior.models</code></td>
<td>
<p>prior.models</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>priorprobs</code></td>
<td>
<p>priorprobs</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Gonzalo Garcia-Donato and Anabel Forte
</p>
<p>Maintainer: &lt;anabel.forte@uv.es&gt;
</p>


<h3>References</h3>

<p>Bayarri, M.J., Berger, J.O., Forte, A. and Garcia-Donato, G.
(2012)&lt;DOI:10.1214/12-aos1013&gt; Criteria for Bayesian Model choice with
Application to Variable Selection. The Annals of Statistics. 40: 1550-1557.
</p>
<p>Berger, J., Garcıa-Donato, G., Moreno, E., and Pericchi, L. (2022). 
The intrinsic hyper-g prior for normal linear models. in preparation.
</p>
<p>Bayarri, M.J. and Garcia-Donato, G. (2007)&lt;DOI:10.1093/biomet/asm014&gt;
Extending conventional priors for testing general hypotheses in linear
models. Biometrika, 94:135-152.
</p>
<p>Barbieri, M and Berger, J (2004)&lt;DOI:10.1214/009053604000000238&gt; Optimal
Predictive Model Selection. The Annals of Statistics, 32, 870-897.
</p>
<p>Fernandez, C., Ley, E. and Steel, M.F.J.
(2001)&lt;DOI:10.1016/s0304-4076(00)00076-2&gt; Benchmark priors for Bayesian
model averaging. Journal of Econometrics, 100, 381-427.
</p>
<p>Greenaway, M. (2019) Numerically stable approximate Bayesian methods for
generalized linear mixed models and linear model selection. Thesis (Department of Statistics,
University of Sydney).
</p>
<p>Liang, F., Paulo, R., Molina, G., Clyde, M. and Berger,J.O.
(2008)&lt;DOI:10.1198/016214507000001337&gt; Mixtures of g-priors for Bayesian
Variable Selection. Journal of the American Statistical Association.
103:410-423
</p>
<p>Moreno, E., Giron, J. and Casella, G. (2015) Posterior model consistency
in variable selection as the model dimension grows. Statistical Science. 30: 228-241.
</p>
<p>Zellner, A. and Siow, A. (1980)&lt;DOI:10.1007/bf02888369&gt; Posterior Odds Ratio
for Selected Regression Hypotheses. In Bayesian Statistics 1 (J.M. Bernardo,
M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.) 585-603. Valencia:
University Press.
</p>
<p>Zellner, A. and Siow, A. (1984). Basic Issues in Econometrics. Chicago:
University of Chicago Press.
</p>
<p>Zellner, A. (1986)&lt;DOI:10.2307/2233941&gt; On Assessing Prior Distributions and
Bayesian Regression Analysis with g-prior Distributions. In Bayesian
Inference and Decision techniques: Essays in Honor of Bruno de Finetti (A.
Zellner, ed.) 389-399. Edward Elgar Publishing Limited.
</p>


<h3>See Also</h3>

<p>Use <code>print.Bvs</code> for the best visited models and an 
estimation of their posterior probabilities and  <code>summary.Bvs</code> for
summaries of the posterior distribution.
</p>
<p><code>plot.Bvs</code> for several plots of the result,
<code>BMAcoeff</code> for obtaining model averaged simulations
of regression coefficients and <code>predict.Bvs</code> for
predictions.
</p>
<p><code>GibbsBvs</code> for a heuristic approximation based on
Gibbs sampling (recommended when p&gt;20, no other possibilities when p&gt;31).
</p>
<p>See <code>GibbsBvsF</code> if there are factors among the explanatory variables
</p>


<h3>Examples</h3>

<pre><code class="language-R">
## Not run: 
#Analysis of Crime Data
#load data
data(UScrime)

#Default arguments are Robust prior for the regression parameters
#and constant prior over the model space
#Here we keep the 1000 most probable models a posteriori:
crime.Bvs&lt;- Bvs(formula= y ~ ., data=UScrime, n.keep=1000)

#A look at the results:
crime.Bvs

summary(crime.Bvs)

#A plot with the posterior probabilities of the dimension of the
#true model:
plot(crime.Bvs, option="dimension")

#Two image plots of the conditional inclusion probabilities:
plot(crime.Bvs, option="conditional")
plot(crime.Bvs, option="not")

## End(Not run)

</code></pre>


</div>