<div class="container">

<table style="width: 100%;"><tr>
<td>brnn_extended</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>brnn_extended</h2>

<h3>Description</h3>

<p>The brnn_extended function
fits a two layer neural network as described in MacKay (1992) and Foresee and Hagan (1997). It uses the 
Nguyen and Widrow algorithm (1990) to assign initial weights and the Gauss-Newton algorithm to 
perform the optimization. The hidden layer contains two groups of neurons 
that allow us to assign different prior distributions for two groups of input variables.
</p>


<h3>Usage</h3>

<pre><code class="language-R">  brnn_extended(x, ...)

  ## S3 method for class 'formula'
brnn_extended(formula, data, contrastsx=NULL,contrastsz=NULL,...)

  ## Default S3 method:
brnn_extended(x,y,z,neurons1,neurons2,normalize=TRUE,epochs=1000,
              mu=0.005,mu_dec=0.1, mu_inc=10,mu_max=1e10,min_grad=1e-10,
              change = 0.001, cores=1,verbose =FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p>A formula of the form <code>y ~ x1 + x2 ... | z1 + z2 ...</code>, the | is used to separate the two groups of input variables.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>Data frame from which variables specified in  <code>formula</code> are preferentially to be taken.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>(numeric, <code class="reqn">n</code>) the response data-vector (NAs not  allowed).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>(numeric, <code class="reqn">n \times p</code>) incidence matrix for variables in group 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>z</code></td>
<td>
<p>(numeric, <code class="reqn">n \times q</code>) incidence matrix for variables in group 2.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>neurons1</code></td>
<td>
<p>positive integer that indicates the number of neurons for variables in group 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>neurons2</code></td>
<td>
<p>positive integer that indicates the number of neurons for variables in group 2.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>normalize</code></td>
<td>
<p>logical, if TRUE will normalize inputs and output, the default value is TRUE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epochs</code></td>
<td>
<p>positive integer, maximum number of epochs to train, default 1000.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mu</code></td>
<td>
<p>positive number that controls the behaviour of the Gauss-Newton optimization algorithm, default value 0.005.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mu_dec</code></td>
<td>
<p>positive number, is the mu decrease ratio, default value 0.1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mu_inc</code></td>
<td>
<p>positive number, is the mu increase ratio, default value 10.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mu_max</code></td>
<td>
<p>maximum mu before training is stopped, strict positive number, default value <code class="reqn">1\times 10^{10}</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min_grad</code></td>
<td>
<p>minimum gradient.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>change</code></td>
<td>
<p>The program  will stop if the maximum (in absolute value) of the differences of the F 
function in 3 consecutive iterations is less than this quantity.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cores</code></td>
<td>
<p>Number of cpu cores to use for calculations (only available in UNIX-like operating systems). The function detectCores in the R package 
parallel can be used to attempt to detect the number of CPUs in the machine that R is running, but not necessarily 
all the cores are available for the current user, because for example in multi-user 
systems it will depend on system policies. Further details can be found in the documentation for the parallel package</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>logical, if TRUE will print iteration history.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>contrastsx</code></td>
<td>
<p>an optional list of contrasts to be used for some or 
all of the factors appearing as variables in the first group of input variables in the model formula.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>contrastsz</code></td>
<td>
<p>an optional list of contrasts to be used for some or 
all of the factors appearing as variables in the second group of input variables in the model formula.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The software fits a two layer network as described in MacKay (1992) and Foresee and Hagan (1997). 
The model is given by:
</p>
<p><code class="reqn">y_i= \sum_{k=1}^{s_1} w_k^{1} g_k (b_k^{1} + \sum_{j=1}^p x_{ij} \beta_j^{1[k]}) +
            \sum_{k=1}^{s_2} w_k^{2} g_k (b_k^{2} + \sum_{j=1}^q z_{ij} \beta_j^{2[k]})\,\,e_i, i=1,...,n</code>
</p>

<ul>
<li>
<p><code class="reqn">e_i \sim N(0,\sigma_e^2)</code>.
</p>
</li>
<li>
<p><code class="reqn">g_k(\cdot)</code> is the activation function, in this implementation <code class="reqn">g_k(x)=\frac{\exp(2x)-1}{\exp(2x)+1}</code>.
</p>
</li>
</ul>
<p>The software will minimize 
</p>
<p style="text-align: center;"><code class="reqn">F=\beta E_D + \alpha \theta_1' \theta_1 +\delta \theta_2' \theta_2 </code>
</p>

<p>where 
</p>

<ul>
<li>
<p><code class="reqn">E_D=\sum_{i=1}^n (y_i-\hat y_i)^2</code>, i.e. the sum of squared errors.
</p>
</li>
<li>
<p><code class="reqn">\beta=\frac{1}{2\sigma^2_e}</code>.
</p>
</li>
<li>
<p><code class="reqn">\alpha=\frac{1}{2\sigma_{\theta_1}^2}</code>, <code class="reqn">\sigma_{\theta_1}^2</code> is a dispersion parameter for weights and biases for the associated to 
the first group of neurons.
</p>
</li>
<li>
<p><code class="reqn">\delta=\frac{1}{2\sigma_{\theta_2}^2}</code>, <code class="reqn">\sigma_{\theta_2}^2</code> is a dispersion parameter for weights and biases for the associated to
the second group of neurons.
</p>
</li>
</ul>
<h3>Value</h3>

<p>object of class <code>"brnn_extended"</code> or <code>"brnn_extended.formula"</code>. Mostly internal structure, but it is a list containing:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>$theta1</code></td>
<td>
<p>A list containing weights and biases. The first <code class="reqn">s_1</code> components of the list contain vectors with 
the estimated parameters for the <code class="reqn">k</code>-th neuron, i.e. <code class="reqn">(w_k^1, b_k^1, \beta_1^{1[k]},...,\beta_p^{1[k]})'</code>. 
<code class="reqn">s_1</code> corresponds to neurons1 in the argument list.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>$theta2</code></td>
<td>
<p>A list containing weights and biases. The first <code class="reqn">s_2</code> components of the list contains vectors with 
the estimated parameters for the <code class="reqn">k</code>-th neuron, i.e. <code class="reqn">(w_k^2, b_k^2, \beta_1^{2[k]},...,\beta_q^{2[k]})'</code>.
<code class="reqn">s_2</code> corresponds to neurons2 in the argument list.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>$message</code></td>
<td>
<p>String that indicates the stopping criteria for the training process.</p>
</td>
</tr>
</table>
<h3>References</h3>

<p>Foresee, F. D., and M. T. Hagan. 1997. "Gauss-Newton approximation to Bayesian regularization", 
<em>Proceedings of the 1997 International Joint Conference on Neural Networks</em>.
</p>
<p>MacKay, D. J. C. 1992. "Bayesian interpolation", <em>Neural Computation</em>, <b>4(3)</b>, 415-447.
</p>
<p>Nguyen, D. and Widrow, B. 1990. "Improving the learning speed of 2-layer neural networks by choosing initial values of the adaptive weights",
<em>Proceedings of the IJCNN</em>, <b>3</b>, 21-26.
</p>


<h3>See Also</h3>

<p><code>predict.brnn_extended</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
## Not run: 

#Example 5
#Warning, it will take a while

#Load the Jersey dataset
data(Jersey)

#Predictive power of the model using the SECOND set for 10 fold CROSS-VALIDATION
data=pheno
data$G=G
data$D=D
data$partitions=partitions

#Fit the model for the TESTING DATA for Additive + Dominant
out=brnn_extended(yield_devMilk ~ G | D,
                                  data=subset(data,partitions!=2),
                                  neurons1=2,neurons2=2,epochs=100,verbose=TRUE)

#Plot the results
#Predicted vs observed values for the training set
par(mfrow=c(2,1))
yhat_R_training=predict(out)
plot(out$y,yhat_R_training,xlab=expression(hat(y)),ylab="y")
cor(out$y,yhat_R_training)

#Predicted vs observed values for the testing set
newdata=subset(data,partitions==2,select=c(D,G))
ytesting=pheno$yield_devMilk[partitions==2]
yhat_R_testing=predict(out,newdata=newdata)
plot(ytesting,yhat_R_testing,xlab=expression(hat(y)),ylab="y")
cor(ytesting,yhat_R_testing)
  

## End(Not run)
 
</code></pre>


</div>