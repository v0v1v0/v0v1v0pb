<div class="container">

<table style="width: 100%;"><tr>
<td>api-perform</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>BigQuery jobs: perform a job</h2>

<h3>Description</h3>

<p>These functions are low-level functions designed to be used by experts.
Each of these low-level functions is paired with a high-level function that
you should use instead:
</p>

<ul>
<li> <p><code>bq_perform_copy()</code>:    <code>bq_table_copy()</code>.
</p>
</li>
<li> <p><code>bq_perform_query()</code>:   <code>bq_dataset_query()</code>, <code>bq_project_query()</code>.
</p>
</li>
<li> <p><code>bq_perform_upload()</code>:  <code>bq_table_upload()</code>.
</p>
</li>
<li> <p><code>bq_perform_load()</code>:    <code>bq_table_load()</code>.
</p>
</li>
<li> <p><code>bq_perform_extract()</code>: <code>bq_table_save()</code>.
</p>
</li>
</ul>
<h3>Usage</h3>

<pre><code class="language-R">bq_perform_extract(
  x,
  destination_uris,
  destination_format = "NEWLINE_DELIMITED_JSON",
  compression = "NONE",
  ...,
  print_header = TRUE,
  billing = x$project
)

bq_perform_upload(
  x,
  values,
  fields = NULL,
  create_disposition = "CREATE_IF_NEEDED",
  write_disposition = "WRITE_EMPTY",
  ...,
  billing = x$project
)

bq_perform_load(
  x,
  source_uris,
  billing = x$project,
  source_format = "NEWLINE_DELIMITED_JSON",
  fields = NULL,
  nskip = 0,
  create_disposition = "CREATE_IF_NEEDED",
  write_disposition = "WRITE_EMPTY",
  ...
)

bq_perform_query(
  query,
  billing,
  ...,
  parameters = NULL,
  destination_table = NULL,
  default_dataset = NULL,
  create_disposition = "CREATE_IF_NEEDED",
  write_disposition = "WRITE_EMPTY",
  use_legacy_sql = FALSE,
  priority = "INTERACTIVE"
)

bq_perform_query_dry_run(
  query,
  billing,
  ...,
  default_dataset = NULL,
  parameters = NULL,
  use_legacy_sql = FALSE
)

bq_perform_copy(
  src,
  dest,
  create_disposition = "CREATE_IF_NEEDED",
  write_disposition = "WRITE_EMPTY",
  ...,
  billing = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>A bq_table</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>destination_uris</code></td>
<td>
<p>A character vector of fully-qualified Google Cloud
Storage URIs where the extracted table should be written. Can export
up to 1 Gb of data per file. Use a wild card URI (e.g.
<code style="white-space: pre;">⁠gs://[YOUR_BUCKET]/file-name-*.json⁠</code>) to automatically create any
number of files.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>destination_format</code></td>
<td>
<p>The exported file format. Possible values
include "CSV", "NEWLINE_DELIMITED_JSON" and "AVRO". Tables with nested or
repeated fields cannot be exported as CSV.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>compression</code></td>
<td>
<p>The compression type to use for exported files. Possible
values include "GZIP", "DEFLATE", "SNAPPY", and "NONE". "DEFLATE" and
"SNAPPY" are only supported for Avro.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Additional arguments passed on to the underlying API call.
snake_case names are automatically converted to camelCase.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>print_header</code></td>
<td>
<p>Whether to print out a header row in the results.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>billing</code></td>
<td>
<p>Identifier of project to bill.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>values</code></td>
<td>
<p>Data frame of values to insert.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fields</code></td>
<td>
<p>A bq_fields specification, or something coercible to it
(like a data frame). Leave as <code>NULL</code> to allow BigQuery to auto-detect
the fields.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>create_disposition</code></td>
<td>
<p>Specifies whether the job is allowed to create
new tables.
</p>
<p>The following values are supported:
</p>

<ul>
<li>
<p> "CREATE_IF_NEEDED": If the table does not exist, BigQuery creates the
table.
</p>
</li>
<li>
<p> "CREATE_NEVER": The table must already exist. If it does not, a
'notFound' error is returned in the job result.
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>write_disposition</code></td>
<td>
<p>Specifies the action that occurs if the
destination table already exists. The following values are supported:
</p>

<ul>
<li>
<p> "WRITE_TRUNCATE": If the table already exists, BigQuery overwrites the
table data.
</p>
</li>
<li>
<p> "WRITE_APPEND": If the table already exists, BigQuery appends the data
to the table.
</p>
</li>
<li>
<p> "WRITE_EMPTY": If the table already exists and contains data, a
'duplicate' error is returned in the job result.
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>source_uris</code></td>
<td>
<p>The fully-qualified URIs that point to your data in
Google Cloud.
</p>
<p>For Google Cloud Storage URIs: Each URI can contain one
''*'“ wildcard character and it must come after the 'bucket' name.
Size limits related to load jobs apply to external data sources.
</p>
<p>For Google Cloud Bigtable URIs: Exactly one URI can be specified and
it has be a fully specified and valid HTTPS URL for a Google Cloud
Bigtable table. For Google Cloud Datastore backups: Exactly one URI
can be specified. Also, the '*' wildcard character is not allowed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>source_format</code></td>
<td>
<p>The format of the data files:
</p>

<ul>
<li>
<p> For CSV files, specify "CSV".
</p>
</li>
<li>
<p> For datastore backups, specify "DATASTORE_BACKUP".
</p>
</li>
<li>
<p> For newline-delimited JSON, specify "NEWLINE_DELIMITED_JSON".
</p>
</li>
<li>
<p> For Avro, specify "AVRO".
</p>
</li>
<li>
<p> For parquet, specify "PARQUET".
</p>
</li>
<li>
<p> For orc, specify "ORC".
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nskip</code></td>
<td>
<p>For <code>source_format = "CSV"</code>, the number of header rows to skip.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>query</code></td>
<td>
<p>SQL query string.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>parameters</code></td>
<td>
<p>Named list of parameters match to query parameters.
Parameter <code>x</code> will be matched to placeholder <code style="white-space: pre;">⁠@x⁠</code>.
</p>
<p>Generally, you can supply R vectors and they will be automatically
converted to the correct type. If you need greater control, you can call
<code>bq_param_scalar()</code> or <code>bq_param_array()</code> explicitly.
</p>
<p>See <a href="https://cloud.google.com/bigquery/docs/parameterized-queries">https://cloud.google.com/bigquery/docs/parameterized-queries</a>
for more details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>destination_table</code></td>
<td>
<p>A bq_table where results should be stored.
If not supplied, results will be saved to a temporary table that lives
in a special dataset. You must supply this parameter for large
queries (&gt; 128 MB compressed).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>default_dataset</code></td>
<td>
<p>A bq_dataset used to automatically qualify table names.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>use_legacy_sql</code></td>
<td>
<p>If <code>TRUE</code> will use BigQuery's legacy SQL format.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>priority</code></td>
<td>
<p>Specifies a priority for the query. Possible values include
"INTERACTIVE" and "BATCH". Batch queries do not start immediately,
but are not rate-limited in the same way as interactive queries.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A bq_job.
</p>


<h3>Google BigQuery API documentation</h3>


<ul><li> <p><a href="https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs">jobs</a>
</p>
</li></ul>
<p>Additional information at:
</p>

<ul>
<li> <p><a href="https://cloud.google.com/bigquery/docs/exporting-data">exporting data</a>
</p>
</li>
<li> <p><a href="https://cloud.google.com/bigquery/docs/loading-data">loading data</a>
</p>
</li>
<li> <p><a href="https://cloud.google.com/bigquery/docs/writing-results">writing queries</a>
</p>
</li>
<li> <p><a href="https://cloud.google.com/bigquery/docs/managing-tables#copy-table">copying a table</a>
</p>
</li>
</ul>
<h3>Examples</h3>

<pre><code class="language-R">
ds &lt;- bq_test_dataset()
bq_mtcars &lt;- bq_table(ds, "mtcars")
job &lt;- bq_perform_upload(bq_mtcars, mtcars)
bq_table_exists(bq_mtcars)

bq_job_wait(job)
bq_table_exists(bq_mtcars)
head(bq_table_download(bq_mtcars))

</code></pre>


</div>