<div class="container">

<table style="width: 100%;"><tr>
<td>bsrr</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Best subset ridge regression</h2>

<h3>Description</h3>

<p>Best subset ridge regression for generalized linear model and Cox's proportional
model.
</p>


<h3>Usage</h3>

<pre><code class="language-R">bsrr(
  x,
  y,
  family = c("gaussian", "binomial", "poisson", "cox"),
  method = c("pgsection", "sequential", "psequential"),
  tune = c("gic", "ebic", "bic", "aic", "cv"),
  s.list,
  lambda.list = 0,
  s.min,
  s.max,
  lambda.min = 0.001,
  lambda.max = 100,
  nlambda = 100,
  always.include = NULL,
  screening.num = NULL,
  normalize = NULL,
  weight = NULL,
  max.iter = 20,
  warm.start = TRUE,
  nfolds = 5,
  group.index = NULL,
  seed = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>Input matrix, of dimension <code class="reqn">n \times p</code>; each row is an observation
vector and each column is a predictor/feature/variable.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>The response variable, of <code>n</code> observations. For <code>family = "binomial"</code> should be
a factor with two levels. For <code>family="poisson"</code>, <code>y</code> should be a vector with positive integer.
For <code>family = "cox"</code>, <code>y</code> should be a two-column matrix
with columns named <code>time</code> and <code>status</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>family</code></td>
<td>
<p>One of the following models: <code>"gaussian"</code>, <code>"binomial"</code>,
<code>"poisson"</code>, or <code>"cox"</code>. Depending on the response. Any unambiguous substring can be given.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>The method to be used to select the optimal model size and <code class="reqn">L_2</code> shrinkage. For
<code>method = "sequential"</code>, we solve the best subset ridge regression
problem for each <code>s</code> in <code>1,2,...,s.max</code> and <code class="reqn">\lambda</code> in <code>lambda.list</code>.
For <code>method = "pgsection"</code> and <code>"psequential"</code>, the Powell method is used to
solve the best subset ridge regression problem. Any unambiguous substring can be given.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tune</code></td>
<td>
<p>The criterion for choosing the model size and <code class="reqn">L_2</code> shrinkage
parameters. Available options are <code>"gic"</code>, <code>"ebic"</code>, <code>"bic"</code>, <code>"aic"</code> and <code>"cv"</code>.
Default is <code>"gic"</code>. <code>"cv"</code> is recommanded for BSRR.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>s.list</code></td>
<td>
<p>An increasing list of sequential values representing the model
sizes. Only used for <code>method = "sequential"</code>. Default is <code>1:min(p,
round(n/log(n)))</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda.list</code></td>
<td>
<p>A lambda sequence for <code>"bsrr"</code>. Only used for <code>method = "sequential"</code>. Default is
<code>exp(seq(log(100), log(0.01), length.out = 100))</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>s.min</code></td>
<td>
<p>The minimum value of model sizes. Only used for <code>"psequential"</code> and <code>"pgsection"</code>. Default is 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>s.max</code></td>
<td>
<p>The maximum value of model sizes. Only used for <code>"psequential"</code> and <code>"pgsection"</code>.
Default is <code>min(p, round(n/log(n)))</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda.min</code></td>
<td>
<p>The minimum value of lambda. Only used for <code>method =
"psequential"</code> and <code>"pgsection"</code>. Default is <code>0.001</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda.max</code></td>
<td>
<p>The maximum value of lambda. Only used for <code>method =
"psequential"</code> and <code>"pgsection"</code>. Default is <code>100</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nlambda</code></td>
<td>
<p>The number of <code class="reqn">\lambda</code>s for the Powell path with sequential line search method.
Only valid for <code>method = "psequential"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>always.include</code></td>
<td>
<p>An integer vector containing the indexes of variables that should always be included in the model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>screening.num</code></td>
<td>
<p>Users can pre-exclude some irrelevant variables according to maximum marginal likelihood estimators before fitting a
model by passing an integer to <code>screening.num</code> and the sure independence screening will choose a set of variables of this size.
Then the active set updates are restricted on this subset.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>normalize</code></td>
<td>
<p>Options for normalization. <code>normalize = 0</code> for
no normalization. Setting <code>normalize = 1</code> will
only subtract the mean of columns of <code>x</code>.
<code>normalize = 2</code> for scaling the columns of <code>x</code> to have <code class="reqn">\sqrt n</code> norm.
<code>normalize = 3</code> for subtracting the means of the columns of <code>x</code> and <code>y</code>, and also
normalizing the columns of <code>x</code> to have <code class="reqn">\sqrt n</code> norm.
If <code>normalize = NULL</code>, by default, <code>normalize</code> will be set <code>1</code> for <code>"gaussian"</code>,
<code>2</code> for <code>"binomial"</code> and <code>"poisson"</code>, <code>3</code> for <code>"cox"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weight</code></td>
<td>
<p>Observation weights. Default is <code>1</code> for each observation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max.iter</code></td>
<td>
<p>The maximum number of iterations in the <code>bsrr</code> function.
In most of the case, only a few steps can guarantee the convergence. Default
is <code>20</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>warm.start</code></td>
<td>
<p>Whether to use the last solution as a warm start. Default
is <code>TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nfolds</code></td>
<td>
<p>The number of folds in cross-validation. Default is <code>5</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>group.index</code></td>
<td>
<p>A vector of integers indicating the which group each variable is in.
For variables in the same group, they should be located in adjacent columns of <code>x</code>
and their corresponding index in <code>group.index</code> should be the same.
Denote the first group as <code>1</code>, the second <code>2</code>, etc.
If you do not fit a model with a group structure,
please set <code>group.index = NULL</code>. Default is <code>NULL</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>
<p>Seed to be used to divide the sample into K cross-validation folds. Default is <code>NULL</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The best ridge regression problem with model size <code class="reqn">s</code> and the shrinkage parameter <code class="reqn">\lambda</code> is
</p>
<p style="text-align: center;"><code class="reqn">\min_\beta -2 \log L(\beta) + \lambda\Vert\beta\Vert_2^2 \;\;{\rm
s.t.}\;\; \|\beta\|_0 \leq s.</code>
</p>
<p> In the GLM case, <code class="reqn">\log L(\beta)</code> is the
log likelihood function; In the Cox model, <code class="reqn">\log L(\beta)</code> is the log
partial likelihood function.
</p>
<p>The best subset selection problem is a special case of the best ridge regression problem
with the shrinkage <code class="reqn">\lambda=0</code>.
</p>
<p>For each candidate model size and <code class="reqn">\lambda</code>, the best subset ridge regression
problems are solved by the <code class="reqn">L_2</code> penalized primal-dual active set (PDAS) algorithm, see Wen et
al (2020) for details. This algorithm
utilizes an active set updating strategy via primal and dual variables and
fits the sub-model by exploiting the fact that their support sets are
non-overlap and complementary. For the case of <code>method = "sequential"</code>
if <code>warm.start = "TRUE"</code>, we run the PDAS algorithm for a list of
sequential model sizes and use the estimate from the last iteration as a
warm start. For the case of
<code> method = "psequential"</code> and <code>method = "pgsection"</code>, the Powell method
using a sequential line search method or a golden section search technique is
used for parameters determination.
</p>


<h3>Value</h3>

<p>A list with class attribute 'bsrr' and named components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>beta</code></td>
<td>
<p>The best fitting coefficients.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>coef0</code></td>
<td>
<p>The best fitting
intercept.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loss</code></td>
<td>
<p>The training loss of the best fitting model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ic</code></td>
<td>
<p>The information criterion of the best fitting model when model
selection is based on a certain information criterion.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cvm</code></td>
<td>
<p>The mean
cross-validated error for the best fitting model when model selection is
based on the cross-validation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>The lambda chosen for the best fitting model</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta.all</code></td>
<td>
<p>For <code>bsrr</code> objects obtained by <code>gsection</code>, <code>pgsection</code>
and <code>psequential</code>, <code>beta.all</code> is a matrix with each column be the coefficients
of the model in each iterative step in the tuning path.
For <code>bsrr</code> objects obtained by <code>sequential</code> method,
A list of the best fitting coefficients of size
<code>s=0,1,...,p</code> and <code class="reqn">\lambda</code> in <code>lambda.list</code> with the
smallest loss function. For <code>"bsrr"</code> objects of <code>"bsrr"</code> type, the fitting coefficients of the
<code class="reqn">i^{th} \lambda</code> and the <code class="reqn">j^{th}</code> <code>s</code> are at the <code class="reqn">i^{th}</code>
list component's <code class="reqn">j^{th}</code> column.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>coef0.all</code></td>
<td>
<p>For <code>bsrr</code> objects obtained from <code>gsection</code>, <code>pgsection</code> and <code>psequential</code>,
<code>coef0.all</code> contains the intercept for the model in each iterative step in the tuning path.
For <code>bsrr</code> objects obtained from <code>sequential</code> path,
<code>coef0.all</code> contains the best fitting
intercepts of size <code class="reqn">s=0,1,\dots,p</code> and <code class="reqn">\lambda</code> in
<code>lambda.list</code> with the smallest loss function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loss.all</code></td>
<td>
<p>For <code>bsrr</code> objects obtained from <code>gsection</code>, <code>pgsection</code> and <code>psequential</code>,
<code>loss.all</code> contains the training loss of the model in each iterative step in the tuning path.
For <code>bsrr</code> objects obtained from <code>sequential</code> path, this is a
list of the training loss of the best fitting intercepts of model size
<code class="reqn">s=0,1,\dots,p</code> and <code class="reqn">\lambda</code> in <code>lambda.list</code>. For <code>"bsrr"</code> object obtained by <code>"bsrr"</code>,
the training loss of the <code class="reqn">i^{th} \lambda</code> and the <code class="reqn">j^{th}</code> <code>s</code>
is at the <code class="reqn">i^{th}</code> list component's <code class="reqn">j^{th}</code> entry.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ic.all</code></td>
<td>
<p>For <code>bsrr</code> objects obtained from <code>gsection</code>, <code>pgsection</code> and <code>psequential</code>,
<code>ic.all</code> contains the values of the chosen information criterion of the model in each iterative step in the tuning path.
For <code>bsrr</code> objects obtained from <code>sequential</code> path, this is a
matrix of the values of the chosen information criterion of model size <code class="reqn">s=0,1,\dots,p</code>
and <code class="reqn">\lambda</code> in <code>lambda.list</code> with the smallest loss function. For <code>"bsrr"</code> object obtained by <code>"bsrr"</code>,
the training loss of the <code class="reqn">i^{th} \lambda</code> and the <code class="reqn">j^{th}</code>
<code>s</code> is at the <code class="reqn">i^{th}</code> row <code class="reqn">j^{th}</code> column. Only available when
model selection is based on a certain information criterion.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cvm.all</code></td>
<td>
<p>For <code>bsrr</code> objects obtained from <code>gsection</code>, <code>pgsection</code> and <code>psequential</code>,
<code>cvm.all</code> contains the mean cross-validation error of the model in each iterative step in the tuning path.
For <code>bsrr</code> objects obtained from <code>sequential</code> path, this is a
matrix of the mean cross-validation error of model size
<code class="reqn">s=0,1,\dots,p</code> and <code class="reqn">\lambda</code> in <code>lambda.list</code> with the
smallest loss function. For <code>"bsrr"</code> object obtained by <code>"bsrr"</code>, the training loss of the <code class="reqn">i^{th}
\lambda</code> and the <code class="reqn">j^{th}</code> <code>s</code> is at the <code class="reqn">i^{th}</code> row
<code class="reqn">j^{th}</code> column. Only available when model selection is based on the
cross-validation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda.all</code></td>
<td>
<p>The lambda chosen for each step in <code>pgsection</code> and <code>psequential</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>family</code></td>
<td>
<p>Type of the model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>s.list</code></td>
<td>
<p>The input
<code>s.list</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nsample</code></td>
<td>
<p>The sample size.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p>Either <code>"bss"</code> or <code>"bsrr"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>Method used for tuning parameters selection.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ic.type</code></td>
<td>
<p>The criterion of model selection.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Liyuan Hu, Kangkang Jiang, Yanhang Zhang, Jin Zhu, Canhong Wen and Xueqin Wang.
</p>


<h3>See Also</h3>

<p><code>plot.bsrr</code>, <code>summary.bsrr</code>,
<code>coef.bsrr</code>, <code>predict.bsrr</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
#-------------------linear model----------------------#
# Generate simulated data
n &lt;- 200
p &lt;- 20
k &lt;- 5
rho &lt;- 0.4
seed &lt;- 10
Tbeta &lt;- rep(0, p)
Tbeta[1:k*floor(p/k):floor(p/k)] &lt;- rep(1, k)
Data &lt;- gen.data(n, p, k, rho, family = "gaussian", beta = Tbeta, seed = seed)
x &lt;- Data$x[1:140, ]
y &lt;- Data$y[1:140]
x_new &lt;- Data$x[141:200, ]
y_new &lt;- Data$y[141:200]
lambda.list &lt;- exp(seq(log(5), log(0.1), length.out = 10))
lm.bsrr &lt;- bsrr(x, y, method = "pgsection")
coef(lm.bsrr)
print(lm.bsrr)
summary(lm.bsrr)
pred.bsrr &lt;- predict(lm.bsrr, newx = x_new)

# generate plots
plot(lm.bsrr)
#-------------------logistic model----------------------#
#Generate simulated data
Data &lt;- gen.data(n, p, k, rho, family = "binomial", beta = Tbeta, seed = seed)

x &lt;- Data$x[1:140, ]
y &lt;- Data$y[1:140]
x_new &lt;- Data$x[141:200, ]
y_new &lt;- Data$y[141:200]
lambda.list &lt;- exp(seq(log(5), log(0.1), length.out = 10))
logi.bsrr &lt;- bsrr(x, y, family = "binomial", lambda.list = lambda.list)
coef(logi.bsrr)
print(logi.bsrr)
summary(logi.bsrr)
pred.bsrr &lt;- predict(logi.bsrr, newx = x_new)

# generate plots
plot(logi.bsrr)
#-------------------poisson model----------------------#
Data &lt;- gen.data(n, p, k, rho=0.3, family = "poisson", beta = Tbeta, seed = seed)

x &lt;- Data$x[1:140, ]
y &lt;- Data$y[1:140]
x_new &lt;- Data$x[141:200, ]
y_new &lt;- Data$y[141:200]
lambda.list &lt;- exp(seq(log(5), log(0.1), length.out = 10))
poi.bsrr &lt;- bsrr(x, y, family = "poisson", lambda.list = lambda.list)
coef(poi.bsrr)
print(poi.bsrr)
summary(poi.bsrr)
pred.bsrr &lt;- predict(poi.bsrr, newx = x_new)

# generate plots
plot(poi.bsrr)
#-------------------coxph model----------------------#
#Generate simulated data
Data &lt;- gen.data(n, p, k, rho, family = "cox", scal = 10, beta = Tbeta)

x &lt;- Data$x[1:140, ]
y &lt;- Data$y[1:140, ]
x_new &lt;- Data$x[141:200, ]
y_new &lt;- Data$y[141:200, ]
lambda.list &lt;- exp(seq(log(5), log(0.1), length.out = 10))
cox.bsrr &lt;- bsrr(x, y, family = "cox", lambda.list = lambda.list)
coef(cox.bsrr)
print(cox.bsrr)
summary(cox.bsrr)
pred.bsrr &lt;- predict(cox.bsrr, newx = x_new)

# generate plots
plot(cox.bsrr)

#----------------------High dimensional linear models--------------------#
## Not run: 
data &lt;- gen.data(n, p = 1000, k, family = "gaussian", seed = seed)

# Best subset selection with SIS screening
lm.high &lt;- bsrr(data$x, data$y, screening.num = 100)

## End(Not run)

#-------------------group selection----------------------#
beta &lt;- rep(c(rep(1,2),rep(0,3)), 4)
Data &lt;- gen.data(200, 20, 5, rho=0.4, beta = beta, seed =10)
x &lt;- Data$x
y &lt;- Data$y

group.index &lt;- c(rep(1, 2), rep(2, 3), rep(3, 2), rep(4, 3),
                rep(5, 2), rep(6, 3), rep(7, 2), rep(8, 3))
lm.groupbsrr &lt;- bsrr(x, y, s.min = 1, s.max = 8, group.index = group.index)
coef(lm.groupbsrr)
print(lm.groupbsrr)
summary(lm.groupbsrr)
pred.groupl0l2 &lt;- predict(lm.groupbsrr, newx = x_new)
#-------------------include specified variables----------------------#
Data &lt;- gen.data(n, p, k, rho, family = "gaussian", beta = Tbeta, seed = seed)
lm.bsrr &lt;- bsrr(Data$x, Data$y, always.include = 2)

</code></pre>


</div>