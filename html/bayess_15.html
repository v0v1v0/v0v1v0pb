<div class="container">

<table style="width: 100%;"><tr>
<td>gibbsnorm</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Gibbs sampler for a generic mixture posterior distribution
</h2>

<h3>Description</h3>

<p>This function implements the generic Gibbs sampler of Diebolt and Robert (1994)
for producing a sample from the posterior distribution associated
with a univariate mixture of <code class="reqn">k</code> normal components with all <code class="reqn">3k-1</code> parameters
unknown.
</p>


<h3>Usage</h3>

<pre><code class="language-R">gibbsnorm(niter, dat, mix)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>niter</code></td>
<td>

<p>number of iterations in the Gibbs sampler
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dat</code></td>
<td>

<p>mixture sample
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mix</code></td>
<td>

<p>list defined as <code>mix=list(k=k,p=p,mu=mu,sig=sig)</code>,
where <code>k</code> is an integer and the remaining entries are
vectors of length <code>k</code>
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Under conjugate priors on the means (normal distributions), variances (inverse gamma
distributions), and weights (Dirichlet distribution), the full conditional distributions
given the latent variables are directly available and can be used in a straightforward 
Gibbs sampler. This function is only the first step of the function <code>gibbs</code>, but it
may be much faster as it avoids the computation of the evidence via Chib's approach.

</p>


<h3>Value</h3>



<table>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>number of components (superfluous)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mu</code></td>
<td>
<p>Gibbs sample of all mean parameters</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sig</code></td>
<td>
<p>Gibbs sample of all variance parameters</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>p</code></td>
<td>
<p>Gibbs sample of all weight parameters</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lopost</code></td>
<td>
<p>sequence of log-likelihood values along Gibbs iterations</p>
</td>
</tr>
</table>
<h3>References</h3>

<p>Chib, S. (1995) Marginal likelihood from the Gibbs output. 
<em>J. American Statist. Associ.</em> <b>90</b>, 1313-1321.
</p>
<p>Diebolt, J. and Robert, C.P. (1992) Estimation of finite mixture distributions by Bayesian sampling.
<em>J. Royal Statist. Society</em> <b>56</b>, 363-375.
</p>


<h3>See Also</h3>

<p><code>rdirichlet</code>, <code>gibbs</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">data(datha)
datha=as.matrix(datha)
mix=list(k=3,mu=mean(datha),sig=var(datha))
res=gibbsnorm(10,datha,mix)
plot(res$p[,1],type="l",col="steelblue3",xlab="iterations",ylab="p")

</code></pre>


</div>